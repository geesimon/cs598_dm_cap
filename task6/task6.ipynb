{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages:\n",
    "* [NLTK](http://www.nltk.org/howto/classify.html)\n",
    "* [SpaCy](https://spacy.io/)\n",
    "* [AllenNLP](https://allennlp.org/tutorials)\n",
    "\n",
    "Articles:\n",
    "* [A Comprehensive Guide to Understand and Implement Text Classification in Python](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)\n",
    "* [Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a)\n",
    "* [State-of-the-Art Text Classification using BERT model: “Predict the Happiness” Challenge](https://appliedmachinelearning.blog/2019/03/04/state-of-the-art-text-classification-using-bert-model-predict-the-happiness-hackerearth-challenge/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13299/13299 [05:58<00:00, 37.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    result_stemmed = []\n",
    "    for token in simple_preprocess(text, min_len = 2):\n",
    "        result.append(token)\n",
    "#         if token not in STOPWORDS:\n",
    "        result_stemmed.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return (result, result_stemmed)\n",
    "\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat.labels\") as f:\n",
    "    LABELS = [int(l) for l in f.readlines() if l[0].isdigit()]\n",
    "\n",
    "ALL_RAW_TEXTS = []\n",
    "ALL_TEXTS = []\n",
    "ALL_STEMMED_TEXTS = []\n",
    "ALL_CONCAT_STEMMED_TEXTS = []\n",
    "LABELED_TEXTS = []\n",
    "LABELED_CONCAT_TEXTS = []\n",
    "LABELED_STEMMED_TEXTS = []\n",
    "LABELED_CONCAT_STEMMED_TEXTS = []\n",
    "\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat\") as f:\n",
    "    ALL_RAW_TEXTS = f.readlines()\n",
    "\n",
    "for _text in tqdm(ALL_RAW_TEXTS):\n",
    "    _result, _result_stemmed = preprocess(_text)\n",
    "    ALL_TEXTS.append(_result)\n",
    "    ALL_STEMMED_TEXTS.append(_result_stemmed)\n",
    "\n",
    "ALL_CONCAT_STEMMED_TEXTS = [\" \".join(_text) for _text in ALL_STEMMED_TEXTS]\n",
    "\n",
    "LABELED_TEXTS = ALL_TEXTS[0:len(LABELS)]\n",
    "LABELED_CONCAT_TEXTS = [\" \".join(_text) for _text in LABELED_TEXTS]\n",
    "\n",
    "LABELED_STEMMED_TEXTS = ALL_STEMMED_TEXTS[0:len(LABELS)]\n",
    "LABELED_CONCAT_STEMMED_TEXTS = [\" \".join(_text) for _text in LABELED_STEMMED_TEXTS]\n",
    "\n",
    "FEATURE_MORE =pd.read_csv(\"./data/raw/Hygiene/hygiene.dat.additional\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe using texts and lables\n",
    "labeled_df = pd.DataFrame()\n",
    "labeled_df['concat_stemmed_text'] = LABELED_CONCAT_STEMMED_TEXTS\n",
    "labeled_df['stemmed_text'] = LABELED_STEMMED_TEXTS\n",
    "# labeled_df['concat_stemmed_text'] = LABELED_CONCAT_TEXTS\n",
    "# labeled_df['stemmed_text'] = LABELED_TEXTS\n",
    "labeled_df['label'] = LABELS\n",
    "\n",
    "# split the dataset into training and validation datasets \n",
    "train_concat_stemmed_text, test_concat_stemmed_text, train_label, test_label = model_selection.train_test_split(labeled_df['concat_stemmed_text'], \n",
    "                                                                                  labeled_df['label'],\n",
    "                                                                                  test_size = 0.2)\n",
    "train_stemmed_text = labeled_df['stemmed_text'][train_concat_stemmed_text.index]\n",
    "test_stemmed_text = labeled_df['stemmed_text'][test_concat_stemmed_text.index]\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_label = encoder.fit_transform(train_label)\n",
    "test_label = encoder.fit_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = corpora.Dictionary(processed_docs)\n",
    "# print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "# dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "# print(\"After prunn:%d\"%(len(dictionary)))\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCountVectorizer:\n",
    "    def __init__(self, max_df = 0.5):\n",
    "        self.vect_model = CountVectorizer(analyzer='word', max_df = max_df)\n",
    "        \n",
    "    def fit(self, texts_concat):\n",
    "        self.vect_model.fit(texts_concat)\n",
    "    \n",
    "    def transform(self, texts_concat):\n",
    "        return self.vect_model.transform(texts_concat)\n",
    "\n",
    "count_vect = MyCountVectorizer()\n",
    "count_vect.fit(train_concat_stemmed_text)\n",
    "train_count = count_vect.transform(train_concat_stemmed_text)\n",
    "test_count =  count_vect.transform(test_concat_stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 174 ms, total: 13.5 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# word level tf-idf\n",
    "class MyTfidfVectorizer(MyCountVectorizer):\n",
    "    def __init__(self, analyzer='word', ngram_range = None, max_features=5000):\n",
    "        if ngram_range is None:\n",
    "            self.vect_model = TfidfVectorizer(analyzer = analyzer, max_features = max_features)\n",
    "        else:\n",
    "            self.vect_model = TfidfVectorizer(analyzer = analyzer, ngram_range = ngram_range, \n",
    "                                              max_features = max_features)\n",
    "            \n",
    "    def fit(self, texts_concat):   \n",
    "        self.vect_model.fit(texts_concat)\n",
    "        self.vocabulary = self.vect_model.vocabulary_\n",
    "\n",
    "\n",
    "tfidf_vect = MyTfidfVectorizer()\n",
    "tfidf_vect.fit(train_concat_stemmed_text)\n",
    "train_tfidf =  tfidf_vect.transform(train_concat_stemmed_text)\n",
    "test_tfidf =  tfidf_vect.transform(test_concat_stemmed_text)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = MyTfidfVectorizer(ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(train_concat_stemmed_text)\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(train_concat_stemmed_text)\n",
    "test_tfidf_ngram =  tfidf_vect_ngram.transform(test_concat_stemmed_text)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = MyTfidfVectorizer(analyzer='char', ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(train_concat_stemmed_text)\n",
    "train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_concat_stemmed_text) \n",
    "test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_concat_stemmed_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build from review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 59s, sys: 38 s, total: 40min 37s\n",
      "Wall time: 1h 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "class MyFastTextTfidfVectorizer(MyCountVectorizer):\n",
    "    def __init__(self, tfidf_vectorizer, size = 100):\n",
    "        self.embedding_size = size\n",
    "        self.tfidf_vectorizer = tfidf_vectorizer\n",
    "        self.fasttext_model = FastText(size = size, window = 5, min_count = 5)\n",
    "\n",
    "    def tfidf2embedding(self, value_vector):\n",
    "        _weighted_value = np.zeros(self.embedding_size)\n",
    "        for key in self.tfidf_vectorizer.vocabulary:\n",
    "            _index = self.tfidf_vectorizer.vocabulary[key]\n",
    "            if value_vector[_index] != 0:\n",
    "                _weighted_value += self.fasttext_model[key] * value_vector[_index]\n",
    "\n",
    "        return _weighted_value\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        _texts_concat = [\" \".join(_text) for _text in texts]\n",
    "        self.tfidf_vectorizer = MyTfidfVectorizer()\n",
    "        self.tfidf_vectorizer.fit(_texts_concat)\n",
    "        \n",
    "        self.fasttext_model.build_vocab(sentences = texts)\n",
    "        self.fasttext_model.train(sentences = texts, \n",
    "                                  total_examples = len(texts), \n",
    "                                  epochs=10)\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        _texts_concat = [\" \".join(_text) for _text in texts]\n",
    "        _tfidf_values = self.tfidf_vectorizer.transform(_texts_concat)\n",
    "        return np.asarray([self.tfidf2embedding(_value.toarray()[0]) for _value in _tfidf_values])\n",
    "\n",
    "fasttext_tfidf_vect = MyFastTextTfidfVectorizer(tfidf_vect)\n",
    "fasttext_tfidf_vect.fit(ALL_STEMMED_TEXTS)\n",
    "train_fasttext_embedding = fasttext_tfidf_vect.transform(train_stemmed_text)\n",
    "test_fasttext_embedding = fasttext_tfidf_vect.transform(test_stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prebuilt Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers\n",
    "\n",
    "# # load the pre-trained word-embedding vectors \n",
    "# embeddings_index = {}\n",
    "# for i, line in enumerate(open('data/model/wiki-news-300d-1M.vec')):\n",
    "#     values = line.split()\n",
    "#     embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# # create a tokenizer \n",
    "# token = text.Tokenizer()\n",
    "# token.fit_on_texts(LABELED_CONCAT_STEMMED_TEXTS)\n",
    "# word_index = token.word_index\n",
    "\n",
    "# # convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "# text_train_seq = sequence.pad_sequences(token.texts_to_sequences(train_text), maxlen=70)\n",
    "# text_test_seq = sequence.pad_sequences(token.texts_to_sequences(test_text), maxlen=70)\n",
    "\n",
    "# # create token-embedding mapping\n",
    "# embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text / NLP based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "# trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "# trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "# trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "# trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "# trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import textblob\n",
    "\n",
    "# pos_family = {\n",
    "#     'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "#     'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "#     'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "#     'adj' :  ['JJ','JJR','JJS'],\n",
    "#     'adv' : ['RB','RBR','RBS','WRB']\n",
    "# }\n",
    "\n",
    "# # function to check and get the part of speech tag count of a words in a given sentence\n",
    "# def check_pos_tag(x, flag):\n",
    "#     cnt = 0\n",
    "#     try:\n",
    "#         wiki = textblob.TextBlob(x)\n",
    "#         for tup in wiki.tags:\n",
    "#             ppo = list(tup)[1]\n",
    "#             if ppo in pos_family[flag]:\n",
    "#                 cnt += 1\n",
    "#     except:\n",
    "#         pass\n",
    "#     return cnt\n",
    "\n",
    "# trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "# trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "# trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "# trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "# trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train a LDA Model\n",
    "# lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "# X_topics = lda_model.fit_transform(text_train_count)\n",
    "# topic_word = lda_model.components_\n",
    "# vocab = count_vect.get_feature_names()\n",
    "\n",
    "# # view the topic models\n",
    "# n_top_words = 10\n",
    "# topic_summaries = []\n",
    "# for i, topic_dist in enumerate(topic_word):\n",
    "#     topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "#     topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.75 s, sys: 246 ms, total: 5.99 s\n",
      "Wall time: 57.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "from gensim import corpora, models\n",
    "\n",
    "class MyLDAVectorizer(MyCountVectorizer):\n",
    "    mallet_path = \"..\" + os.sep + \"mallet-2.0.8\"+ os.sep + \"bin\" + os.sep +\"mallet\"\n",
    "    \n",
    "    def __init__(self, TOPIC_COUNT = 100):\n",
    "        self.topic_count = TOPIC_COUNT\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        self.dictionary = corpora.Dictionary(texts)\n",
    "        _corpus = [self.dictionary.doc2bow(_doc) for _doc in texts]\n",
    "        self.tfidf_model = models.TfidfModel(_corpus)\n",
    "        _tfidf_corpus = self.tfidf_model[_corpus]\n",
    "\n",
    "#         self.vect_model = models.LdaModel(_tfidf_corpus, \n",
    "#                             num_topics = self.topic_count, \n",
    "#                             id2word = self.dictionary,\n",
    "#                             random_state = 100,\n",
    "#                             eval_every = 5, \n",
    "#                             alpha = 'auto', \n",
    "#                             gamma_threshold = 0.01)\n",
    "        \n",
    "        self.vect_model = models.wrappers.LdaMallet(self.mallet_path, \n",
    "                                                     corpus = _corpus, \n",
    "                                                     num_topics = self.topic_count, \n",
    "                                                     id2word = self.dictionary)\n",
    "    \n",
    "    def toarray(self, doc_topics):\n",
    "        _doc_vect  = np.zeros((len(doc_topics), self.topic_count))\n",
    "        \n",
    "        for i, _doc in enumerate(doc_topics):\n",
    "            for _topic, _weight in _doc:\n",
    "                _doc_vect[i][_topic] = _weight\n",
    "        \n",
    "        return _doc_vect\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        _corpus = [self.dictionary.doc2bow(_doc) for _doc in texts]\n",
    "#         _tfidf_corpus = self.tfidf_model[_corpus]\n",
    "        \n",
    "        return self.toarray(self.vect_model[_corpus])\n",
    "\n",
    "lda_vect = MyLDAVectorizer()\n",
    "lda_vect.fit(train_stemmed_text)\n",
    "train_lda = lda_vect.transform(train_stemmed_text)\n",
    "test_lda = lda_vect.transform(test_stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 46s, sys: 23.6 s, total: 40min 10s\n",
      "Wall time: 13min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "\n",
    "class MyDoc2Vectorizer(MyCountVectorizer):\n",
    "    def __init__(self, size = 100):\n",
    "        self.embedding_size = size\n",
    "            \n",
    "    def fit(self, texts):\n",
    "        _docs = [TaggedDocument(_doc, [i]) for i, _doc in enumerate(texts)]\n",
    "        self.vect_model = Doc2Vec(_docs, \n",
    "                                  vector_size = self.embedding_size, \n",
    "                                  window = 8,\n",
    "                                  epochs=40, \n",
    "                                  workers=4)\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        return np.asarray([self.vect_model.infer_vector(_text) for _text in texts])\n",
    "\n",
    "doc2vec_vect = MyDoc2Vectorizer(size = 400)\n",
    "# doc2vec_vect.fit(ALL_STEMMED_TEXTS)\n",
    "doc2vec_vect.fit(train_stemmed_text)\n",
    "train_doc2vec = doc2vec_vect.transform(train_stemmed_text)\n",
    "test_doc2vec = doc2vec_vect.transform(test_stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10351: 1,\n",
       "         8630: 1,\n",
       "         9857: 1,\n",
       "         522: 1,\n",
       "         9457: 1,\n",
       "         11979: 1,\n",
       "         9759: 1,\n",
       "         11165: 1,\n",
       "         7415: 1,\n",
       "         6445: 1,\n",
       "         3062: 1,\n",
       "         504: 1,\n",
       "         3260: 1,\n",
       "         3973: 1,\n",
       "         3004: 1,\n",
       "         8366: 1,\n",
       "         4633: 1,\n",
       "         11473: 1,\n",
       "         12368: 1,\n",
       "         413: 1,\n",
       "         11545: 1,\n",
       "         7815: 1,\n",
       "         7079: 1,\n",
       "         7713: 1,\n",
       "         2217: 1,\n",
       "         8586: 1,\n",
       "         1008: 2,\n",
       "         8247: 1,\n",
       "         12221: 1,\n",
       "         2184: 1,\n",
       "         9873: 1,\n",
       "         6551: 1,\n",
       "         9359: 1,\n",
       "         4441: 1,\n",
       "         10253: 1,\n",
       "         4816: 1,\n",
       "         7403: 1,\n",
       "         9394: 1,\n",
       "         12814: 2,\n",
       "         12097: 1,\n",
       "         12835: 1,\n",
       "         1893: 1,\n",
       "         2233: 1,\n",
       "         8998: 1,\n",
       "         12056: 1,\n",
       "         4840: 1,\n",
       "         6394: 1,\n",
       "         11186: 1,\n",
       "         9851: 1,\n",
       "         1570: 1,\n",
       "         3777: 1,\n",
       "         1494: 1,\n",
       "         1178: 1,\n",
       "         3667: 1,\n",
       "         10589: 1,\n",
       "         93: 1,\n",
       "         10120: 1,\n",
       "         5082: 1,\n",
       "         6220: 1,\n",
       "         7594: 1,\n",
       "         2892: 1,\n",
       "         9661: 1,\n",
       "         10603: 1,\n",
       "         2196: 1,\n",
       "         12239: 1,\n",
       "         2366: 1,\n",
       "         1426: 1,\n",
       "         1671: 1,\n",
       "         885: 1,\n",
       "         7724: 1,\n",
       "         13207: 1,\n",
       "         4647: 1,\n",
       "         1969: 1,\n",
       "         3781: 1,\n",
       "         1142: 1,\n",
       "         2758: 1,\n",
       "         4913: 1,\n",
       "         3824: 1,\n",
       "         1853: 1,\n",
       "         9082: 1,\n",
       "         11982: 1,\n",
       "         6466: 1,\n",
       "         11250: 1,\n",
       "         12794: 1,\n",
       "         11882: 1,\n",
       "         10983: 1,\n",
       "         11410: 1,\n",
       "         6337: 1,\n",
       "         9918: 1,\n",
       "         7936: 1,\n",
       "         8253: 2,\n",
       "         7872: 1,\n",
       "         10699: 1,\n",
       "         2965: 1,\n",
       "         4386: 1,\n",
       "         5665: 1,\n",
       "         10623: 1,\n",
       "         11224: 1,\n",
       "         4989: 1,\n",
       "         5861: 1,\n",
       "         936: 1,\n",
       "         4724: 1,\n",
       "         1696: 1,\n",
       "         12411: 1,\n",
       "         3602: 1,\n",
       "         10489: 1,\n",
       "         7790: 1,\n",
       "         1755: 1,\n",
       "         12078: 1,\n",
       "         433: 1,\n",
       "         12557: 1,\n",
       "         4453: 1,\n",
       "         7610: 1,\n",
       "         9881: 2,\n",
       "         10141: 1,\n",
       "         10721: 2,\n",
       "         3398: 1,\n",
       "         11374: 1,\n",
       "         6215: 1,\n",
       "         5832: 1,\n",
       "         4472: 1,\n",
       "         7192: 1,\n",
       "         5156: 1,\n",
       "         7443: 1,\n",
       "         7514: 1,\n",
       "         5496: 1,\n",
       "         7550: 1,\n",
       "         8180: 1,\n",
       "         857: 1,\n",
       "         9882: 1,\n",
       "         7575: 1,\n",
       "         8337: 1,\n",
       "         8492: 1,\n",
       "         141: 1,\n",
       "         7699: 1,\n",
       "         13246: 1,\n",
       "         12476: 1,\n",
       "         1355: 1,\n",
       "         10008: 1,\n",
       "         10251: 1,\n",
       "         13266: 1,\n",
       "         7703: 1,\n",
       "         5018: 1,\n",
       "         8345: 1,\n",
       "         9719: 1,\n",
       "         9443: 1,\n",
       "         3673: 1,\n",
       "         6403: 1,\n",
       "         12085: 1,\n",
       "         5785: 1,\n",
       "         1613: 1,\n",
       "         701: 1,\n",
       "         7465: 1,\n",
       "         10111: 1,\n",
       "         8670: 1,\n",
       "         3565: 1,\n",
       "         5902: 1,\n",
       "         10668: 1,\n",
       "         9754: 1,\n",
       "         1979: 1,\n",
       "         10286: 1,\n",
       "         4022: 1,\n",
       "         10938: 1,\n",
       "         752: 1,\n",
       "         7830: 1,\n",
       "         8484: 1,\n",
       "         2495: 1,\n",
       "         4166: 1,\n",
       "         5432: 1,\n",
       "         7111: 1,\n",
       "         10047: 1,\n",
       "         13075: 1,\n",
       "         11816: 1,\n",
       "         7193: 1,\n",
       "         12706: 1,\n",
       "         5410: 1,\n",
       "         8897: 1,\n",
       "         3646: 1,\n",
       "         6198: 1,\n",
       "         1283: 1,\n",
       "         3005: 1,\n",
       "         11019: 1,\n",
       "         5597: 1,\n",
       "         6014: 1,\n",
       "         11270: 1,\n",
       "         6839: 1,\n",
       "         8876: 1,\n",
       "         2923: 1,\n",
       "         10294: 1,\n",
       "         10869: 1,\n",
       "         12891: 1,\n",
       "         10118: 1,\n",
       "         10224: 1,\n",
       "         3461: 1,\n",
       "         11802: 1,\n",
       "         12467: 1,\n",
       "         12522: 1,\n",
       "         7633: 1,\n",
       "         783: 1,\n",
       "         1997: 1,\n",
       "         10870: 1,\n",
       "         13089: 1,\n",
       "         12763: 1,\n",
       "         10793: 1,\n",
       "         12191: 1,\n",
       "         12408: 1,\n",
       "         6213: 1,\n",
       "         11573: 1,\n",
       "         10557: 1,\n",
       "         6753: 1,\n",
       "         116: 1,\n",
       "         6172: 1,\n",
       "         12579: 1,\n",
       "         5871: 1,\n",
       "         11513: 1,\n",
       "         10103: 1,\n",
       "         12603: 1,\n",
       "         5985: 1,\n",
       "         5542: 1,\n",
       "         5841: 1,\n",
       "         9365: 1,\n",
       "         6699: 1,\n",
       "         10616: 1,\n",
       "         2310: 1,\n",
       "         4258: 1,\n",
       "         10400: 1,\n",
       "         3065: 1,\n",
       "         686: 1,\n",
       "         5444: 1,\n",
       "         1678: 1,\n",
       "         6349: 1,\n",
       "         11588: 1,\n",
       "         3042: 1,\n",
       "         12648: 1,\n",
       "         7714: 1,\n",
       "         3357: 1,\n",
       "         2775: 1,\n",
       "         10769: 1,\n",
       "         10468: 1,\n",
       "         10643: 1,\n",
       "         13066: 1,\n",
       "         1109: 1,\n",
       "         13185: 1,\n",
       "         11474: 1,\n",
       "         264: 1,\n",
       "         12396: 1,\n",
       "         5003: 1,\n",
       "         10617: 1,\n",
       "         10717: 1,\n",
       "         7022: 1,\n",
       "         4440: 1,\n",
       "         5124: 1,\n",
       "         7401: 1,\n",
       "         2526: 1,\n",
       "         1818: 1,\n",
       "         3610: 1,\n",
       "         7296: 1,\n",
       "         8485: 1,\n",
       "         2804: 1,\n",
       "         8065: 1,\n",
       "         4712: 1,\n",
       "         11277: 1,\n",
       "         7099: 1,\n",
       "         8039: 1,\n",
       "         9732: 1,\n",
       "         196: 1,\n",
       "         7102: 1,\n",
       "         8635: 1,\n",
       "         1432: 1,\n",
       "         1774: 1,\n",
       "         5782: 1,\n",
       "         3871: 1,\n",
       "         10291: 1,\n",
       "         4976: 1,\n",
       "         13028: 1,\n",
       "         11234: 1,\n",
       "         6393: 1,\n",
       "         8223: 1,\n",
       "         11524: 1,\n",
       "         10987: 1,\n",
       "         4150: 1,\n",
       "         9802: 1,\n",
       "         2904: 1,\n",
       "         5163: 1,\n",
       "         2471: 1,\n",
       "         9429: 1,\n",
       "         3162: 1,\n",
       "         7070: 1,\n",
       "         3889: 1,\n",
       "         10598: 1,\n",
       "         4477: 1,\n",
       "         13204: 1,\n",
       "         4185: 1,\n",
       "         5363: 1,\n",
       "         12748: 1,\n",
       "         12947: 1,\n",
       "         1360: 1,\n",
       "         5109: 1,\n",
       "         10510: 1,\n",
       "         6500: 1,\n",
       "         8949: 1,\n",
       "         2840: 1,\n",
       "         7065: 1,\n",
       "         6401: 1,\n",
       "         11184: 1,\n",
       "         3098: 1,\n",
       "         1800: 1,\n",
       "         11447: 1,\n",
       "         10564: 1,\n",
       "         5580: 1,\n",
       "         12912: 1,\n",
       "         7217: 1,\n",
       "         5890: 1,\n",
       "         11130: 1,\n",
       "         6006: 1,\n",
       "         233: 1,\n",
       "         373: 1,\n",
       "         8186: 1,\n",
       "         2628: 1,\n",
       "         11689: 1,\n",
       "         8468: 1,\n",
       "         7685: 1,\n",
       "         243: 1,\n",
       "         11341: 1,\n",
       "         3302: 1,\n",
       "         1322: 1,\n",
       "         3523: 1,\n",
       "         9531: 1,\n",
       "         6513: 1,\n",
       "         5121: 1,\n",
       "         8353: 1,\n",
       "         12336: 1,\n",
       "         7720: 1,\n",
       "         6716: 1,\n",
       "         7289: 1,\n",
       "         7316: 1,\n",
       "         10953: 1,\n",
       "         11078: 1,\n",
       "         4606: 1,\n",
       "         9540: 1,\n",
       "         11478: 1,\n",
       "         1384: 1,\n",
       "         10593: 1,\n",
       "         12305: 1,\n",
       "         9276: 1,\n",
       "         7323: 1,\n",
       "         4378: 1,\n",
       "         126: 1,\n",
       "         13227: 1,\n",
       "         3563: 1,\n",
       "         10685: 1,\n",
       "         6809: 1,\n",
       "         13181: 1,\n",
       "         11667: 1,\n",
       "         1392: 1,\n",
       "         7183: 1,\n",
       "         9902: 1,\n",
       "         10282: 1,\n",
       "         4721: 1,\n",
       "         12458: 1,\n",
       "         9650: 1,\n",
       "         11845: 1,\n",
       "         10705: 1,\n",
       "         6246: 1,\n",
       "         6713: 1,\n",
       "         5551: 1,\n",
       "         10949: 1,\n",
       "         4436: 1,\n",
       "         379: 1,\n",
       "         9270: 1,\n",
       "         6478: 1,\n",
       "         12698: 1,\n",
       "         10470: 1,\n",
       "         12779: 1,\n",
       "         501: 1,\n",
       "         11262: 1,\n",
       "         5395: 1,\n",
       "         6018: 1,\n",
       "         10194: 1,\n",
       "         11662: 1,\n",
       "         1746: 1,\n",
       "         1499: 1,\n",
       "         3447: 1,\n",
       "         9226: 2,\n",
       "         3409: 1,\n",
       "         11312: 1,\n",
       "         6324: 1,\n",
       "         151: 1,\n",
       "         2444: 1,\n",
       "         2654: 1,\n",
       "         6613: 1,\n",
       "         12403: 1,\n",
       "         7551: 1,\n",
       "         932: 1,\n",
       "         306: 1,\n",
       "         5064: 1,\n",
       "         1253: 1,\n",
       "         5562: 1,\n",
       "         5718: 1,\n",
       "         12966: 1,\n",
       "         72: 1,\n",
       "         9521: 1,\n",
       "         5668: 1,\n",
       "         5095: 1,\n",
       "         2593: 1,\n",
       "         10892: 1,\n",
       "         7535: 1,\n",
       "         2658: 1,\n",
       "         4450: 1,\n",
       "         10734: 1,\n",
       "         1213: 1,\n",
       "         12269: 1,\n",
       "         1279: 1,\n",
       "         3577: 1,\n",
       "         1443: 1,\n",
       "         4138: 1,\n",
       "         12494: 1,\n",
       "         9256: 1,\n",
       "         8060: 1,\n",
       "         2551: 1,\n",
       "         4772: 1,\n",
       "         10075: 1,\n",
       "         11134: 1,\n",
       "         770: 1,\n",
       "         12409: 1,\n",
       "         11120: 1,\n",
       "         2209: 1,\n",
       "         13110: 1,\n",
       "         4522: 1,\n",
       "         2266: 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test quality\n",
    "\n",
    "import collections\n",
    "\n",
    "ranks = []\n",
    "for doc_id in range(len(train_stemmed_text)):\n",
    "    inferred_vector = doc2vec_vect.vect_model.infer_vector(train_stemmed_text[train_stemmed_text.index[doc_id]])\n",
    "    sims = doc2vec_vect.vect_model.docvecs.most_similar([inferred_vector], topn=len(doc2vec_vect.vect_model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "collections.Counter(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, naive_bayes, metrics, svm\n",
    "\n",
    "def show_score(classifier_name, score):\n",
    "    print(\"Accuracy:%0.2f Precission:%0.2f Recall:%0.2f F1:%0.2f\"%scores, \"-> [%s]\"%(classifier_name))\n",
    "    \n",
    "def train_model(classifier, train_feature, train_label, test_feature, test_label, is_neural_net=False):\n",
    "#     print(train_feature)\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(train_feature, train_label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(test_feature)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return (metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))\n",
    "\n",
    "def run_model(classfier_configs):\n",
    "    for _name in classfier_configs:\n",
    "#         print(classfier_configs[_name].train)\n",
    "        scores = train_model(classfier_configs[_name][0], classfier_configs[_name][1], train_label, classfier_configs[_name][2], test_label)\n",
    "        show_score(_name, scores)\n",
    "        \n",
    "class ClassifierConfig:\n",
    "     def __init__(self, classifier, train, test):\n",
    "            self.classifier = classifier\n",
    "            self.train = train\n",
    "            self.test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.65 Precission:0.64 Recall:0.67 F1:0.65 -> [NB Count]\n",
      "Accuracy:0.62 Precission:0.57 Recall:0.66 F1:0.61 -> [NB TFIDF]\n",
      "Accuracy:0.62 Precission:0.60 Recall:0.65 F1:0.62 -> [NB TFIDF NGram]\n",
      "Accuracy:0.62 Precission:0.38 Recall:0.79 F1:0.51 -> [NB TFIDF NGram Chars]\n",
      "Accuracy:0.65 Precission:0.41 Recall:0.83 F1:0.55 -> [NB LDA]\n"
     ]
    }
   ],
   "source": [
    "nb_classifiers = {\n",
    "    \"NB Count\": (naive_bayes.MultinomialNB(), train_count, test_count),\n",
    "    \"NB TFIDF\": (naive_bayes.MultinomialNB(), train_tfidf, test_tfidf),\n",
    "    \"NB TFIDF NGram\": (naive_bayes.MultinomialNB(), train_tfidf_ngram, test_tfidf_ngram),\n",
    "    \"NB TFIDF NGram Chars\": (naive_bayes.MultinomialNB(), train_tfidf_ngram_chars, test_tfidf_ngram_chars),\n",
    "    \"NB LDA\": (naive_bayes.MultinomialNB(), train_lda, test_lda),\n",
    "}\n",
    "\n",
    "for name in nb_classifiers:\n",
    "    scores = train_model(nb_classifiers[name][0], nb_classifiers[name][1], train_label, nb_classifiers[name][2], test_label)\n",
    "    show_score(name, scores)\n",
    "# run_model(nb_classifiers)\n",
    "\n",
    "# print(train_model(naive_bayes.MultinomialNB(), train_count, train_label, test_count, test_label))\n",
    "# print(train_model(naive_bayes.MultinomialNB(), train_tfidf_ngram, train_label, test_tfidf_ngram, test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.54 Precission:0.40 Recall:0.59 F1:0.47 -> [Linear Count]\n",
      "Accuracy:0.65 Precission:0.55 Recall:0.73 F1:0.63 -> [Linear TFIDF]\n",
      "Accuracy:0.60 Precission:0.53 Recall:0.65 F1:0.58 -> [Linear TFIDF NGram]\n",
      "Accuracy:0.65 Precission:0.59 Recall:0.69 F1:0.64 -> [Linear TFIDF NGram Chars]\n",
      "Accuracy:0.59 Precission:0.57 Recall:0.62 F1:0.59 -> [Linear FastText Embedding]\n",
      "Accuracy:0.65 Precission:0.41 Recall:0.83 F1:0.55 -> [Linear LDA]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.50 Precission:0.48 Recall:0.53 F1:0.50 -> [Linear Doc2Vec]\n",
      "Accuracy:0.50 Precission:0.48 Recall:0.53 F1:0.50 -> [Linear Count]\n",
      "Accuracy:0.50 Precission:0.48 Recall:0.53 F1:0.50 -> [Linear TFIDF]\n",
      "Accuracy:0.50 Precission:0.48 Recall:0.53 F1:0.50 -> [Linear TFIDF NGram]\n",
      "Accuracy:0.50 Precission:0.48 Recall:0.53 F1:0.50 -> [Linear TFIDF NGram Chars]\n",
      "Accuracy:0.50 Precission:0.48 Recall:0.53 F1:0.50 -> [Linear FastText Embedding]\n",
      "Accuracy:0.50 Precission:0.48 Recall:0.53 F1:0.50 -> [Linear LDA]\n",
      "Accuracy:0.50 Precission:0.48 Recall:0.53 F1:0.50 -> [Linear Doc2Vec]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "linear_classifiers = {\n",
    "    \"Linear Count\": (linear_model.LogisticRegression(), train_count, test_count),\n",
    "    \"Linear TFIDF\": (linear_model.LogisticRegression(), train_tfidf, test_tfidf),\n",
    "    \"Linear TFIDF NGram\": (linear_model.LogisticRegression(), train_tfidf_ngram, test_tfidf_ngram),\n",
    "    \"Linear TFIDF NGram Chars\": (linear_model.LogisticRegression(), train_tfidf_ngram_chars, test_tfidf_ngram_chars),\n",
    "    \"Linear FastText Embedding\": (linear_model.LogisticRegression(), train_fasttext_embedding, test_fasttext_embedding),\n",
    "    \"Linear LDA\": (linear_model.LogisticRegression(), train_lda, test_lda),\n",
    "    \"Linear Doc2Vec\": (linear_model.LogisticRegression(), train_doc2vec, test_doc2vec),\n",
    "}\n",
    "\n",
    "for name in linear_classifiers:\n",
    "    scores = train_model(linear_classifiers[name][0], linear_classifiers[name][1], train_label, linear_classifiers[name][2], test_label)\n",
    "    show_score(name, scores)\n",
    "run_model(linear_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifiers = {\n",
    "    \"SVM Count\": (svm.SVC(), train_count, test_count),\n",
    "    \"SVM TFIDF\": (svm.SVC(), train_tfidf, test_tfidf),\n",
    "    \"SVM TFIDF NGram\": (svm.SVC(), train_tfidf_ngram, test_tfidf_ngram),\n",
    "    \"SVM TFIDF NGram Chars\": (svm.SVC(), train_tfidf_ngram_chars, test_tfidf_ngram_chars),\n",
    "    \"SVM FastText Embedding\": (svm.SVC(), train_fasttext_embedding, test_fasttext_embedding),\n",
    "    \"SVM LDA\": (svm.SVC(), train_lda, test_lda),\n",
    "    \"SVM Doc2Vec\": (svm.SVC(), train_doc2vec, test_doc2vec),\n",
    "}\n",
    "\n",
    "run_model(svm_classifiers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

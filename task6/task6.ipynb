{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages:\n",
    "* [NLTK](http://www.nltk.org/howto/classify.html)\n",
    "* [SpaCy](https://spacy.io/)\n",
    "* [AllenNLP](https://allennlp.org/tutorials)\n",
    "\n",
    "Articles:\n",
    "* [A Comprehensive Guide to Understand and Implement Text Classification in Python](https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/)\n",
    "* [Machine Learning, NLP: Text Classification using scikit-learn, python and NLTK](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a)\n",
    "* [State-of-the-Art Text Classification using BERT model: “Predict the Happiness” Challenge](https://appliedmachinelearning.blog/2019/03/04/state-of-the-art-text-classification-using-bert-model-predict-the-happiness-hackerearth-challenge/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13299/13299 [03:49<00:00, 58.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    result_stemmed = []\n",
    "    for token in simple_preprocess(text, min_len = 2):\n",
    "        result.append(token)\n",
    "        if token not in STOPWORDS:\n",
    "            result_stemmed.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return (result, result_stemmed)\n",
    "\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat.labels\") as f:\n",
    "    LABELS = [int(l) for l in f.readlines() if l[0].isdigit()]\n",
    "\n",
    "ALL_RAW_TEXTS = []\n",
    "ALL_TEXTS = []\n",
    "ALL_STEMMED_TEXTS = []\n",
    "ALL_CONCAT_STEMMED_TEXTS = []\n",
    "LABELED_TEXTS = []\n",
    "LABELED_CONCAT_TEXTS = []\n",
    "LABELED_STEMMED_TEXTS = []\n",
    "LABELED_CONCAT_STEMMED_TEXTS = []\n",
    "\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat\") as f:\n",
    "    ALL_RAW_TEXTS = f.readlines()\n",
    "\n",
    "for _text in tqdm(ALL_RAW_TEXTS):\n",
    "    _result, _result_stemmed = preprocess(_text)\n",
    "    ALL_TEXTS.append(_result)\n",
    "    ALL_STEMMED_TEXTS.append(_result_stemmed)\n",
    "\n",
    "ALL_CONCAT_STEMMED_TEXTS = [\" \".join(_text) for _text in ALL_STEMMED_TEXTS]\n",
    "\n",
    "LABELED_TEXTS = ALL_TEXTS[0:len(LABELS)]\n",
    "LABELED_CONCAT_TEXTS = [\" \".join(_text) for _text in LABELED_TEXTS]\n",
    "\n",
    "LABELED_STEMMED_TEXTS = ALL_STEMMED_TEXTS[0:len(LABELS)]\n",
    "LABELED_CONCAT_STEMMED_TEXTS = [\" \".join(_text) for _text in LABELED_STEMMED_TEXTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handle Extra Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_map = 'abcdefghij'\n",
    "# code_map = '0123456789'\n",
    "\n",
    "def encode_zipcode(zip_code):\n",
    "    _code = ''\n",
    "    _rest = zip_code    \n",
    "    while _rest > 0:\n",
    "        _code = code_map[_rest % 10] + _code\n",
    "        _rest = int(_rest / 10)\n",
    "\n",
    "    return _code\n",
    "\n",
    "FEATURE_MORE =pd.read_csv(\"./data/raw/Hygiene/hygiene.dat.additional\", header=None)\n",
    "EXTRA_FEATURE = pd.DataFrame()\n",
    "\n",
    "EXTRA_FEATURE['Cuisines'] = [simple_preprocess(_text) for _text in FEATURE_MORE[0]]\n",
    "\n",
    "EXTRA_FEATURE['Stars'] = ['PoorStars' for _star in FEATURE_MORE[3]]\n",
    "star_std_range = (FEATURE_MORE[3].mean() - FEATURE_MORE[3].std(), FEATURE_MORE[3].mean() + FEATURE_MORE[3].std())\n",
    "# EXTRA_FEATURE['Stars'][FEATURE_MORE[3] < star_std_range[0]] = 'PoorStars'\n",
    "EXTRA_FEATURE['Stars'][(FEATURE_MORE[3] >= star_std_range[0])] = 'StandardStars'\n",
    "EXTRA_FEATURE['Stars'][FEATURE_MORE[3] > star_std_range[1]] = 'GoodStars'\n",
    "\n",
    "EXTRA_FEATURE['ReviewCount'] = ['NoReviews' for _star in FEATURE_MORE[2]]\n",
    "EXTRA_FEATURE['ReviewCount'][FEATURE_MORE[2] > 2] = \"FewReviews\"\n",
    "EXTRA_FEATURE['ReviewCount'][FEATURE_MORE[2] > 6] = \"SomeReviews\"\n",
    "EXTRA_FEATURE['ReviewCount'][FEATURE_MORE[2] > 13] = \"ManyReviews\"\n",
    "EXTRA_FEATURE['ReviewCount'][FEATURE_MORE[2] > 50] = \"LotReviews\"\n",
    "\n",
    "EXTRA_FEATURE['ZIPCode'] = [encode_zipcode(_code) for _code in FEATURE_MORE[1]]\n",
    "\n",
    "EXTRA_FEATURE['MergedText'] = [ [_cuisine for _cuisine in _record[1][0]]\n",
    "                                + [_record[1][1]] + [_record[1][2]] + [_record[1][3]]\n",
    "                               for _record in EXTRA_FEATURE.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(analyzer='word')\n",
    "\n",
    "vect.fit(FEATURE_MORE[0].replace('Restaurants', '', regex=True))\n",
    "CUISINES_FEATURE = vect.transform(FEATURE_MORE[0]).todense()\n",
    "\n",
    "_zipcode = [str(i) for i in FEATURE_MORE[1]]\n",
    "vect.fit(_zipcode)\n",
    "ZIPCODE_FEATURE = vect.transform(_zipcode).todense()\n",
    "\n",
    "REVIEW_COUNT_FEATURE = np.array(FEATURE_MORE[2]).reshape((len(FEATURE_MORE[2]),1))\n",
    "RATING_FEATURE = np.array(FEATURE_MORE[3]).reshape((len(FEATURE_MORE[3]),1))\n",
    "\n",
    "ALL_EXTRA_FEATURE = np.hstack((CUISINES_FEATURE, ZIPCODE_FEATURE, REVIEW_COUNT_FEATURE, RATING_FEATURE))\n",
    "COUSINE_ZIPCODE_EXTRA_FEATURE = np.hstack((CUISINES_FEATURE, ZIPCODE_FEATURE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Extra Features to the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA_FEATURES = EXTRA_FEATURE['MergedText'][0:len(LABELS)]\n",
    "# EXTRA_CONCAT_FEATURES = [\" \".join(_text) for _text in EXTRA_FEATURES]\n",
    "                  \n",
    "# LABELED_EXTRA_TEXTS =  [EXTRA_FEATURES[i] + _text for i, _text in enumerate(LABELED_TEXTS)]\n",
    "# LABELED_EXTRA_CONCAT_TEXTS = [\" \".join(_text) for _text in LABELED_EXTRA_TEXTS]\n",
    "\n",
    "# LABELED_EXTRA_STEMMED_TEXTS = [LABELED_EXTRA_TEXTS[i] + _text for i, _text in enumerate(LABELED_STEMMED_TEXTS)]\n",
    "# LABELED_EXTRA_CONCAT_STEMMED_TEXTS = [\" \".join(_text) for _text in LABELED_EXTRA_STEMMED_TEXTS]\n",
    "\n",
    "# LABLED_RAW_TEXTS_1 = [EXTRA_CONCAT_FEATURES[i] + \"|\" + _text for i, _text in enumerate(ALL_RAW_TEXTS[0:len(LABELS)]) if LABELS[i] == 1]\n",
    "# LABLED_RAW_TEXTS_0 = [EXTRA_CONCAT_FEATURES[i] + \"|\" + _text for i, _text in enumerate(ALL_RAW_TEXTS[0:len(LABELS)]) if LABELS[i] == 0]\n",
    "\n",
    "# SMALL_RAW_TEXTS_1 = [_text for _text in LABLED_RAW_TEXTS_1 if len(_text) < 1000]\n",
    "# SMALL_RAW_TEXTS_0 = [_text for _text in LABLED_RAW_TEXTS_0 if len(_text) < 1000]\n",
    "# print(len(SMALL_RAW_TEXTS_1), len(SMALL_RAW_TEXTS_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe using texts and lables\n",
    "labeled_df = pd.DataFrame()\n",
    "labeled_df['concat_stemmed_text'] = LABELED_CONCAT_STEMMED_TEXTS\n",
    "labeled_df['stemmed_text'] = LABELED_STEMMED_TEXTS\n",
    "# labeled_df['concat_stemmed_text'] = LABELED_CONCAT_TEXTS\n",
    "# labeled_df['stemmed_text'] = LABELED_TEXTS\n",
    "# labeled_df['concat_stemmed_text'] = LABELED_CONCAT_EXTRA_TEXTS\n",
    "# labeled_df['stemmed_text'] = LABELED_EXTRA_TEXTS\n",
    "labeled_df['label'] = LABELS\n",
    "labeled_extra_all = ALL_EXTRA_FEATURE[0:len(LABELS)]\n",
    "labeled_extra_cuisine_zipcode = COUSINE_ZIPCODE_EXTRA_FEATURE[0:len(LABELS)]\n",
    "\n",
    "# split the dataset into training and validation datasets \n",
    "train_concat_stemmed_text, test_concat_stemmed_text, train_label, test_label = model_selection.train_test_split(labeled_df['concat_stemmed_text'], \n",
    "                                                                                  labeled_df['label'],\n",
    "                                                                                  test_size = 0.2,\n",
    "                                                                                  random_state = 10)\n",
    "train_stemmed_text = labeled_df['stemmed_text'][train_concat_stemmed_text.index]\n",
    "test_stemmed_text = labeled_df['stemmed_text'][test_concat_stemmed_text.index]\n",
    "\n",
    "# # label encode the target variable \n",
    "# encoder = preprocessing.LabelEncoder()\n",
    "# train_label = encoder.fit_transform(train_label)\n",
    "# test_label = encoder.fit_transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = corpora.Dictionary(processed_docs)\n",
    "# print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "# dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "# print(\"After prunn:%d\"%(len(dictionary)))\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCountVectorizer:\n",
    "    def __init__(self, max_df = 0.5):\n",
    "        self.vect_model = CountVectorizer(analyzer='word', max_df = max_df)\n",
    "        \n",
    "    def fit(self, texts_concat):\n",
    "        self.vect_model.fit(texts_concat)\n",
    "    \n",
    "    def transform(self, texts_concat):\n",
    "        return self.vect_model.transform(texts_concat)\n",
    "\n",
    "count_vect = MyCountVectorizer()\n",
    "count_vect.fit(train_concat_stemmed_text)\n",
    "train_count = count_vect.transform(train_concat_stemmed_text)\n",
    "test_count =  count_vect.transform(test_concat_stemmed_text)\n",
    "labeled_count = count_vect.transform(LABELED_CONCAT_STEMMED_TEXTS)\n",
    "labeled_count_extra_all = np.hstack([labeled_count.todense(), labeled_extra_all])\n",
    "labeled_count_extra_cuisine_zipcode = np.hstack([labeled_count.todense(), labeled_extra_cuisine_zipcode])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 421 ms, total: 13 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# word level tf-idf\n",
    "class MyTfidfVectorizer(MyCountVectorizer):\n",
    "    def __init__(self, analyzer='word', ngram_range = None, max_features=5000):\n",
    "        if ngram_range is None:\n",
    "            self.vect_model = TfidfVectorizer(analyzer = analyzer, max_features = max_features)\n",
    "        else:\n",
    "            self.vect_model = TfidfVectorizer(analyzer = analyzer, ngram_range = ngram_range, \n",
    "                                              max_features = max_features)\n",
    "            \n",
    "    def fit(self, texts_concat):   \n",
    "        self.vect_model.fit(texts_concat)\n",
    "        self.vocabulary = self.vect_model.vocabulary_\n",
    "\n",
    "\n",
    "tfidf_vect = MyTfidfVectorizer(max_features = 10000)\n",
    "tfidf_vect.fit(train_concat_stemmed_text)\n",
    "train_tfidf =  tfidf_vect.transform(train_concat_stemmed_text)\n",
    "test_tfidf =  tfidf_vect.transform(test_concat_stemmed_text)\n",
    "labeled_tfidf =  tfidf_vect.transform(LABELED_CONCAT_STEMMED_TEXTS)\n",
    "labeled_tfidf_extra_all = np.hstack([labeled_tfidf.todense(), labeled_extra_all])\n",
    "labeled_tfidf_extra_cuisine_zipcode = np.hstack([labeled_tfidf.todense(), labeled_extra_cuisine_zipcode])\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = MyTfidfVectorizer(ngram_range=(2,3), max_features = 10000)\n",
    "tfidf_vect_ngram.fit(train_concat_stemmed_text)\n",
    "train_tfidf_ngram =  tfidf_vect_ngram.transform(train_concat_stemmed_text)\n",
    "test_tfidf_ngram =  tfidf_vect_ngram.transform(test_concat_stemmed_text)\n",
    "labeled_tfidf_ngram = tfidf_vect_ngram.transform(LABELED_CONCAT_STEMMED_TEXTS)\n",
    "labeled_tfidf_ngram_extra_all = np.hstack([labeled_tfidf_ngram.todense(), labeled_extra_all])\n",
    "labeled_tfidf_ngram_extra_cuisine_zipcode = np.hstack([labeled_tfidf_ngram.todense(), labeled_extra_cuisine_zipcode])\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = MyTfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features = 10000)\n",
    "tfidf_vect_ngram_chars.fit(train_concat_stemmed_text)\n",
    "train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_concat_stemmed_text) \n",
    "test_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_concat_stemmed_text)\n",
    "labeled_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(LABELED_CONCAT_STEMMED_TEXTS)\n",
    "labeled_tfidf_ngram_chars_extra_all = np.hstack([labeled_tfidf_ngram_chars.todense(), labeled_extra_all])\n",
    "labeled_tfidf_ngram_chars_extra_cuisine_zipcode = np.hstack([labeled_tfidf_ngram_chars.todense(), labeled_extra_cuisine_zipcode])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build from review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from gensim.models.fasttext import FastText\n",
    "\n",
    "# class MyFastTextTfidfVectorizer(MyCountVectorizer):\n",
    "#     def __init__(self, tfidf_vectorizer, size = 100):\n",
    "#         self.embedding_size = size\n",
    "#         self.tfidf_vectorizer = tfidf_vectorizer\n",
    "#         self.fasttext_model = FastText(size = size, window = 5, min_count = 5)\n",
    "\n",
    "#     def tfidf2embedding(self, value_vector):\n",
    "#         _weighted_value = np.zeros(self.embedding_size)\n",
    "#         for key in self.tfidf_vectorizer.vocabulary:\n",
    "#             _index = self.tfidf_vectorizer.vocabulary[key]\n",
    "#             if value_vector[_index] != 0:\n",
    "#                 _weighted_value += self.fasttext_model[key] * value_vector[_index]\n",
    "\n",
    "#         return _weighted_value\n",
    "    \n",
    "#     def fit(self, texts):\n",
    "#         _texts_concat = [\" \".join(_text) for _text in texts]\n",
    "#         self.tfidf_vectorizer = MyTfidfVectorizer()\n",
    "#         self.tfidf_vectorizer.fit(_texts_concat)\n",
    "        \n",
    "#         self.fasttext_model.build_vocab(sentences = texts)\n",
    "#         self.fasttext_model.train(sentences = texts, \n",
    "#                                   total_examples = len(texts), \n",
    "#                                   epochs=10)\n",
    "        \n",
    "#     def transform(self, texts):\n",
    "#         _texts_concat = [\" \".join(_text) for _text in texts]\n",
    "#         _tfidf_values = self.tfidf_vectorizer.transform(_texts_concat)\n",
    "#         return np.asarray([self.tfidf2embedding(_value.toarray()[0]) for _value in _tfidf_values])\n",
    "\n",
    "# fasttext_tfidf_vect = MyFastTextTfidfVectorizer(tfidf_vect)\n",
    "# fasttext_tfidf_vect.fit(ALL_STEMMED_TEXTS)\n",
    "# train_fasttext_embedding = fasttext_tfidf_vect.transform(train_stemmed_text)\n",
    "# test_fasttext_embedding = fasttext_tfidf_vect.transform(test_stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prebuilt Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers\n",
    "\n",
    "# # load the pre-trained word-embedding vectors \n",
    "# embeddings_index = {}\n",
    "# for i, line in enumerate(open('data/model/wiki-news-300d-1M.vec')):\n",
    "#     values = line.split()\n",
    "#     embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# # create a tokenizer \n",
    "# token = text.Tokenizer()\n",
    "# token.fit_on_texts(LABELED_CONCAT_STEMMED_TEXTS)\n",
    "# word_index = token.word_index\n",
    "\n",
    "# # convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "# text_train_seq = sequence.pad_sequences(token.texts_to_sequences(train_text), maxlen=70)\n",
    "# text_test_seq = sequence.pad_sequences(token.texts_to_sequences(test_text), maxlen=70)\n",
    "\n",
    "# # create token-embedding mapping\n",
    "# embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text / NLP based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "# trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "# trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "# trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "# trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "# trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import textblob\n",
    "\n",
    "# pos_family = {\n",
    "#     'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "#     'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "#     'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "#     'adj' :  ['JJ','JJR','JJS'],\n",
    "#     'adv' : ['RB','RBR','RBS','WRB']\n",
    "# }\n",
    "\n",
    "# # function to check and get the part of speech tag count of a words in a given sentence\n",
    "# def check_pos_tag(x, flag):\n",
    "#     cnt = 0\n",
    "#     try:\n",
    "#         wiki = textblob.TextBlob(x)\n",
    "#         for tup in wiki.tags:\n",
    "#             ppo = list(tup)[1]\n",
    "#             if ppo in pos_family[flag]:\n",
    "#                 cnt += 1\n",
    "#     except:\n",
    "#         pass\n",
    "#     return cnt\n",
    "\n",
    "# trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "# trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "# trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "# trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "# trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train a LDA Model\n",
    "# lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "# X_topics = lda_model.fit_transform(text_train_count)\n",
    "# topic_word = lda_model.components_\n",
    "# vocab = count_vect.get_feature_names()\n",
    "\n",
    "# # view the topic models\n",
    "# n_top_words = 10\n",
    "# topic_summaries = []\n",
    "# for i, topic_dist in enumerate(topic_word):\n",
    "#     topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "#     topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.99 s, sys: 275 ms, total: 5.27 s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "from gensim import corpora, models\n",
    "\n",
    "class MyLDAVectorizer(MyCountVectorizer):\n",
    "    mallet_path = \"..\" + os.sep + \"mallet-2.0.8\"+ os.sep + \"bin\" + os.sep +\"mallet\"\n",
    "    \n",
    "    def __init__(self, TOPIC_COUNT = 100):\n",
    "        self.topic_count = TOPIC_COUNT\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        self.dictionary = corpora.Dictionary(texts)\n",
    "        _corpus = [self.dictionary.doc2bow(_doc) for _doc in texts]\n",
    "        self.tfidf_model = models.TfidfModel(_corpus)\n",
    "        _tfidf_corpus = self.tfidf_model[_corpus]\n",
    "\n",
    "#         self.vect_model = models.LdaModel(_tfidf_corpus, \n",
    "#                             num_topics = self.topic_count, \n",
    "#                             id2word = self.dictionary,\n",
    "#                             random_state = 100,\n",
    "#                             eval_every = 5, \n",
    "#                             alpha = 'auto', \n",
    "#                             gamma_threshold = 0.01)\n",
    "        \n",
    "        self.vect_model = models.wrappers.LdaMallet(self.mallet_path, \n",
    "                                                     corpus = _corpus, \n",
    "                                                     num_topics = self.topic_count, \n",
    "                                                     id2word = self.dictionary)\n",
    "    \n",
    "    def toarray(self, doc_topics):\n",
    "        _doc_vect  = np.zeros((len(doc_topics), self.topic_count))\n",
    "        \n",
    "        for i, _doc in enumerate(doc_topics):\n",
    "            for _topic, _weight in _doc:\n",
    "                _doc_vect[i][_topic] = _weight\n",
    "        \n",
    "        return _doc_vect\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        _corpus = [self.dictionary.doc2bow(_doc) for _doc in texts]\n",
    "#         _tfidf_corpus = self.tfidf_model[_corpus]\n",
    "        \n",
    "        return self.toarray(self.vect_model[_corpus])\n",
    "\n",
    "lda_vect = MyLDAVectorizer(TOPIC_COUNT = 200)\n",
    "# lda_vect.fit(train_stemmed_text)\n",
    "lda_vect.fit(LABELED_STEMMED_TEXTS)\n",
    "train_lda = lda_vect.transform(train_stemmed_text)\n",
    "test_lda = lda_vect.transform(test_stemmed_text)\n",
    "labeled_lda = lda_vect.transform(LABELED_STEMMED_TEXTS)\n",
    "labeled_lda_extra_all = np.hstack([labeled_lda, labeled_extra_all])\n",
    "labeled_lda_extra_cuisine_zipcode = np.hstack([labeled_lda, labeled_extra_cuisine_zipcode])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 2s, sys: 14.8 s, total: 14min 17s\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "\n",
    "class MyDoc2Vectorizer(MyCountVectorizer):\n",
    "    def __init__(self, size = 100):\n",
    "        self.embedding_size = size\n",
    "            \n",
    "    def fit(self, texts):\n",
    "        _docs = [TaggedDocument(_doc, [i]) for i, _doc in enumerate(texts)]\n",
    "        self.vect_model = Doc2Vec(_docs, \n",
    "                                  vector_size = self.embedding_size, \n",
    "                                  window = 5,\n",
    "                                  min_count = 3,\n",
    "                                  epochs = 40, \n",
    "                                  workers = 4)\n",
    "        \n",
    "    def transform(self, texts):\n",
    "        return np.asarray([self.vect_model.infer_vector(_text) for _text in texts])\n",
    "\n",
    "doc2vec_vect = MyDoc2Vectorizer(size = 200)\n",
    "doc2vec_vect.fit(ALL_STEMMED_TEXTS)\n",
    "# doc2vec_vect.fit(train_stemmed_text)\n",
    "train_doc2vec = doc2vec_vect.transform(train_stemmed_text)\n",
    "test_doc2vec = doc2vec_vect.transform(test_stemmed_text)\n",
    "labeled_doc2vec = doc2vec_vect.transform(LABELED_STEMMED_TEXTS)\n",
    "labeled_doc2vec_extra_all = np.hstack([labeled_doc2vec, labeled_extra_all])\n",
    "labeled_doc2vec_extra_cuisine_zipcode = np.hstack([labeled_doc2vec, labeled_extra_cuisine_zipcode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({11596: 1,\n",
       "         8485: 1,\n",
       "         3093: 1,\n",
       "         221: 1,\n",
       "         8521: 2,\n",
       "         1237: 1,\n",
       "         5879: 1,\n",
       "         6731: 1,\n",
       "         1611: 1,\n",
       "         12589: 1,\n",
       "         5535: 1,\n",
       "         5: 1,\n",
       "         5310: 1,\n",
       "         10324: 1,\n",
       "         7754: 1,\n",
       "         4890: 1,\n",
       "         391: 1,\n",
       "         1976: 1,\n",
       "         11106: 1,\n",
       "         2432: 1,\n",
       "         1014: 1,\n",
       "         9043: 1,\n",
       "         8783: 1,\n",
       "         3411: 1,\n",
       "         3377: 1,\n",
       "         8359: 1,\n",
       "         4268: 1,\n",
       "         1599: 1,\n",
       "         9125: 1,\n",
       "         1594: 1,\n",
       "         8707: 1,\n",
       "         10676: 1,\n",
       "         11748: 1,\n",
       "         8219: 1,\n",
       "         3273: 1,\n",
       "         3302: 1,\n",
       "         4692: 1,\n",
       "         7485: 1,\n",
       "         7201: 1,\n",
       "         7834: 1,\n",
       "         8897: 1,\n",
       "         7142: 1,\n",
       "         610: 1,\n",
       "         6360: 1,\n",
       "         10809: 1,\n",
       "         9684: 1,\n",
       "         13025: 1,\n",
       "         8871: 1,\n",
       "         12817: 1,\n",
       "         8566: 1,\n",
       "         12674: 1,\n",
       "         9265: 1,\n",
       "         633: 1,\n",
       "         9753: 1,\n",
       "         7830: 1,\n",
       "         3535: 1,\n",
       "         11137: 1,\n",
       "         12228: 1,\n",
       "         3333: 1,\n",
       "         4304: 1,\n",
       "         5578: 1,\n",
       "         8977: 1,\n",
       "         6740: 1,\n",
       "         3404: 1,\n",
       "         9601: 1,\n",
       "         8850: 1,\n",
       "         2730: 1,\n",
       "         5935: 1,\n",
       "         4790: 1,\n",
       "         11518: 1,\n",
       "         7195: 1,\n",
       "         5218: 1,\n",
       "         1998: 1,\n",
       "         3915: 1,\n",
       "         1324: 1,\n",
       "         5888: 1,\n",
       "         9573: 1,\n",
       "         7388: 1,\n",
       "         11416: 1,\n",
       "         944: 1,\n",
       "         4322: 1,\n",
       "         5115: 1,\n",
       "         6383: 1,\n",
       "         6559: 1,\n",
       "         4167: 1,\n",
       "         6571: 1,\n",
       "         11598: 1,\n",
       "         12218: 1,\n",
       "         12805: 1,\n",
       "         696: 1,\n",
       "         6451: 1,\n",
       "         825: 1,\n",
       "         11158: 1,\n",
       "         7173: 1,\n",
       "         7712: 1,\n",
       "         11247: 1,\n",
       "         10108: 1,\n",
       "         4125: 1,\n",
       "         11496: 1,\n",
       "         10114: 1,\n",
       "         96: 1,\n",
       "         2855: 1,\n",
       "         91: 1,\n",
       "         10652: 1,\n",
       "         9579: 1,\n",
       "         5565: 1,\n",
       "         9913: 1,\n",
       "         4406: 1,\n",
       "         12640: 1,\n",
       "         7109: 1,\n",
       "         1247: 1,\n",
       "         3913: 1,\n",
       "         3830: 1,\n",
       "         6782: 1,\n",
       "         6448: 1,\n",
       "         4139: 1,\n",
       "         3890: 1,\n",
       "         3470: 1,\n",
       "         5011: 1,\n",
       "         4965: 1,\n",
       "         5394: 1,\n",
       "         11530: 1,\n",
       "         538: 1,\n",
       "         6657: 1,\n",
       "         5808: 1,\n",
       "         2929: 1,\n",
       "         1647: 1,\n",
       "         8398: 1,\n",
       "         962: 1,\n",
       "         4751: 1,\n",
       "         843: 1,\n",
       "         11808: 1,\n",
       "         11864: 1,\n",
       "         5288: 1,\n",
       "         1462: 1,\n",
       "         1302: 1,\n",
       "         2221: 1,\n",
       "         9702: 2,\n",
       "         3575: 1,\n",
       "         7153: 1,\n",
       "         4448: 1,\n",
       "         5319: 1,\n",
       "         8029: 1,\n",
       "         12599: 1,\n",
       "         6573: 1,\n",
       "         2218: 1,\n",
       "         11047: 1,\n",
       "         1797: 1,\n",
       "         5481: 1,\n",
       "         7098: 1,\n",
       "         8206: 1,\n",
       "         9605: 1,\n",
       "         3994: 1,\n",
       "         13071: 2,\n",
       "         9249: 1,\n",
       "         9048: 1,\n",
       "         9283: 1,\n",
       "         4867: 1,\n",
       "         2509: 1,\n",
       "         2709: 1,\n",
       "         3450: 1,\n",
       "         3034: 1,\n",
       "         3089: 1,\n",
       "         12062: 1,\n",
       "         3903: 1,\n",
       "         7775: 1,\n",
       "         11506: 1,\n",
       "         3922: 1,\n",
       "         12149: 1,\n",
       "         6665: 1,\n",
       "         11178: 1,\n",
       "         6020: 1,\n",
       "         2000: 1,\n",
       "         4312: 1,\n",
       "         7812: 1,\n",
       "         3959: 1,\n",
       "         5361: 1,\n",
       "         6934: 1,\n",
       "         5624: 1,\n",
       "         1616: 1,\n",
       "         847: 1,\n",
       "         9325: 1,\n",
       "         6834: 1,\n",
       "         4789: 1,\n",
       "         9598: 1,\n",
       "         5646: 1,\n",
       "         12743: 2,\n",
       "         10960: 1,\n",
       "         7626: 1,\n",
       "         5952: 1,\n",
       "         13234: 1,\n",
       "         9341: 1,\n",
       "         11633: 1,\n",
       "         7964: 1,\n",
       "         3151: 1,\n",
       "         2392: 1,\n",
       "         12768: 1,\n",
       "         11643: 1,\n",
       "         5386: 1,\n",
       "         4675: 1,\n",
       "         6758: 1,\n",
       "         8816: 1,\n",
       "         11522: 1,\n",
       "         11699: 1,\n",
       "         11184: 1,\n",
       "         8133: 1,\n",
       "         5083: 1,\n",
       "         12926: 1,\n",
       "         9556: 1,\n",
       "         11966: 1,\n",
       "         8283: 1,\n",
       "         9148: 1,\n",
       "         126: 1,\n",
       "         9306: 1,\n",
       "         5061: 1,\n",
       "         10961: 1,\n",
       "         1163: 1,\n",
       "         11469: 1,\n",
       "         1455: 1,\n",
       "         4723: 1,\n",
       "         8831: 1,\n",
       "         2372: 1,\n",
       "         2179: 1,\n",
       "         7624: 1,\n",
       "         12550: 1,\n",
       "         11925: 1,\n",
       "         5322: 1,\n",
       "         3821: 1,\n",
       "         3764: 1,\n",
       "         9004: 1,\n",
       "         6594: 1,\n",
       "         12650: 1,\n",
       "         12349: 1,\n",
       "         1164: 1,\n",
       "         5426: 1,\n",
       "         10043: 1,\n",
       "         9552: 1,\n",
       "         10644: 1,\n",
       "         10391: 1,\n",
       "         13198: 1,\n",
       "         4321: 1,\n",
       "         11342: 1,\n",
       "         6175: 1,\n",
       "         9010: 1,\n",
       "         9496: 1,\n",
       "         9807: 1,\n",
       "         6710: 1,\n",
       "         11232: 1,\n",
       "         10586: 1,\n",
       "         10464: 1,\n",
       "         11848: 1,\n",
       "         8629: 1,\n",
       "         3896: 1,\n",
       "         12618: 1,\n",
       "         6433: 1,\n",
       "         3835: 1,\n",
       "         7353: 1,\n",
       "         9229: 1,\n",
       "         10277: 1,\n",
       "         1033: 1,\n",
       "         3340: 1,\n",
       "         7681: 1,\n",
       "         3712: 1,\n",
       "         6784: 1,\n",
       "         2768: 1,\n",
       "         10184: 1,\n",
       "         51: 1,\n",
       "         3265: 1,\n",
       "         341: 1,\n",
       "         2667: 1,\n",
       "         5137: 1,\n",
       "         5749: 1,\n",
       "         12655: 1,\n",
       "         12612: 1,\n",
       "         9551: 1,\n",
       "         10718: 1,\n",
       "         6709: 1,\n",
       "         3906: 1,\n",
       "         8223: 1,\n",
       "         5908: 1,\n",
       "         9412: 1,\n",
       "         5476: 1,\n",
       "         5154: 1,\n",
       "         10816: 1,\n",
       "         10853: 1,\n",
       "         5018: 1,\n",
       "         7133: 1,\n",
       "         12440: 1,\n",
       "         6099: 1,\n",
       "         9790: 1,\n",
       "         11628: 1,\n",
       "         6327: 2,\n",
       "         12800: 1,\n",
       "         10848: 1,\n",
       "         1882: 1,\n",
       "         5527: 1,\n",
       "         2549: 1,\n",
       "         8291: 1,\n",
       "         5272: 1,\n",
       "         6258: 1,\n",
       "         6229: 1,\n",
       "         10748: 1,\n",
       "         1804: 1,\n",
       "         9570: 1,\n",
       "         7827: 1,\n",
       "         12466: 1,\n",
       "         8508: 1,\n",
       "         2151: 1,\n",
       "         10754: 1,\n",
       "         11674: 1,\n",
       "         3249: 1,\n",
       "         10551: 1,\n",
       "         12085: 1,\n",
       "         3402: 1,\n",
       "         12295: 1,\n",
       "         10624: 1,\n",
       "         5904: 1,\n",
       "         1557: 1,\n",
       "         7859: 1,\n",
       "         3998: 1,\n",
       "         8664: 1,\n",
       "         8349: 1,\n",
       "         12786: 1,\n",
       "         1305: 1,\n",
       "         611: 1,\n",
       "         4337: 1,\n",
       "         4766: 1,\n",
       "         9449: 1,\n",
       "         9980: 1,\n",
       "         5281: 1,\n",
       "         12893: 1,\n",
       "         4344: 1,\n",
       "         9130: 1,\n",
       "         12152: 1,\n",
       "         1627: 1,\n",
       "         2622: 1,\n",
       "         13099: 1,\n",
       "         6137: 1,\n",
       "         7670: 1,\n",
       "         11593: 1,\n",
       "         9005: 1,\n",
       "         381: 1,\n",
       "         12544: 1,\n",
       "         4624: 1,\n",
       "         11094: 1,\n",
       "         11553: 1,\n",
       "         9206: 1,\n",
       "         148: 1,\n",
       "         8962: 1,\n",
       "         8564: 1,\n",
       "         9616: 1,\n",
       "         8778: 1,\n",
       "         11143: 1,\n",
       "         9578: 1,\n",
       "         5023: 1,\n",
       "         11134: 1,\n",
       "         6549: 1,\n",
       "         2138: 1,\n",
       "         8049: 1,\n",
       "         10097: 1,\n",
       "         11792: 1,\n",
       "         13184: 1,\n",
       "         5135: 1,\n",
       "         4695: 1,\n",
       "         11734: 1,\n",
       "         9355: 1,\n",
       "         4433: 1,\n",
       "         3085: 1,\n",
       "         12278: 1,\n",
       "         2055: 1,\n",
       "         113: 1,\n",
       "         12945: 1,\n",
       "         9725: 1,\n",
       "         3857: 1,\n",
       "         3159: 1,\n",
       "         11677: 1,\n",
       "         10874: 1,\n",
       "         4157: 1,\n",
       "         10681: 1,\n",
       "         6079: 1,\n",
       "         8144: 1,\n",
       "         3772: 1,\n",
       "         11608: 1,\n",
       "         5774: 1,\n",
       "         1370: 1,\n",
       "         10459: 1,\n",
       "         3365: 1,\n",
       "         6849: 1,\n",
       "         212: 1,\n",
       "         2185: 1,\n",
       "         2743: 1,\n",
       "         5198: 1,\n",
       "         4341: 1,\n",
       "         7990: 1,\n",
       "         1737: 1,\n",
       "         2577: 1,\n",
       "         9961: 1,\n",
       "         85: 1,\n",
       "         5106: 1,\n",
       "         2703: 1,\n",
       "         11308: 1,\n",
       "         2384: 1,\n",
       "         1664: 1,\n",
       "         7714: 1,\n",
       "         11937: 1,\n",
       "         3852: 1,\n",
       "         13166: 1,\n",
       "         8339: 1,\n",
       "         11128: 1,\n",
       "         10693: 1,\n",
       "         9333: 1,\n",
       "         1306: 1,\n",
       "         11400: 1,\n",
       "         989: 1,\n",
       "         8807: 1,\n",
       "         3730: 1,\n",
       "         2224: 1,\n",
       "         1977: 1,\n",
       "         3474: 1,\n",
       "         10689: 1,\n",
       "         5259: 1,\n",
       "         1085: 1,\n",
       "         12391: 1,\n",
       "         8027: 1,\n",
       "         790: 1,\n",
       "         8843: 1,\n",
       "         7919: 1,\n",
       "         2580: 1,\n",
       "         5873: 1,\n",
       "         1467: 1,\n",
       "         1355: 1})"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test quality\n",
    "\n",
    "import collections\n",
    "\n",
    "ranks = []\n",
    "for doc_id in range(len(train_stemmed_text)):\n",
    "    inferred_vector = doc2vec_vect.vect_model.infer_vector(train_stemmed_text[train_stemmed_text.index[doc_id]])\n",
    "    sims = doc2vec_vect.vect_model.docvecs.most_similar([inferred_vector], topn=len(doc2vec_vect.vect_model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "collections.Counter(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def show_score(classifier_name, scores):\n",
    "    print(\"Accuracy:%0.2f Precission:%0.2f Recall:%0.2f F1:%0.2f\"%scores, \"-> [%s]\"%(classifier_name))\n",
    "    \n",
    "def train_model(classifier, train_feature, train_label, test_feature, test_label, is_neural_net=False):\n",
    "#     print(train_feature)\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(train_feature, train_label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(test_feature)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    print(metrics.confusion_matrix(test_label, predictions))\n",
    "    return (metrics.accuracy_score(test_label, predictions),\n",
    "            metrics.precision_score(test_label, predictions),\n",
    "            metrics.recall_score(test_label, predictions),\n",
    "            metrics.f1_score(test_label, predictions))\n",
    "\n",
    "def run_model(classfier_configs):\n",
    "    for _name in classfier_configs:\n",
    "#         print(classfier_configs[_name].train)\n",
    "        scores = train_model(classfier_configs[_name][0], classfier_configs[_name][1], train_label, classfier_configs[_name][2], test_label)\n",
    "        show_score(_name, scores)\n",
    "\n",
    "def eval_model(classfier_configs, param_distributions = None, do_grid_search = False):\n",
    "    test_scores = []\n",
    "    best_params = []\n",
    "    model_names = []\n",
    "    \n",
    "    for _name in classfier_configs:\n",
    "        model_names.append(_name)\n",
    "        if not (param_distributions is None):\n",
    "            if do_grid_search:\n",
    "                best_model = GridSearchCV(classfier_configs[_name][0], \n",
    "                                             scoring = \"f1\", \n",
    "                                             param_grid = param_distributions, \n",
    "                                             cv = 5)\n",
    "            else:\n",
    "                best_model = RandomizedSearchCV(classfier_configs[_name][0], \n",
    "                                             scoring = \"f1\", \n",
    "                                             param_distributions = param_distributions, \n",
    "                                           cv = 5)\n",
    "            \n",
    "            best_model.fit(classfier_configs[_name][1], classfier_configs[_name][2])\n",
    "            _score = best_model.best_score_\n",
    "            _params = best_model.best_params_\n",
    "\n",
    "        else:\n",
    "            cv_results = cross_validate(classfier_configs[_name][0], \n",
    "                                        classfier_configs[_name][1], \n",
    "                                        classfier_configs[_name][2],\n",
    "                                        scoring = 'f1',\n",
    "                                        cv = 5)\n",
    "            \n",
    "            _score = np.mean(cv_results['test_score'])\n",
    "            _params = None\n",
    "        \n",
    "        print(_score, '\\t', _params, '\\t', _name)\n",
    "        test_scores.append(_score)\n",
    "        best_params.append(_params)\n",
    "    \n",
    "    best_of_best = np.argmax(test_scores)\n",
    "    print('Best Model:[', model_names[best_of_best], '] Test Score:', \"%0.3f\"%(test_scores[best_of_best]))\n",
    "        \n",
    "    return {'test_scores': test_scores, 'best_parameters': best_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6059380201315685 \t None \t NB Extra All\n",
      "0.6272072595274806 \t None \t NB Extra Cuisine Zipcode\n",
      "0.657405178554519 \t None \t NB Count\n",
      "0.6618923600434579 \t None \t NB Count + Extra All\n",
      "0.6525285916293603 \t None \t NB Count + Extra Cuisine Zipcode\n",
      "0.6816912232452474 \t None \t NB TFIDF\n",
      "0.7037103358645334 \t None \t NB TFIDF + Extra All\n",
      "0.6760530479848176 \t None \t NB TFIDF + Extra Cuisine Zipcode\n",
      "0.6502677467936558 \t None \t NB TFIDF NGram\n",
      "0.6928174173727925 \t None \t NB TFIDF NGram + Extra All\n",
      "0.6623921796651286 \t None \t NB TFIDF NGram + Extra Cuisine ZipCode\n",
      "0.6466715366588932 \t None \t NB LDA\n",
      "0.6139791475610921 \t None \t NB LDA + Extra All\n",
      "0.6454713229354379 \t None \t NB LDA + Extra Cuisine Zipcode\n",
      "Best Model:[ NB TFIDF + Extra All ] Test Score: 0.704\n"
     ]
    }
   ],
   "source": [
    "# nb_config = {\n",
    "#     \"NB Count\": (naive_bayes.MultinomialNB(), train_count, test_count),\n",
    "#     \"NB TFIDF\": (naive_bayes.MultinomialNB(), train_tfidf, test_tfidf),\n",
    "#     \"NB TFIDF NGram\": (naive_bayes.MultinomialNB(), train_tfidf_ngram, test_tfidf_ngram),\n",
    "#     \"NB TFIDF NGram Chars\": (naive_bayes.MultinomialNB(), train_tfidf_ngram_chars, test_tfidf_ngram_chars),\n",
    "#     \"NB LDA\": (naive_bayes.MultinomialNB(), train_lda, test_lda),\n",
    "# }\n",
    "\n",
    "# run_model(nb_config)\n",
    "\n",
    "nb_config = {\n",
    "    \"NB Extra All\": (naive_bayes.MultinomialNB(), labeled_extra_all, LABELS)\n",
    "    , \"NB Extra Cuisine Zipcode\": (naive_bayes.MultinomialNB(), labeled_extra_cuisin_zipcode, LABELS)\n",
    "    , \"NB Count\": (naive_bayes.MultinomialNB(), labeled_count, LABELS)\n",
    "    , \"NB Count + Extra All\": (naive_bayes.MultinomialNB(), labeled_count_extra_all, LABELS)\n",
    "    , \"NB Count + Extra Cuisine Zipcode\": (naive_bayes.MultinomialNB(), labeled_count_extra_cuisine_zipcode, LABELS)\n",
    "    , \"NB TFIDF\": (naive_bayes.MultinomialNB(), labeled_tfidf, LABELS)\n",
    "    , \"NB TFIDF + Extra All\": (naive_bayes.MultinomialNB(), labeled_tfidf_extra_all, LABELS)\n",
    "    , \"NB TFIDF + Extra Cuisine Zipcode\": (naive_bayes.MultinomialNB(), labeled_tfidf_extra_cuisine_zipcode, LABELS)\n",
    "    , \"NB TFIDF NGram\": (naive_bayes.MultinomialNB(), labeled_tfidf_ngram, LABELS)\n",
    "    , \"NB TFIDF NGram + Extra All\": (naive_bayes.MultinomialNB(), labeled_tfidf_ngram_extra, LABELS)\n",
    "    , \"NB TFIDF NGram + Extra Cuisine ZipCode\": (naive_bayes.MultinomialNB(), labeled_tfidf_ngram_extra_cuisine_zipcode, LABELS)\n",
    "#     , \"NB TFIDF NGram Chars\": (naive_bayes.MultinomialNB(), labeled_tfidf_ngram_chars, LABELS)\n",
    "#     , \"NB TFIDF NGram Chars + Extra\": (naive_bayes.MultinomialNB(), labeled_tfidf_ngram_chars_extra, LABELS)\n",
    "    , \"NB LDA\": (naive_bayes.MultinomialNB(), labeled_lda, LABELS)\n",
    "    , \"NB LDA + Extra All\": (naive_bayes.MultinomialNB(), labeled_lda_extra_all, LABELS)\n",
    "    , \"NB LDA + Extra Cuisine Zipcode\": (naive_bayes.MultinomialNB(), labeled_lda_extra_cuisine_zipcode, LABELS)\n",
    "}\n",
    "\n",
    "nb_eval = eval_model(nb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_model = naive_bayes.MultinomialNB()\n",
    "# nb_model.fit(train_tfidf, train_label)\n",
    "# predictions = nb_model.predict(test_tfidf)     \n",
    "# predictions_probs = nb_model.predict_proba(test_tfidf)\n",
    "\n",
    "# new_predictions = []\n",
    "# for p in predictions_probs:\n",
    "#     if p[0] > 0.4:\n",
    "#          new_predictions.append(0)\n",
    "#     else:\n",
    "#         new_predictions.append(1)\n",
    "        \n",
    "# print(metrics.confusion_matrix(test_label, predictions))\n",
    "# print (metrics.accuracy_score(test_label, predictions),\n",
    "#             metrics.precision_score(test_label, predictions),\n",
    "#             metrics.recall_score(test_label, predictions),\n",
    "#             metrics.f1_score(test_label, predictions))\n",
    "\n",
    "# print(metrics.confusion_matrix(test_label, new_predictions))\n",
    "# print (metrics.accuracy_score(test_label, new_predictions),\n",
    "#             metrics.precision_score(test_label, new_predictions),\n",
    "#             metrics.recall_score(test_label, new_predictions),\n",
    "#             metrics.f1_score(test_label, new_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:0.55 Precission:0.62 Recall:0.52 F1:0.56 -> [NB Count]\n",
    "Accuracy:0.51 Precission:0.88 Recall:0.49 F1:0.63 -> [NB TFIDF]\n",
    "Accuracy:0.52 Precission:0.58 Recall:0.49 F1:0.53 -> [NB TFIDF NGram]\n",
    "Accuracy:0.49 Precission:0.96 Recall:0.48 F1:0.64 -> [NB TFIDF NGram Chars]\n",
    "Accuracy:0.55 Precission:0.69 Recall:0.52 F1:0.60 -> [NB LDA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# linear_parameters = {'penalty':('l1', 'l2'), 'C':[10, 1, 0.1, 0.01]}\n",
    "# linear_model = GridSearchCV(LogisticRegression(solver='saga'), scoring = \"f1\",param_grid = linear_parameters, cv=5)\n",
    "\n",
    "# linear_model.fit(train_count, train_label)\n",
    "\n",
    "# predictions = linear_model.predict(test_count)\n",
    "\n",
    "# (metrics.accuracy_score(predictions, test_label),\n",
    "#             metrics.precision_score(predictions, test_label),\n",
    "#             metrics.recall_score(predictions, test_label),\n",
    "#             metrics.f1_score(predictions, test_label))\n",
    "\n",
    "# print(linear_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5735167000739209 \t {'C': 1, 'penalty': 'l1', 'solver': 'saga'} \t Linear Extra All\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.642433672505511 \t {'C': 1, 'penalty': 'l2', 'solver': 'saga'} \t Linear Extra Cuisine Zipcode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6515652424608699 \t {'C': 10, 'penalty': 'l2', 'solver': 'saga'} \t Linear Count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6524011955097618 \t {'C': 0.01, 'penalty': 'l1', 'solver': 'saga'} \t Linear Count Extra All\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%% time\n",
    "# linear_config = {\n",
    "#     \"Linear Count\": (LogisticRegression(C= 0.1, penalty='l1', tol=0.1, solver='saga'), train_count, test_count)\n",
    "#     , \"Linear TFIDF\": (LogisticRegression(C= 0.1, penalty='l2', tol=0.1, solver='saga'), train_tfidf, test_tfidf)\n",
    "#     , \"Linear TFIDF NGram\": (LogisticRegression(C= 0.1, penalty='l2', tol=0.1, solver='saga'), train_tfidf_ngram, test_tfidf_ngram)\n",
    "#     , \"Linear TFIDF NGram Chars\": (LogisticRegression(C= 0.1, penalty='l2', tol=0.1, solver='saga'), train_tfidf_ngram_chars, test_tfidf_ngram_chars)\n",
    "# #     , \"Linear FastText Embedding\": (LogisticRegression(solver='lbfgs', max_iter=int(1e6)), train_fasttext_embedding, test_fasttext_embedding)\n",
    "#     , \"Linear LDA\": (LogisticRegression(solver='lbfgs'), train_lda, test_lda)\n",
    "#     , \"Linear Doc2Vec\": (LogisticRegression(solver='lbfgs'), train_doc2vec, test_doc2vec)\n",
    "# }\n",
    "\n",
    "# run_model(linear_config)\n",
    "\n",
    "linear_parameters = {'penalty':('l1', 'l2'), 'C':[10, 1, 0.1, 0.01], 'solver':['saga']}\n",
    "\n",
    "linear_config = {\n",
    "    \"Linear Extra All\": (LogisticRegression(), labeled_extra_all, LABELS)\n",
    "    , \"Linear Extra Cuisine Zipcode\": (LogisticRegression(), labeled_extra_cuisin_zipcode, LABELS)\n",
    "    , \"Linear Count\": (LogisticRegression(), labeled_count, LABELS)\n",
    "    , \"Linear Count Extra All\": (LogisticRegression(), labeled_count_extra_all, LABELS)\n",
    "    , \"Linear Count Extra Cuisine+Zipcode\": (LogisticRegression(), labeled_count_extra_cuisine_zipcode, LABELS)\n",
    "    , \"Linear TFIDF\": (LogisticRegression(), labeled_tfidf, LABELS)\n",
    "    , \"Linear TFIDF Extra All\": (LogisticRegression(), labeled_tfidf_extra_all, LABELS)\n",
    "    , \"Linear TFIDF Extra Cuisine Zipcode\": (LogisticRegression(), labeled_tfidf_extra_cuisine_zipcode, LABELS)\n",
    "    , \"Linear TFIDF NGram\": (LogisticRegression(), labeled_tfidf_ngram, LABELS)\n",
    "    , \"Linear TFIDF NGram Extra All\": (LogisticRegression(), labeled_tfidf_ngram_extra_all, LABELS)\n",
    "    , \"Linear TFIDF NGram Extra Cuisine Zipcode\": (LogisticRegression(), labeled_tfidf_ngram_extra_cuisine_zipcode, LABELS)\n",
    "\n",
    "#     , \"Linear TFIDF NGram Chars\": (LogisticRegression(C= 0.1, penalty='l2', tol=0.1, solver='saga'), labeled_tfidf_ngram_chars, LABELS)\n",
    "#     , \"Linear TFIDF NGram Chars Extra\": (LogisticRegression(C= 0.1, penalty='l2', tol=0.1, solver='saga'), labeled_tfidf_ngram_chars_extra, LABELS)\n",
    "    , \"Linear LDA\": (LogisticRegression(), labeled_lda, LABELS)\n",
    "    , \"Linear LDA Extra All\": (LogisticRegression(), labeled_lda_extra_all, LABELS)\n",
    "    , \"Linear LDA Extra Cuisine Zipcode\": (LogisticRegression(), labeled_lda_extra_cuisine_zipcode, LABELS)\n",
    "    , \"Linear Doc2Vec\": (LogisticRegression(), labeled_doc2vec, LABELS)\n",
    "    , \"Linear Doc2Vec Extra All\": (LogisticRegression(), labeled_doc2vec_extra_all, LABELS)\n",
    "    , \"Linear Doc2Vec Extra Cuisine Zipcode\": (LogisticRegression(), labeled_doc2vec_extra_all, LABELS)\n",
    "}\n",
    "\n",
    "linear_eval = eval_model(linear_config, param_distributions = linear_parameters, do_grid_search = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_scores': [0.626801630824723,\n",
       "  0.6449984819134719,\n",
       "  0.578211846856285,\n",
       "  0.5722496078405415,\n",
       "  0.5815346549774638,\n",
       "  0.6262137864319106,\n",
       "  0.649781461802015,\n",
       "  0.6525754468816669,\n",
       "  0.6133425467129582,\n",
       "  0.6487110688085156,\n",
       "  0.6508696294298575,\n",
       "  0.6070709000705748,\n",
       "  0.6256979013438977,\n",
       "  0.6407545503102018,\n",
       "  0.5443858998144713,\n",
       "  0.558356583800378,\n",
       "  0.558356583800378],\n",
       " 'best_parameters': [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None]}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# svm_parameters = {'kernel':('linear', 'rbf'), 'C':[0.1, 1, 10, 100], \"gamma\": np.logspace(-2, 2, 5)}\n",
    "# svm_model = GridSearchCV(SVC(), scoring = \"f1\", param_grid = svm_parameters, cv=5)\n",
    "\n",
    "# svm_model.fit(train_tfidf_ngram_chars, train_label)\n",
    "\n",
    "# predictions = svm_model.predict(test_tfidf_ngram_chars)\n",
    "\n",
    "# print(metrics.accuracy_score(predictions, test_label),\n",
    "#             metrics.precision_score(predictions, test_label),\n",
    "#             metrics.recall_score(predictions, test_label),\n",
    "#             metrics.f1_score(predictions, test_label))\n",
    "# print(svm_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59649123 0.54       0.49122807 0.59677419 0.57692308] 0.5602833137434157 \t SVM Extra Only\n",
      "[0.6        0.71028037 0.66071429 0.60952381 0.58333333] 0.632770360480641 \t SVM Extra Cuisin & Zipcode\n",
      "[0.675      0.6625     0.65822785 0.66216216 0.65771812] 0.6631216262137595 \t SVM Count\n",
      "[0.675      0.6625     0.65822785 0.65771812 0.65771812] 0.6622328179424009 \t SVM Count Extra\n",
      "[0.58695652 0.56818182 0.55913978 0.60674157 0.42666667] 0.549537272913512 \t SVM TFIDF\n",
      "[0.59047619 0.66666667 0.55855856 0.62295082 0.54545455] 0.5968213561656184 \t SVM TFIDF Extra\n",
      "[0.58333333 0.62626263 0.71428571 0.56       0.56179775] 0.6091358853381326 \t SVM TFIDF NGram\n",
      "[0.63247863 0.72897196 0.65       0.63247863 0.58585859] 0.6459575626865347 \t SVM TFIDF NGram Extra\n",
      "[0.67096774 0.67114094 0.63380282 0.64827586 0.67647059] 0.6601315897476935 \t SVM TFIDF NGram Chars\n",
      "[0.59047619 0.66       0.55855856 0.61788618 0.55445545] 0.5962752746882183 \t SVM TFIDF NGram Chars Extra\n",
      "[0.57471264 0.30555556 0.48192771 0.41666667 0.29411765] 0.41459604476051604 \t SVM LDA\n",
      "[0.59649123 0.55445545 0.49122807 0.59677419 0.57692308] 0.5631744028523265 \t SVM LDA Extra\n",
      "[0.7        0.63235294 0.59701493 0.68148148 0.703125  ] 0.6627948696062173 \t SVM Doc2Vec\n",
      "[0.71328671 0.60740741 0.6        0.67153285 0.6969697 ] 0.6578393328758293 \t SVM Doc2Vec Extra\n"
     ]
    }
   ],
   "source": [
    "# svm_config = {\n",
    "#     \"SVM Count\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), train_count, test_count),\n",
    "#     \"SVM TFIDF\": (SVC(C = 10, gamma = 0.01, kernel='rbf'), train_tfidf, test_tfidf),\n",
    "#     \"SVM TFIDF NGram\": (SVC(C = 1, gamma = 0.01, kernel='linear'), train_tfidf_ngram, test_tfidf_ngram),\n",
    "#     \"SVM TFIDF NGram Chars\": (SVC(C = 10, gamma = 0.01, kernel='rbf'), train_tfidf_ngram_chars, test_tfidf_ngram_chars),\n",
    "# #     \"SVM FastText Embedding\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), train_fasttext_embedding, test_fasttext_embedding),\n",
    "#     \"SVM LDA\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), train_lda, test_lda),\n",
    "#     \"SVM Doc2Vec\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), train_doc2vec, test_doc2vec),\n",
    "# }\n",
    "    \n",
    "# run_model(svm_config)\n",
    "\n",
    "svm_config = {\n",
    "    \"SVM Extra Only\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), labeled_extra_all, LABELS)\n",
    "    , \"SVM Extra Cuisin & Zipcode\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), labeled_extra_cuisin_zipcode, LABELS)\n",
    "    , \"SVM Count\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), labeled_count, LABELS)\n",
    "    , \"SVM Count Extra All\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), labeled_count_extra_all, LABELS)\n",
    "    , \"SVM Count Extra Cuisine Zipcode\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), labeled_count_extra_cuisine_zipcode, LABELS)\n",
    "    , \"SVM TFIDF\": (SVC(C = 10, gamma = 0.01, kernel='rbf'), labeled_tfidf, LABELS)\n",
    "    , \"SVM TFIDF Extra\": (SVC(C = 10, gamma = 0.01, kernel='rbf'), labeled_tfidf_extra, LABELS)\n",
    "    , \"SVM TFIDF NGram\": (SVC(C = 1, gamma = 0.01, kernel='linear'), labeled_tfidf_ngram, LABELS)\n",
    "    , \"SVM TFIDF NGram Extra\": (SVC(C = 1, gamma = 0.01, kernel='linear'), labeled_tfidf_ngram_extra, LABELS)\n",
    "    , \"SVM TFIDF NGram Chars\": (SVC(C = 10, gamma = 0.01, kernel='rbf'), labeled_tfidf_ngram_chars, LABELS)\n",
    "    , \"SVM TFIDF NGram Chars Extra\": (SVC(C = 10, gamma = 0.01, kernel='rbf'), labeled_tfidf_ngram_chars_extra, LABELS)\n",
    "    , \"SVM LDA\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), labeled_lda, LABELS)\n",
    "    , \"SVM LDA Extra\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), labeled_lda_extra, LABELS)\n",
    "    , \"SVM Doc2Vec\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), labeled_doc2vec, LABELS)\n",
    "    , \"SVM Doc2Vec Extra\": (SVC(C = 1, gamma = 0.1, kernel='rbf'), labeled_doc2vec_extra, LABELS)\n",
    "}\n",
    "\n",
    "eval_model(svm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 tree = self._make_estimator(append=False,\n\u001b[0;32m--> 319\u001b[0;31m                                             random_state=random_state)\n\u001b[0m\u001b[1;32m    320\u001b[0m                 \u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/ensemble/base.py\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0msub\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         estimator.set_params(**dict((p, getattr(self, p))\n\u001b[1;32m    128\u001b[0m                                     for p in self.estimator_params))\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mparams_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# quick sanity check of the parameters of the clone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[1;32m    180\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# introspect the constructor arguments to find the model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# to represent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0minit_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;31m# Consider the constructor parameters excluding 'self'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         parameters = [p for p in init_signature.parameters.values()\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   3063\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   2813\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0;32m-> 2815\u001b[0;31m                                         follow_wrapper_chains=follow_wrapped)\n\u001b[0m\u001b[1;32m   2816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2209\u001b[0m     \u001b[0;31m# Was this function wrapped by a decorator?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2211\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__signature__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2213\u001b[0m             \u001b[0;31m# If the unwrapped object is a *method*, we might want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf_parameters = {'n_estimators':[100, 200, 500], 'max_features':[None, 0.25, 0.5, 0.75],\n",
    "                'max_depth': [None, 5, 10], 'min_samples_leaf': [0.0005, 0.01, 0.05, 0.1],\n",
    "                 'min_samples_split':[2, 5, 10]}\n",
    "labeled_data = [labeled_extra_all, labeled_extra_cuisin_zipcode, \n",
    "                , labeled_count, labeled_count_extra_all, labeled_count_extra_cuisin_zipcode\n",
    "                , labeled_tfidf, labeled_tfidf_extra_all, labeled_tfidf_extra_cuisin_zipcode,\n",
    "                , labeled_tfidf_ngram, labeled_tfidf_ngram_extra_all, labeled_tfidf_ngram_extra_cuisin_zipcode,\n",
    "                , labeled_lda, labeled_lda_extra_all, labeled_lda_extra__cuisin_zipcode\n",
    "                , labeled_doc2vec, labeled_doc2vec_extra_all, labeled_lda_extra__cuisin_zipcode]\n",
    "\n",
    "rf_model = RandomizedSearchCV(RandomForestClassifier(), n_iter = 100, scoring = \"f1\", param_distributions = rf_parameters, cv=5)\n",
    "\n",
    "for _data in labeled_data:\n",
    "    %time rf_model.fit(_data, LABELS)\n",
    "    print(rf_model.best_score_)\n",
    "    print(rf_model.best_params_)\n",
    "    \n",
    "# predictions = rf_model.predict(test_tfidf_ngram_chars)\n",
    "\n",
    "# print(metrics.accuracy_score(predictions, test_label),\n",
    "#             metrics.precision_score(predictions, test_label),\n",
    "#             metrics.recall_score(predictions, test_label),\n",
    "#             metrics.f1_score(predictions, test_label))\n",
    "# print(rf_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58715596 0.64705882 0.62608696 0.60377358 0.6       ] 0.6128150656519127 \t RF Extra Only\n",
      "[0.63247863 0.69724771 0.65546218 0.61403509 0.59405941] 0.6386566034868986 \t RF Extra Cuisine + Zipcode\n",
      "[0.65420561 0.58       0.5840708  0.64814815 0.5625    ] 0.6057849104169921 \t RF Count\n",
      "[0.65486726 0.69565217 0.57657658 0.66055046 0.55670103] 0.6288694993540439 \t RF Count Extra\n",
      "[0.55238095 0.67857143 0.60194175 0.64150943 0.61363636] 0.6176079852247649 \t RF TFIDF\n",
      "[0.59047619 0.64912281 0.65420561 0.64761905 0.59183673] 0.626652077456659 \t RF TFIDF Extra\n",
      "[0.56470588 0.55319149 0.53763441 0.53488372 0.44444444] 0.5269719891382941 \t RF TFIDF NGram\n",
      "[0.64583333 0.63366337 0.59183673 0.59574468 0.45783133] 0.5849818881032226 \t RF TFIDF NGram Extra\n",
      "[0.62962963 0.61403509 0.59130435 0.61403509 0.56842105] 0.6034850411051784 \t RF TFIDF NGram Chars\n",
      "[0.57692308 0.63247863 0.55045872 0.66037736 0.51612903] 0.5872733631493341 \t RF TFIDF NGram Chars Extra\n",
      "[0.58490566 0.6504065  0.55652174 0.63636364 0.42222222] 0.5700839524317385 \t RF LDA\n",
      "[0.58715596 0.66666667 0.55855856 0.63063063 0.4516129 ] 0.5789249444768829 \t RF LDA Extra\n",
      "[0.58252427 0.64347826 0.56637168 0.59459459 0.59405941] 0.5962056429330687 \t RF Doc2Vec\n",
      "[0.62264151 0.63247863 0.56896552 0.60550459 0.64583333] 0.6150847159286542 \t RF Doc2Vec Extra\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_config = {\n",
    "    \"RF Extra Only\": (RandomForestClassifier(criterion = 'entropy', n_estimators = 500), \n",
    "                      labeled_extra_all, LABELS)\n",
    "    , \"RF Extra Cuisine + Zipcode\": (RandomForestClassifier(criterion = 'entropy', n_estimators = 100), labeled_extra_cuisin_zipcode, LABELS)\n",
    "    , \"RF Count\": (RandomForestClassifier(criterion = 'entropy', n_estimators = 100), labeled_count, LABELS)\n",
    "    , \"RF Count Extra\": (RandomForestClassifier(criterion = 'entropy', n_estimators = 100), labeled_count_extra, LABELS)\n",
    "    , \"RF TFIDF\": (RandomForestClassifier(criterion = 'entropy', n_estimators = 200), labeled_tfidf, LABELS)\n",
    "    , \"RF TFIDF Extra\": (RandomForestClassifier(criterion = 'entropy', n_estimators = 200), labeled_tfidf_extra, LABELS)\n",
    "    , \"RF TFIDF NGram\": (RandomForestClassifier(criterion = 'gini', n_estimators = 200), labeled_tfidf_ngram, LABELS)\n",
    "    , \"RF TFIDF NGram Extra\": (RandomForestClassifier(criterion = 'gini', n_estimators = 200), labeled_tfidf_ngram_extra, LABELS)\n",
    "    , \"RF TFIDF NGram Chars\": (RandomForestClassifier(criterion = 'entropy', n_estimators = 200), labeled_tfidf_ngram_chars, LABELS)\n",
    "    , \"RF TFIDF NGram Chars Extra\": (RandomForestClassifier(criterion = 'entropy', n_estimators = 200), labeled_tfidf_ngram_chars_extra, LABELS)\n",
    "    , \"RF LDA\": (RandomForestClassifier(criterion = 'gini', n_estimators = 300), labeled_lda, LABELS)\n",
    "    , \"RF LDA Extra\": (RandomForestClassifier(criterion = 'gini', n_estimators = 300), labeled_lda_extra, LABELS)\n",
    "    , \"RF Doc2Vec\": (RandomForestClassifier(criterion = 'gini', n_estimators = 300), labeled_doc2vec, LABELS)\n",
    "    , \"RF Doc2Vec Extra\": (RandomForestClassifier(criterion = 'gini', n_estimators = 300), labeled_doc2vec_extra, LABELS)\n",
    "}\n",
    "\n",
    "eval_model(rf_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# xgb_parameters = {\n",
    "#         'min_child_weight': [1, 5, 10],\n",
    "#         'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "#         'subsample': [0.6, 0.8, 1.0],\n",
    "#         'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#         'max_depth': [3, 4, 5]\n",
    "#         }\n",
    "\n",
    "# xgb_model = GridSearchCV(XGBClassifier(), scoring = \"f1\", param_grid = xgb_parameters, cv=5, verbose = 3)\n",
    "\n",
    "# xgb_model.fit(train_count, train_label)\n",
    "\n",
    "# predictions = xgb_model.predict(test_count)\n",
    "\n",
    "# print(metrics.accuracy_score(predictions, test_label),\n",
    "#             metrics.precision_score(predictions, test_label),\n",
    "#             metrics.recall_score(predictions, test_label),\n",
    "#             metrics.f1_score(predictions, test_label))\n",
    "\n",
    "# print(xgb_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 0.63265306 0.61261261 0.60176991 0.57446809] 0.6176340674229154 \t XGBoost Extra Only\n",
      "[0.61818182 0.71559633 0.63793103 0.60714286 0.5625    ] 0.6282704080165327 \t XGBoost Extra Cuisine + Zipcode\n",
      "[0.66666667 0.61111111 0.5631068  0.65420561 0.56      ] 0.6110180362741835 \t XGBoost Count\n",
      "[0.62184874 0.57692308 0.56862745 0.63551402 0.49462366] 0.5795073884009668 \t XGBoost Count Extra\n",
      "[0.625      0.60377358 0.59459459 0.62385321 0.58585859] 0.606615995273603 \t XGBoost TFIDF\n",
      "[0.62385321 0.59405941 0.59813084 0.64864865 0.60416667] 0.6137717546773158 \t XGBoost TFIDF Extra\n",
      "[0.63366337 0.62857143 0.62264151 0.58333333 0.61538462] 0.6167188506119947 \t XGBoost TFIDF NGram\n",
      "[0.62626263 0.60784314 0.59615385 0.59405941 0.62745098] 0.6103539992008251 \t XGBoost TFIDF NGram Extra\n",
      "[0.4952381  0.62711864 0.55855856 0.55238095 0.59405941] 0.5654711312371995 \t XGBoost TFIDF NGram Chars\n",
      "[0.54054054 0.57657658 0.57391304 0.55769231 0.56565657] 0.5628758067888503 \t XGBoost TFIDF NGram Chars Extra\n",
      "[0.48598131 0.65546218 0.56140351 0.56603774 0.46808511] 0.547393968857826 \t XGBoost LDA\n",
      "[0.55045872 0.65       0.58928571 0.57657658 0.4494382 ] 0.5631518417411625 \t XGBoost LDA Extra\n",
      "[0.57391304 0.56603774 0.59459459 0.54205607 0.58585859] 0.5724920069093706 \t XGBoost Doc2Vec\n",
      "[0.60344828 0.58252427 0.61261261 0.51923077 0.55319149] 0.5742014837823627 \t XGBoost Doc2Vec Extra\n"
     ]
    }
   ],
   "source": [
    "# xgboost_config = {\n",
    "#     \"XGBoost Count\": (XGBClassifier(colsample_bytree = 0.8, gamma= 2, max_depth = 4, min_child_weight = 1, subsample =1.0), train_count, test_count)\n",
    "#     , \"XGBoost TFIDF\": (XGBClassifier(colsample_bytree = 0.8, gamma= 2, max_depth = 4, min_child_weight = 1, subsample =1.0), train_tfidf, test_tfidf)\n",
    "#     , \"XGBoost TFIDF NGram\": (XGBClassifier(colsample_bytree = 0.8, gamma= 2, max_depth = 4, min_child_weight = 1, subsample =1.0), train_tfidf_ngram.tocsc(), test_tfidf_ngram.tocsc())\n",
    "#     , \"XGBoost TFIDF NGram Chars\": (XGBClassifier(colsample_bytree = 0.8, gamma= 2, max_depth = 4, min_child_weight = 1, subsample =1.0), train_tfidf_ngram_chars, test_tfidf_ngram_chars)\n",
    "#     , \"XGBoost LDA\": (XGBClassifier(colsample_bytree = 0.8, gamma= 2, max_depth = 4, min_child_weight = 1, subsample =1.0), train_lda, test_lda)\n",
    "#     , \"XGBoost Doc2Vec\": (XGBClassifier(colsample_bytree = 0.8, gamma= 2, max_depth = 4, min_child_weight = 1, subsample =1.0), train_doc2vec, test_doc2vec)\n",
    "# }\n",
    "\n",
    "# xgboost_config = {\n",
    "#     \"XGBoost Count\": (XGBClassifier(), train_count, test_count)\n",
    "#     , \"XGBoost TFIDF\": (XGBClassifier(), train_tfidf, test_tfidf)\n",
    "#     , \"XGBoost TFIDF NGram\": (XGBClassifier(), train_tfidf_ngram.tocsc(), test_tfidf_ngram.tocsc())\n",
    "#     , \"XGBoost TFIDF NGram Chars\": (XGBClassifier(), train_tfidf_ngram_chars, test_tfidf_ngram_chars)\n",
    "#     , \"XGBoost LDA\": (XGBClassifier(), train_lda, test_lda)\n",
    "#     , \"XGBoost Doc2Vec\": (XGBClassifier(), train_doc2vec, test_doc2vec)\n",
    "# }\n",
    "\n",
    "# run_model(xgboost_config)\n",
    "\n",
    "xgboost_config = {\n",
    "    \"XGBoost Extra Only\": (XGBClassifier(), labeled_extra_all, LABELS)\n",
    "    , \"XGBoost Extra Cuisine + Zipcode\": (XGBClassifier(), labeled_extra_cuisin_zipcode, LABELS)\n",
    "    , \"XGBoost Count\": (XGBClassifier(), labeled_count, LABELS)\n",
    "    , \"XGBoost Count Extra\": (XGBClassifier(), labeled_count_extra, LABELS)\n",
    "    , \"XGBoost TFIDF\": (XGBClassifier(), labeled_tfidf, LABELS)\n",
    "    , \"XGBoost TFIDF Extra\": (XGBClassifier(), labeled_tfidf_extra, LABELS)\n",
    "    , \"XGBoost TFIDF NGram\": (XGBClassifier(), labeled_tfidf_ngram, LABELS)\n",
    "    , \"XGBoost TFIDF NGram Extra\": (XGBClassifier(), labeled_tfidf_ngram_extra, LABELS)\n",
    "    , \"XGBoost TFIDF NGram Chars\": (XGBClassifier(), labeled_tfidf_ngram_chars, LABELS)\n",
    "    , \"XGBoost TFIDF NGram Chars Extra\": (XGBClassifier(), labeled_tfidf_ngram_chars_extra, LABELS)\n",
    "    , \"XGBoost LDA\": (XGBClassifier(), labeled_lda, LABELS)\n",
    "    , \"XGBoost LDA Extra\": (XGBClassifier(), labeled_lda_extra, LABELS)\n",
    "    , \"XGBoost Doc2Vec\": (XGBClassifier(), labeled_doc2vec, LABELS)\n",
    "    , \"XGBoost Doc2Vec Extra\": (XGBClassifier(), labeled_doc2vec_extra, LABELS)\n",
    "}\n",
    "\n",
    "eval_model(xgboost_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

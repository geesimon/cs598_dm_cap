{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/raw/Hygiene/hygiene.dat.labels\") as f:\n",
    "    labels = [int(l) for l in f.readlines() if l[0].isdigit()]\n",
    "\n",
    "docs = []\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat\") as f:\n",
    "    for i in range(len(labels)):\n",
    "        docs.append(f.readline().strip(\"\\n\"))\n",
    "\n",
    "with open(\"./data/processed/train.txt\", \"w\") as f:\n",
    "    for i, _doc in enumerate(docs[0:int(len(docs)*0.8)]):\n",
    "        f.write(\"__label__\" + str(labels[i]) + \" \" + _doc + \"\\n\")\n",
    "\n",
    "with open(\"./data/processed/dev.txt\", \"w\") as f:\n",
    "    for i, _doc in enumerate(docs[int(len(docs)*0.8):int(len(docs)*0.9)]):\n",
    "        f.write(\"__label__\" + str(labels[i]) + \" \" + _doc + \"\\n\")\n",
    "        \n",
    "with open(\"./data/processed/test.txt\", \"w\") as f:\n",
    "    for i, _doc in enumerate(docs[int(len(docs)*0.9):]):\n",
    "        f.write(\"__label__\" + str(labels[i]) + \" \" + _doc + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_bert_train = pd.DataFrame({'user_id':0,\n",
    "                              'label':labels[0:int(len(docs)*0.8)],\n",
    "                              'alpha': 0,\n",
    "                             'text':docs[0:int(len(docs)*0.8)]})\n",
    "df_bert_train['text'] = df_bert_train['text'].replace(r'\\n', ' ', regex=True).replace(r',', ' ', regex=True)\n",
    "\n",
    "df_bert_dev = pd.DataFrame({'user_id':0,\n",
    "                            'label':labels[int(len(docs)*0.8):int(len(docs)*0.9)],\n",
    "                            'alpha': 0,\n",
    "                           'text':docs[int(len(docs)*0.8):int(len(docs)*0.9)]})\n",
    "df_bert_dev['text'] = df_bert_dev['text'].replace(r'\\n', ' ', regex=True).replace(r',', ' ', regex=True)\n",
    "\n",
    "df_bert_test = pd.DataFrame({'user_id':0,\n",
    "                             'label':labels[int(len(docs)*0.9):],\n",
    "                             'alpha': 0,\n",
    "                            'text':docs[int(len(docs)*0.9):]})\n",
    "df_bert_test['text'] = df_bert_test['text'].replace(r'\\n', ' ', regex=True).replace(r',', ' ', regex=True)\n",
    "\n",
    "df_bert_train.to_csv('./data/processed/train.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_dev.to_csv('./data/processed/dev.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_test.to_csv('./data/processed/test.tsv', sep='\\t', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_fasttext = pd.DataFrame()\n",
    "train_df = pd.read_csv(\"~/Downloads/dbpedia_csv/train.csv\", header=None)\n",
    "\n",
    "train_fasttext['text'] = train_df.iloc[:,2]\n",
    "train_fasttext['label'] = [\"__label__\" + str(i) for i in train_df.iloc[:,0]]\n",
    "train_fasttext.to_csv(\"~/Downloads/dbpedia_csv/train_fasttext.csv\", header = False, index = False, sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fasttext = pd.DataFrame()\n",
    "test_df = pd.read_csv(\"~/Downloads/dbpedia_csv/test.csv\", header=None)\n",
    "\n",
    "test_fasttext['text'] = test_df.iloc[:,2]\n",
    "test_fasttext['label'] = [\"__label__\" + str(i) for i in test_df.iloc[:,0]]\n",
    "test_fasttext.to_csv(\"~/Downloads/dbpedia_csv/test_fasttext.csv\", header = False, index = False,  sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [00:10<00:00, 48.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    result_stemmed = []\n",
    "    for token in simple_preprocess(text, min_len = 2):\n",
    "        result.append(token)\n",
    "        if token not in STOPWORDS:\n",
    "            result_stemmed.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return (result, result_stemmed)\n",
    "\n",
    "\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat.labels\") as f:\n",
    "    LABELS = [int(i) for i in f.readlines()[0:546]]\n",
    "\n",
    "LABELED_STEMMED_TEXTS = []\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat\") as f:\n",
    "    for i in tqdm(range(len(LABELS))):\n",
    "        _text = f.readline()\n",
    "        _result, _result_stemmed = preprocess(_text)\n",
    "        LABELED_STEMMED_TEXTS.append(_result_stemmed)\n",
    "        \n",
    "train_text = LABELED_STEMMED_TEXTS[0:400]\n",
    "train_label = LABELS[0:400]\n",
    "test_text = LABELED_STEMMED_TEXTS[400:len(LABELS)]\n",
    "test_label = LABELS[400:len(LABELS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dictionary = corpora.Dictionary(train_text)\n",
    "train_count = [_dictionary.doc2bow(_doc) for _doc in train_text]\n",
    "test_count = [_dictionary.doc2bow(_doc) for _doc in test_text]\n",
    "\n",
    "tfidf_model = models.TfidfModel(train_count)\n",
    "train_tfidf = [_doc for _doc in tfidf_model[train_count]]\n",
    "test_tfidf = [_doc for _doc in tfidf_model[test_count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_count = [(dict(_doc), train_label[i]) for i, _doc in enumerate(train_count)]\n",
    "labeled_test_count = [dict(_doc) for _doc in test_count]\n",
    "\n",
    "labeled_train_tfidf = [(dict(_doc), train_label[i]) for i, _doc in enumerate(train_tfidf)]\n",
    "labeled_test_tfidf = [dict(_doc) for _doc in test_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  7]\n",
      " [62 59]]\n",
      "0.5273972602739726 0.8939393939393939 0.48760330578512395 0.6310160427807486\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "NB_classifier_count = NaiveBayesClassifier.train(labeled_train_count)\n",
    "\n",
    "predictions = NB_classifier_count.classify_many(labeled_test_count)\n",
    "\n",
    "print(metrics.confusion_matrix(predictions, test_label))\n",
    "print(metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43 41]\n",
      " [37 25]]\n",
      "0.4657534246575342 0.3787878787878788 0.4032258064516129 0.39062499999999994\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "DT_classifier_count = DecisionTreeClassifier.train(labeled_train_count)\n",
    "predictions = DT_classifier_count.classify_many(labeled_test_count)\n",
    "\n",
    "print(metrics.confusion_matrix(predictions, test_label))\n",
    "print(metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0]\n",
      " [80 66]]\n",
      "0.4520547945205479 1.0 0.4520547945205479 0.6226415094339622\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.maxent import TypedMaxentFeatureEncoding, MaxentClassifier\n",
    "\n",
    "encoding = TypedMaxentFeatureEncoding.train(labeled_train_tfidf, \n",
    "                                                   count_cutoff=3, \n",
    "                                                   alwayson_features=True)\n",
    "Maxent_classifier_count = MaxentClassifier.train(labeled_train_tfidf, \n",
    "                                           bernoulli=False, \n",
    "                                           encoding=encoding, \n",
    "                                           trace=0)\n",
    "\n",
    "predictions = Maxent_classifier_count.classify_many(labeled_test_tfidf)\n",
    "\n",
    "print(metrics.confusion_matrix(predictions, test_label))\n",
    "print(metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-19 19:04:23,620 Reading data from data/processed\n",
      "2019-07-19 19:04:23,623 Train: data/processed/train.txt\n",
      "2019-07-19 19:04:23,625 Dev: data/processed/dev.txt\n",
      "2019-07-19 19:04:23,627 Test: data/processed/test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/flair/data_fetcher.py:447: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n"
     ]
    }
   ],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./data/processed'), \n",
    "                                                       test_file='test.txt', \n",
    "                                                       dev_file='dev.txt', \n",
    "                                                       train_file='train.txt')\n",
    "word_embeddings = [WordEmbeddings('glove'), \n",
    "                   FlairEmbeddings('news-forward-fast'), \n",
    "                   FlairEmbeddings('news-backward-fast')]\n",
    "\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, \n",
    "                            label_dictionary = corpus.make_label_dictionary(), \n",
    "                            multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('./', max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load_from_file('./best-model.pt')\n",
    "sentence = Sentence(\" \".join(test_text[0]))\n",
    "classifier.predict(sentence)\n",
    "print(sentence.labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

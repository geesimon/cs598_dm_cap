{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/raw/Hygiene/hygiene.dat.labels\") as f:\n",
    "    labels = [int(l) for l in f.readlines() if l[0].isdigit()]\n",
    "\n",
    "docs = []\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat\") as f:\n",
    "    for i in range(len(labels)):\n",
    "        docs.append(f.readline().strip(\"\\n\"))\n",
    "\n",
    "with open(\"./data/processed/train.txt\", \"w\") as f:\n",
    "    for i, _doc in enumerate(docs[0:int(len(docs)*0.8)]):\n",
    "        f.write(\"__label__\" + str(labels[i]) + \" \" + _doc + \"\\n\")\n",
    "\n",
    "with open(\"./data/processed/dev.txt\", \"w\") as f:\n",
    "    for i, _doc in enumerate(docs[int(len(docs)*0.8):int(len(docs)*0.9)]):\n",
    "        f.write(\"__label__\" + str(labels[i]) + \" \" + _doc + \"\\n\")\n",
    "        \n",
    "with open(\"./data/processed/test.txt\", \"w\") as f:\n",
    "    for i, _doc in enumerate(docs[int(len(docs)*0.9):]):\n",
    "        f.write(\"__label__\" + str(labels[i]) + \" \" + _doc + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_fasttext = pd.DataFrame()\n",
    "train_df = pd.read_csv(\"~/Downloads/dbpedia_csv/train.csv\", header=None)\n",
    "\n",
    "train_fasttext['text'] = train_df.iloc[:,2]\n",
    "train_fasttext['label'] = [\"__label__\" + str(i) for i in train_df.iloc[:,0]]\n",
    "train_fasttext.to_csv(\"~/Downloads/dbpedia_csv/train_fasttext.csv\", header = False, index = False, sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fasttext = pd.DataFrame()\n",
    "test_df = pd.read_csv(\"~/Downloads/dbpedia_csv/test.csv\", header=None)\n",
    "\n",
    "test_fasttext['text'] = test_df.iloc[:,2]\n",
    "test_fasttext['label'] = [\"__label__\" + str(i) for i in test_df.iloc[:,0]]\n",
    "test_fasttext.to_csv(\"~/Downloads/dbpedia_csv/test_fasttext.csv\", header = False, index = False,  sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [00:10<00:00, 48.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    result_stemmed = []\n",
    "    for token in simple_preprocess(text, min_len = 2):\n",
    "        result.append(token)\n",
    "        if token not in STOPWORDS:\n",
    "            result_stemmed.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return (result, result_stemmed)\n",
    "\n",
    "\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat.labels\") as f:\n",
    "    LABELS = [int(i) for i in f.readlines()[0:546]]\n",
    "\n",
    "LABELED_STEMMED_TEXTS = []\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat\") as f:\n",
    "    for i in tqdm(range(len(LABELS))):\n",
    "        _text = f.readline()\n",
    "        _result, _result_stemmed = preprocess(_text)\n",
    "        LABELED_STEMMED_TEXTS.append(_result_stemmed)\n",
    "        \n",
    "train_text = LABELED_STEMMED_TEXTS[0:400]\n",
    "train_label = LABELS[0:400]\n",
    "test_text = LABELED_STEMMED_TEXTS[400:len(LABELS)]\n",
    "test_label = LABELS[400:len(LABELS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dictionary = corpora.Dictionary(train_text)\n",
    "train_count = [_dictionary.doc2bow(_doc) for _doc in train_text]\n",
    "test_count = [_dictionary.doc2bow(_doc) for _doc in test_text]\n",
    "\n",
    "tfidf_model = models.TfidfModel(train_count)\n",
    "train_tfidf = [_doc for _doc in tfidf_model[train_count]]\n",
    "test_tfidf = [_doc for _doc in tfidf_model[test_count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_count = [(dict(_doc), train_label[i]) for i, _doc in enumerate(train_count)]\n",
    "labeled_test_count = [dict(_doc) for _doc in test_count]\n",
    "\n",
    "labeled_train_tfidf = [(dict(_doc), train_label[i]) for i, _doc in enumerate(train_tfidf)]\n",
    "labeled_test_tfidf = [dict(_doc) for _doc in test_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  7]\n",
      " [62 59]]\n",
      "0.5273972602739726 0.8939393939393939 0.48760330578512395 0.6310160427807486\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "NB_classifier_count = NaiveBayesClassifier.train(labeled_train_count)\n",
    "\n",
    "predictions = NB_classifier_count.classify_many(labeled_test_count)\n",
    "\n",
    "print(metrics.confusion_matrix(predictions, test_label))\n",
    "print(metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43 41]\n",
      " [37 25]]\n",
      "0.4657534246575342 0.3787878787878788 0.4032258064516129 0.39062499999999994\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "DT_classifier_count = DecisionTreeClassifier.train(labeled_train_count)\n",
    "predictions = DT_classifier_count.classify_many(labeled_test_count)\n",
    "\n",
    "print(metrics.confusion_matrix(predictions, test_label))\n",
    "print(metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0]\n",
      " [80 66]]\n",
      "0.4520547945205479 1.0 0.4520547945205479 0.6226415094339622\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.maxent import TypedMaxentFeatureEncoding, MaxentClassifier\n",
    "\n",
    "encoding = TypedMaxentFeatureEncoding.train(labeled_train_tfidf, \n",
    "                                                   count_cutoff=3, \n",
    "                                                   alwayson_features=True)\n",
    "Maxent_classifier_count = MaxentClassifier.train(labeled_train_tfidf, \n",
    "                                           bernoulli=False, \n",
    "                                           encoding=encoding, \n",
    "                                           trace=0)\n",
    "\n",
    "predictions = Maxent_classifier_count.classify_many(labeled_test_tfidf)\n",
    "\n",
    "print(metrics.confusion_matrix(predictions, test_label))\n",
    "print(metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 11:34:49,351 Reading data from data\\processed\n",
      "2019-07-20 11:34:49,353 Train: data\\processed\\train.txt\n",
      "2019-07-20 11:34:49,355 Dev: data\\processed\\dev.txt\n",
      "2019-07-20 11:34:49,356 Test: data\\processed\\test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\data_fetcher.py:447: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\data_fetcher.py:454: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\data_fetcher.py:463: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 11:35:07,820 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to C:\\Users\\geesi\\AppData\\Local\\Temp\\tmp55wazxz2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160000128/160000128 [08:53<00:00, 299743.68B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 11:44:03,323 copying C:\\Users\\geesi\\AppData\\Local\\Temp\\tmp55wazxz2 to cache at C:\\Users\\geesi\\.flair\\embeddings\\glove.gensim.vectors.npy\n",
      "2019-07-20 11:44:03,546 removing temp file C:\\Users\\geesi\\AppData\\Local\\Temp\\tmp55wazxz2\n",
      "2019-07-20 11:44:05,366 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to C:\\Users\\geesi\\AppData\\Local\\Temp\\tmppzpjk47q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21494764/21494764 [01:47<00:00, 200485.57B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 11:45:54,267 copying C:\\Users\\geesi\\AppData\\Local\\Temp\\tmppzpjk47q to cache at C:\\Users\\geesi\\.flair\\embeddings\\glove.gensim\n",
      "2019-07-20 11:45:54,322 removing temp file C:\\Users\\geesi\\AppData\\Local\\Temp\\tmppzpjk47q\n",
      "2019-07-20 11:45:54,327 this function is deprecated, use smart_open.open instead\n",
      "2019-07-20 11:45:56,636 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt not found in cache, downloading to C:\\Users\\geesi\\AppData\\Local\\Temp\\tmpyq5_mlws\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19689779/19689779 [01:09<00:00, 285083.93B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 11:47:07,245 copying C:\\Users\\geesi\\AppData\\Local\\Temp\\tmpyq5_mlws to cache at C:\\Users\\geesi\\.flair\\embeddings\\lm-news-english-forward-1024-v0.2rc.pt\n",
      "2019-07-20 11:47:07,290 removing temp file C:\\Users\\geesi\\AppData\\Local\\Temp\\tmpyq5_mlws\n",
      "2019-07-20 11:47:15,050 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt not found in cache, downloading to C:\\Users\\geesi\\AppData\\Local\\Temp\\tmptx5hmvus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19689779/19689779 [01:05<00:00, 299751.78B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 11:48:22,304 copying C:\\Users\\geesi\\AppData\\Local\\Temp\\tmptx5hmvus to cache at C:\\Users\\geesi\\.flair\\embeddings\\lm-news-english-backward-1024-v0.2rc.pt\n",
      "2019-07-20 11:48:22,363 removing temp file C:\\Users\\geesi\\AppData\\Local\\Temp\\tmptx5hmvus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-20 11:48:22,474 {'0', '1'}\n",
      "2019-07-20 11:48:22,481 ----------------------------------------------------------------------------------------------------\n",
      "2019-07-20 11:48:22,482 Evaluation method: MICRO_F1_SCORE\n",
      "2019-07-20 11:48:25,960 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 8.00 GiB total capacity; 6.13 GiB already allocated; 57.52 MiB free; 25.69 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8228cab17519>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m               \u001b[0manneal_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m               \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m               max_epochs=150)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\trainers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, base_path, evaluation_metric, learning_rate, mini_batch_size, eval_mini_batch_size, max_epochs, anneal_factor, patience, train_with_dev, monitor_train, embeddings_in_memory, checkpoint, save_final_model, anneal_with_restarts, shuffle, param_selection_mode, num_workers, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mbatch_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\models\\text_classification_model.py\u001b[0m in \u001b[0;36mforward_loss\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_calculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\models\\text_classification_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         text_embedding_list = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\embeddings.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m   2193\u001b[0m         \u001b[0msentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2195\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2197\u001b[0m         \u001b[1;31m# first, sort sentences by number of tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\embeddings.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\embeddings.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0meverything_embedded\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\embeddings.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m   1181\u001b[0m             \u001b[1;31m# get hidden states from language model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             all_hidden_states_in_lm = self.lm.get_representation(\n\u001b[1;32m-> 1183\u001b[1;33m                 \u001b[0msentences_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchars_per_chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m             )\n\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\models\\language_model.py\u001b[0m in \u001b[0;36mget_representation\u001b[1;34m(self, strings, chars_per_chunk)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflair\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mrnn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\flair\\models\\language_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden, ordered_sequence_lengths)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[1;32m--> 522\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    523\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 8.00 GiB total capacity; 6.13 GiB already allocated; 57.52 MiB free; 25.69 MiB cached)"
     ]
    }
   ],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./data/processed'), \n",
    "                                                       test_file='test.txt', \n",
    "                                                       dev_file='dev.txt', \n",
    "                                                       train_file='train.txt')\n",
    "word_embeddings = [WordEmbeddings('glove'), \n",
    "                   FlairEmbeddings('news-forward-fast'), \n",
    "                   FlairEmbeddings('news-backward-fast')]\n",
    "\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, \n",
    "                            label_dictionary = corpus.make_label_dictionary(), \n",
    "                            multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "# trainer.train('./', max_epochs=10)\n",
    "trainer.train('./data/model',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves('./data/modelloss.tsv')\n",
    "plotter.plot_weights('./data/model/weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load_from_file('./data/model/best-model.pt')\n",
    "sentence = Sentence(\" \".join(test_text[0]))\n",
    "classifier.predict(sentence)\n",
    "print(sentence.labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

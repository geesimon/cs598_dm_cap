{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/raw/Hygiene/hygiene.dat.labels\") as f:\n",
    "    labels = [int(l) for l in f.readlines() if l[0].isdigit()]\n",
    "\n",
    "docs = []\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat\") as f:\n",
    "    for i in range(len(labels)):\n",
    "        docs.append(f.readline().strip(\"\\n\"))\n",
    "\n",
    "with open(\"./data/processed/train.txt\", \"w\") as f:\n",
    "    for i, _doc in enumerate(docs[0:int(len(docs)*0.8)]):\n",
    "        f.write(_doc + \" __label__\" + str(labels[i]) + \"\\n\")\n",
    "\n",
    "with open(\"./data/processed/dev.txt\", \"w\") as f:\n",
    "    for i, _doc in enumerate(docs[int(len(docs)*0.8):int(len(docs)*0.9)]):\n",
    "        f.write(_doc + \" __label__\" + str(labels[i]) + \"\\n\")\n",
    "        \n",
    "with open(\"./data/processed/test.txt\", \"w\") as f:\n",
    "    for i, _doc in enumerate(docs[int(len(docs)*0.9):]):\n",
    "        f.write(_doc + \" __label__\" + str(labels[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_fasttext = pd.DataFrame()\n",
    "train_df = pd.read_csv(\"~/Downloads/dbpedia_csv/train.csv\", header=None)\n",
    "\n",
    "train_fasttext['text'] = train_df.iloc[:,2]\n",
    "train_fasttext['label'] = [\"__label__\" + str(i) for i in train_df.iloc[:,0]]\n",
    "train_fasttext.to_csv(\"~/Downloads/dbpedia_csv/train_fasttext.csv\", header = False, index = False, sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fasttext = pd.DataFrame()\n",
    "test_df = pd.read_csv(\"~/Downloads/dbpedia_csv/test.csv\", header=None)\n",
    "\n",
    "test_fasttext['text'] = test_df.iloc[:,2]\n",
    "test_fasttext['label'] = [\"__label__\" + str(i) for i in test_df.iloc[:,0]]\n",
    "test_fasttext.to_csv(\"~/Downloads/dbpedia_csv/test_fasttext.csv\", header = False, index = False,  sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [00:10<00:00, 48.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    result_stemmed = []\n",
    "    for token in simple_preprocess(text, min_len = 2):\n",
    "        result.append(token)\n",
    "        if token not in STOPWORDS:\n",
    "            result_stemmed.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return (result, result_stemmed)\n",
    "\n",
    "\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat.labels\") as f:\n",
    "    LABELS = [int(i) for i in f.readlines()[0:546]]\n",
    "\n",
    "LABELED_STEMMED_TEXTS = []\n",
    "with open(\"./data/raw/Hygiene/hygiene.dat\") as f:\n",
    "    for i in tqdm(range(len(LABELS))):\n",
    "        _text = f.readline()\n",
    "        _result, _result_stemmed = preprocess(_text)\n",
    "        LABELED_STEMMED_TEXTS.append(_result_stemmed)\n",
    "        \n",
    "train_text = LABELED_STEMMED_TEXTS[0:400]\n",
    "train_label = LABELS[0:400]\n",
    "test_text = LABELED_STEMMED_TEXTS[400:len(LABELS)]\n",
    "test_label = LABELS[400:len(LABELS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dictionary = corpora.Dictionary(train_text)\n",
    "train_count = [_dictionary.doc2bow(_doc) for _doc in train_text]\n",
    "test_count = [_dictionary.doc2bow(_doc) for _doc in test_text]\n",
    "\n",
    "tfidf_model = models.TfidfModel(train_count)\n",
    "train_tfidf = [_doc for _doc in tfidf_model[train_count]]\n",
    "test_tfidf = [_doc for _doc in tfidf_model[test_count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_count = [(dict(_doc), train_label[i]) for i, _doc in enumerate(train_count)]\n",
    "labeled_test_count = [dict(_doc) for _doc in test_count]\n",
    "\n",
    "labeled_train_tfidf = [(dict(_doc), train_label[i]) for i, _doc in enumerate(train_tfidf)]\n",
    "labeled_test_tfidf = [dict(_doc) for _doc in test_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  7]\n",
      " [62 59]]\n",
      "0.5273972602739726 0.8939393939393939 0.48760330578512395 0.6310160427807486\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "NB_classifier_count = NaiveBayesClassifier.train(labeled_train_count)\n",
    "\n",
    "predictions = NB_classifier_count.classify_many(labeled_test_count)\n",
    "\n",
    "print(metrics.confusion_matrix(predictions, test_label))\n",
    "print(metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43 41]\n",
      " [37 25]]\n",
      "0.4657534246575342 0.3787878787878788 0.4032258064516129 0.39062499999999994\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "DT_classifier_count = DecisionTreeClassifier.train(labeled_train_count)\n",
    "predictions = DT_classifier_count.classify_many(labeled_test_count)\n",
    "\n",
    "print(metrics.confusion_matrix(predictions, test_label))\n",
    "print(metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0]\n",
      " [80 66]]\n",
      "0.4520547945205479 1.0 0.4520547945205479 0.6226415094339622\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.maxent import TypedMaxentFeatureEncoding, MaxentClassifier\n",
    "\n",
    "encoding = TypedMaxentFeatureEncoding.train(labeled_train_tfidf, \n",
    "                                                   count_cutoff=3, \n",
    "                                                   alwayson_features=True)\n",
    "Maxent_classifier_count = MaxentClassifier.train(labeled_train_tfidf, \n",
    "                                           bernoulli=False, \n",
    "                                           encoding=encoding, \n",
    "                                           trace=0)\n",
    "\n",
    "predictions = Maxent_classifier_count.classify_many(labeled_test_tfidf)\n",
    "\n",
    "print(metrics.confusion_matrix(predictions, test_label))\n",
    "print(metrics.accuracy_score(predictions, test_label),\n",
    "            metrics.precision_score(predictions, test_label),\n",
    "            metrics.recall_score(predictions, test_label),\n",
    "            metrics.f1_score(predictions, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./spam.csv\", encoding='latin-1').sample(frac=1).drop_duplicates()\n",
    "data = data[['v1', 'v2']].rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
    " \n",
    "data['label'] = '__label__' + data['label'].astype(str)\n",
    "data.iloc[0:int(len(data)*0.8)].to_csv('train.csv', sep='\\t', index = False, header = False)\n",
    "data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv('test.csv', sep='\\t', index = False, header = False)\n",
    "data.iloc[int(len(data)*0.9):].to_csv('dev.csv', sep='\\t', index = False, header = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-19 18:43:11,236 Reading data from data/processed\n",
      "2019-07-19 18:43:11,237 Train: data/processed/train.txt\n",
      "2019-07-19 18:43:11,238 Dev: data/processed/dev.txt\n",
      "2019-07-19 18:43:11,239 Test: data/processed/test.txt\n",
      "2019-07-19 18:43:11,312 this function is deprecated, use smart_open.open instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/flair/data_fetcher.py:447: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/flair/data_fetcher.py:454: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/flair/data_fetcher.py:463: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  max_tokens_per_doc=max_tokens_per_doc,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-19 18:43:13,251 set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-363d2af25c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdocument_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentLSTMEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreproject_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreproject_words_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m classifier = TextClassifier(document_embeddings, \n\u001b[0;32m---> 18\u001b[0;31m                             \u001b[0mlabel_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_label_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                             multi_label=False)\n\u001b[1;32m     20\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/flair/data.py\u001b[0m in \u001b[0;36mmake_label_dictionary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mlabel_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0mmax_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_labels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0mlabel_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./data/processed'), \n",
    "                                                       test_file='test.txt', \n",
    "                                                       dev_file='dev.txt', \n",
    "                                                       train_file='train.txt')\n",
    "word_embeddings = [WordEmbeddings('glove'), \n",
    "                   FlairEmbeddings('news-forward-fast'), \n",
    "                   FlairEmbeddings('news-backward-fast')]\n",
    "\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, \n",
    "                            label_dictionary = corpus.make_label_dictionary(), \n",
    "                            multi_label=False)\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('./', max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flair.data.Corpus at 0x32b1a3e9e8>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load_from_file('./best-model.pt')\n",
    "sentence = Sentence(\" \".join(test_text[0]))\n",
    "classifier.predict(sentence)\n",
    "print(sentence.labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [3:09:54<00:00, 58.16s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before prunn:193047\n",
      "After prunn:100000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "cat_list = glob.glob (\"cuisines/*\")\n",
    "cat_size = len(cat_list)\n",
    "\n",
    "random.seed(0)\n",
    "cat_names = []\n",
    "cat_text = []\n",
    "# sample_size = min(30, cat_size)\n",
    "# cat_sample = sorted(random.sample(range(cat_size), sample_size))\n",
    "cat_sample = range(0, cat_size)\n",
    "\n",
    "count = 0\n",
    "for i in cat_sample:\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_text.append(f.read().replace(\"\\n\", \"\").replace(\"\\r\",\"\"))\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(cat_text)]\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(dictionary)))\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    b = dict(b)\n",
    "    norm_a = 0\n",
    "    norm_b = 0\n",
    "    denom = 0\n",
    "    for a_i, a_v in a:\n",
    "        norm_a += a_v * a_v\n",
    "        if a_i in b:\n",
    "            denom += a_v * b[a_i]\n",
    "    for b_i in b:\n",
    "        norm_b += b[b_i] * b[b_i]\n",
    "    \n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "#     print(norm_a)\n",
    "#     print(norm_b)\n",
    "#     print(denom)\n",
    "    \n",
    "    return denom / (norm_a * norm_b)\n",
    "\n",
    "def top_n(df, n, thresh_hold = 0.1):\n",
    "    df_count = np.zeros(df.shape)\n",
    "    df_bak = df\n",
    "    df_count[df >= thresh_hold] = 1\n",
    "    _counts = np.sum(df_count, axis=1)\n",
    "    max_index = []\n",
    "    for i in range(0, n):\n",
    "        _index = np.argmax(_counts)\n",
    "        max_index.append(_index)\n",
    "        _counts[_index] = -1\n",
    "    \n",
    "    return df.iloc[max_index][df.columns[max_index]]\n",
    "\n",
    "def format_obj(df, groups):\n",
    "    _nodes = \"nodes\"\n",
    "    _links = \"links\"\n",
    "    json_obj = {_nodes:[], _links:[]}\n",
    "    for i in range(0, len(df.columns)):\n",
    "        json_obj[_nodes].append({\"name\":df.columns[i], \"group\":groups[i]})\n",
    "    \n",
    "    for i in range(0, df.shape[0] - 1):\n",
    "        for j in range(i + 1, df.shape[0]):\n",
    "            json_obj[_links].append({\"source\":i, \"target\":j, \"value\":df.iloc[i][j]})\n",
    "    \n",
    "    return json_obj\n",
    "\n",
    "def corpus_similarity(corpus, vector_dimension):\n",
    "#     _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "#     for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#         _sim[i][i] = 1\n",
    "#         for j in range(i + 1, len(corpus)):\n",
    "#             _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#             _sim[j][i] = _sim[i][j]\n",
    "    _corpus_matrix = np.zeros([len(corpus), vector_dimension])\n",
    "    for i, row in enumerate(corpus):\n",
    "        for j, v in row:\n",
    "            _corpus_matrix[i][j] = v\n",
    "    \n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i, j] = spatial.distance.cosine(_corpus_matrix[i], _corpus_matrix[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return 1 - _sim\n",
    "\n",
    "def corpus_similarity_1(corpus):\n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        _sim[i][i] = 1\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return 1 - _sim\n",
    "\n",
    "def slice_df_by_name(df,names):\n",
    "    return df.loc[names][names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:08<00:00, 17.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "# for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#     sim[i][i] = 1\n",
    "#     for j in range(i + 1, len(corpus)):\n",
    "#         sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#         sim[j][i] = sim[i][j]\n",
    "        \n",
    "sim = corpus_similarity(corpus, len(dictionary))\n",
    "\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.index = cat_names\n",
    "sim_df.columns = cat_names\n",
    "data = top_n(sim_df, 50)\n",
    "selected_names = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(data, np.ones(data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns; \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sample = 20\n",
    "# ax = sns.heatmap(data.iloc[0:sample][data.columns[0:sample]],cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:11<00:00, 12.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "tfidf_sim = corpus_similarity(tfidf_corpus, len(dictionary))\n",
    "# np.zeros([len(tfidf_corpus), len(tfidf_corpus)])\n",
    "\n",
    "# for i in tqdm(range(0, len(tfidf_corpus) - 1)):\n",
    "#     tfidf_sim[i][i] = 1\n",
    "#     for j in range(i + 1, len(tfidf_corpus)):\n",
    "#         tfidf_sim[i][j] = cosine_similarity(tfidf_corpus[i], tfidf_corpus[j])\n",
    "#         tfidf_sim[j][i] = tfidf_sim[i][j]\n",
    "        \n",
    "tfidf_sim_df = pd.DataFrame(tfidf_sim)\n",
    "tfidf_sim_df.index = cat_names\n",
    "tfidf_sim_df.columns = cat_names\n",
    "# tfidf_data = top_n(tfidf_sim_df, 50)\n",
    "tfidf_data = slice_df_by_name(tfidf_sim_df, selected_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/tfidf_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_data, np.ones(tfidf_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/gensim/models/ldamodel.py:582: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 74.336288s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "lda_model = models.LdaModel(tfidf_corpus, num_topics = 100, id2word=dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "doc_topics = lda_model.get_document_topics(tfidf_corpus)\n",
    "print(\"done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:12<00:00, 11.60it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_sim = corpus_similarity(doc_topics, len(dictionary))\n",
    "\n",
    "lda_sim_df = pd.DataFrame(lda_sim)\n",
    "lda_sim_df.index = cat_names\n",
    "lda_sim_df.columns = cat_names\n",
    "# lda_data = top_n(lda_sim_df, 50)\n",
    "lda_data = slice_df_by_name(lda_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_data, np.ones(lda_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest_coherence = -1e20\n",
    "# best_k = 0\n",
    "# best_model = None\n",
    "# for k in range(5, 150, 2):\n",
    "#     model = models.LdaModel(tfidf_corpus, num_topics = k, id2word=dictionary)\n",
    "#     cm = models.coherencemodel.CoherenceModel(model=model, corpus=tfidf_corpus, coherence='u_mass')\n",
    "#     coherence = cm.get_coherence()\n",
    "#     print(\"k=%d coherence=%f\"%(k, coherence))\n",
    "#     if (coherence > largest_coherence):\n",
    "#         largest_coherence = coherence\n",
    "#         best_model = model\n",
    "#         best_k = k\n",
    "\n",
    "# print(\"best_k:%d\"%(best_k))\n",
    "# for idx, topic in best_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_file = \"cuisine_indices.txt\"\n",
    "matrix_file = \"cuisine_sim_matrix.csv\"\n",
    "\n",
    "with open (names_file, 'r') as f:\n",
    "    names = f.read().split(\"\\n\")\n",
    "\n",
    "demo_data = pd.read_csv(matrix_file, header=None)\n",
    "demo_data.index = names\n",
    "demo_data.columns = names\n",
    "\n",
    "with open(\"display/demo_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(demo_data, np.ones(demo_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 703473/703473 [2:08:08<00:00, 91.50it/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before prunn:174743\n",
      "After prunn:20242\n"
     ]
    }
   ],
   "source": [
    "path2reviewdump = \"reviews/reviews.dat\"\n",
    "\n",
    "with open(path2reviewdump, \"r\") as f:\n",
    "    reviews = f.readlines()\n",
    "review_docs = [preprocess(text) for text in tqdm(reviews)]\n",
    "review_dictionary = corpora.Dictionary(review_docs)\n",
    "print(\"Before prunn:%d\"%(len(review_dictionary)))\n",
    "review_dictionary.filter_extremes(no_below=15, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(review_dictionary)))\n",
    "review_corpus = [review_dictionary.doc2bow(doc) for doc in review_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7373.724247s\n",
      "Topic: 0 Words: 0.495*\"taco\" + 0.134*\"burrito\" + 0.074*\"tortilla\" + 0.058*\"chipotl\" + 0.034*\"bean\" + 0.030*\"corn\" + 0.026*\"shred\" + 0.023*\"carnita\" + 0.023*\"bell\" + 0.018*\"steakhous\"\n",
      "Topic: 1 Words: 0.204*\"custom\" + 0.181*\"owner\" + 0.090*\"today\" + 0.089*\"thank\" + 0.067*\"welcom\" + 0.049*\"outstand\" + 0.033*\"guest\" + 0.031*\"provid\" + 0.024*\"polit\" + 0.022*\"entertain\"\n",
      "Topic: 2 Words: 0.097*\"cucumb\" + 0.096*\"notch\" + 0.084*\"specialti\" + 0.080*\"gyro\" + 0.069*\"organ\" + 0.055*\"secret\" + 0.050*\"farm\" + 0.046*\"glaze\" + 0.042*\"martini\" + 0.039*\"calori\"\n",
      "Topic: 3 Words: 0.459*\"locat\" + 0.149*\"strip\" + 0.036*\"mall\" + 0.032*\"prime\" + 0.029*\"mile\" + 0.026*\"bright\" + 0.022*\"john\" + 0.021*\"advertis\" + 0.018*\"patient\" + 0.017*\"count\"\n",
      "Topic: 4 Words: 0.299*\"sauc\" + 0.188*\"meat\" + 0.054*\"garlic\" + 0.048*\"bland\" + 0.043*\"salti\" + 0.040*\"tast\" + 0.036*\"piec\" + 0.028*\"dip\" + 0.018*\"plain\" + 0.016*\"tough\"\n",
      "Topic: 5 Words: 0.390*\"roll\" + 0.362*\"sushi\" + 0.061*\"spring\" + 0.032*\"coupon\" + 0.018*\"fresh\" + 0.014*\"piec\" + 0.012*\"spici\" + 0.010*\"print\" + 0.009*\"oreo\" + 0.008*\"pinch\"\n",
      "Topic: 6 Words: 0.297*\"impress\" + 0.069*\"hotel\" + 0.065*\"calamari\" + 0.065*\"doubl\" + 0.051*\"mini\" + 0.039*\"clam\" + 0.039*\"bore\" + 0.039*\"philli\" + 0.039*\"sad\" + 0.038*\"vanilla\"\n",
      "Topic: 7 Words: 0.140*\"music\" + 0.101*\"play\" + 0.100*\"crowd\" + 0.078*\"loud\" + 0.047*\"convers\" + 0.033*\"danc\" + 0.032*\"quiet\" + 0.030*\"loung\" + 0.027*\"live\" + 0.025*\"sear\"\n",
      "Topic: 8 Words: 0.330*\"price\" + 0.217*\"pretti\" + 0.117*\"reason\" + 0.040*\"real\" + 0.040*\"cheap\" + 0.025*\"good\" + 0.024*\"aren\" + 0.021*\"over\" + 0.017*\"qualiti\" + 0.015*\"complaint\"\n",
      "Topic: 9 Words: 0.149*\"word\" + 0.097*\"afternoon\" + 0.071*\"overwhelm\" + 0.070*\"mozzarella\" + 0.070*\"chat\" + 0.068*\"name\" + 0.066*\"pour\" + 0.053*\"york\" + 0.035*\"part\" + 0.030*\"ruin\"\n",
      "Topic: 10 Words: 0.116*\"order\" + 0.046*\"wait\" + 0.045*\"come\" + 0.030*\"say\" + 0.029*\"take\" + 0.026*\"time\" + 0.025*\"tell\" + 0.025*\"minut\" + 0.023*\"go\" + 0.020*\"want\"\n",
      "Topic: 11 Words: 0.377*\"beer\" + 0.151*\"wing\" + 0.111*\"watch\" + 0.087*\"game\" + 0.056*\"select\" + 0.044*\"sport\" + 0.022*\"screen\" + 0.020*\"team\" + 0.016*\"flat\" + 0.012*\"footbal\"\n",
      "Topic: 12 Words: 0.265*\"chip\" + 0.231*\"salsa\" + 0.202*\"mexican\" + 0.087*\"margarita\" + 0.072*\"guacamol\" + 0.053*\"enchilada\" + 0.021*\"tequila\" + 0.019*\"street\" + 0.018*\"authent\" + 0.005*\"fuss\"\n",
      "Topic: 13 Words: 0.176*\"breakfast\" + 0.089*\"bacon\" + 0.073*\"egg\" + 0.069*\"pancak\" + 0.055*\"toast\" + 0.047*\"brunch\" + 0.042*\"morn\" + 0.035*\"french\" + 0.035*\"gravi\" + 0.031*\"waffl\"\n",
      "Topic: 14 Words: 0.123*\"dish\" + 0.061*\"appet\" + 0.050*\"dessert\" + 0.044*\"share\" + 0.038*\"cours\" + 0.036*\"entre\" + 0.029*\"main\" + 0.027*\"present\" + 0.027*\"meal\" + 0.025*\"lobster\"\n",
      "Topic: 15 Words: 0.297*\"chef\" + 0.042*\"finger\" + 0.039*\"kink\" + 0.036*\"execut\" + 0.035*\"fell\" + 0.029*\"asparagus\" + 0.023*\"knock\" + 0.023*\"degre\" + 0.022*\"gras\" + 0.021*\"palat\"\n",
      "Topic: 16 Words: 0.262*\"larg\" + 0.167*\"group\" + 0.165*\"parti\" + 0.060*\"accommod\" + 0.037*\"larger\" + 0.030*\"specif\" + 0.027*\"cater\" + 0.026*\"lay\" + 0.022*\"freak\" + 0.012*\"event\"\n",
      "Topic: 17 Words: 0.232*\"wife\" + 0.090*\"rock\" + 0.064*\"chain\" + 0.062*\"sell\" + 0.049*\"smaller\" + 0.049*\"caesar\" + 0.048*\"airport\" + 0.045*\"agre\" + 0.034*\"countri\" + 0.031*\"cheaper\"\n",
      "Topic: 18 Words: 0.287*\"water\" + 0.136*\"mouth\" + 0.042*\"spoon\" + 0.031*\"layer\" + 0.031*\"describ\" + 0.028*\"fan\" + 0.023*\"ceil\" + 0.022*\"dri\" + 0.022*\"magic\" + 0.019*\"curd\"\n",
      "Topic: 19 Words: 0.147*\"butter\" + 0.079*\"banana\" + 0.076*\"delight\" + 0.075*\"section\" + 0.069*\"peanut\" + 0.063*\"conveni\" + 0.061*\"bigger\" + 0.040*\"purchas\" + 0.033*\"passion\" + 0.032*\"children\"\n",
      "Topic: 20 Words: 0.091*\"ayc\" + 0.079*\"craft\" + 0.077*\"lime\" + 0.072*\"load\" + 0.072*\"sprout\" + 0.065*\"skin\" + 0.063*\"tuesday\" + 0.057*\"quesadilla\" + 0.044*\"cheesi\" + 0.029*\"coney\"\n",
      "Topic: 21 Words: 0.207*\"insid\" + 0.167*\"outsid\" + 0.137*\"patio\" + 0.064*\"beauti\" + 0.056*\"station\" + 0.049*\"view\" + 0.048*\"window\" + 0.047*\"korean\" + 0.032*\"creativ\" + 0.020*\"weather\"\n",
      "Topic: 22 Words: 0.224*\"potato\" + 0.200*\"sweet\" + 0.184*\"line\" + 0.086*\"corn\" + 0.069*\"bake\" + 0.042*\"version\" + 0.034*\"mash\" + 0.029*\"regist\" + 0.014*\"inspir\" + 0.013*\"strike\"\n",
      "Topic: 23 Words: 0.181*\"style\" + 0.109*\"chines\" + 0.089*\"asian\" + 0.081*\"authent\" + 0.067*\"american\" + 0.059*\"dumpl\" + 0.055*\"crepe\" + 0.051*\"cuisin\" + 0.023*\"needl\" + 0.021*\"grit\"\n",
      "Topic: 24 Words: 0.122*\"yelp\" + 0.119*\"drive\" + 0.090*\"glad\" + 0.085*\"read\" + 0.064*\"downtown\" + 0.059*\"review\" + 0.055*\"build\" + 0.043*\"decid\" + 0.041*\"shoot\" + 0.026*\"temp\"\n",
      "Topic: 25 Words: 0.385*\"night\" + 0.086*\"late\" + 0.083*\"saturday\" + 0.074*\"friday\" + 0.055*\"open\" + 0.053*\"sunday\" + 0.040*\"club\" + 0.027*\"monday\" + 0.019*\"wednesday\" + 0.015*\"tray\"\n",
      "Topic: 26 Words: 0.127*\"walk\" + 0.120*\"clean\" + 0.070*\"door\" + 0.058*\"counter\" + 0.048*\"open\" + 0.042*\"sign\" + 0.035*\"modern\" + 0.028*\"pictur\" + 0.021*\"kitchen\" + 0.021*\"greet\"\n",
      "Topic: 27 Words: 0.215*\"manag\" + 0.115*\"wrong\" + 0.062*\"terribl\" + 0.040*\"mess\" + 0.034*\"cashier\" + 0.034*\"correct\" + 0.029*\"apolog\" + 0.029*\"mistak\" + 0.023*\"proper\" + 0.023*\"handl\"\n",
      "Topic: 28 Words: 0.165*\"wrap\" + 0.140*\"boyfriend\" + 0.107*\"melt\" + 0.083*\"similar\" + 0.047*\"admit\" + 0.040*\"lettuc\" + 0.034*\"coleslaw\" + 0.032*\"fork\" + 0.030*\"scoop\" + 0.028*\"vinegar\"\n",
      "Topic: 29 Words: 0.100*\"guy\" + 0.089*\"spend\" + 0.081*\"shake\" + 0.058*\"dollar\" + 0.045*\"book\" + 0.044*\"yeah\" + 0.041*\"bunch\" + 0.033*\"dont\" + 0.028*\"flight\" + 0.026*\"anyway\"\n",
      "Topic: 30 Words: 0.310*\"option\" + 0.094*\"compar\" + 0.084*\"standard\" + 0.064*\"limit\" + 0.052*\"frequent\" + 0.025*\"travel\" + 0.022*\"offer\" + 0.020*\"vegetarian\" + 0.019*\"jack\" + 0.018*\"custard\"\n",
      "Topic: 31 Words: 0.410*\"chees\" + 0.149*\"grill\" + 0.080*\"veggi\" + 0.044*\"vegan\" + 0.043*\"vegetarian\" + 0.040*\"blue\" + 0.027*\"caramel\" + 0.027*\"tomato\" + 0.022*\"goat\" + 0.020*\"onion\"\n",
      "Topic: 32 Words: 0.194*\"issu\" + 0.173*\"asada\" + 0.167*\"carn\" + 0.060*\"awhil\" + 0.060*\"express\" + 0.060*\"match\" + 0.027*\"echo\" + 0.027*\"drunken\" + 0.026*\"panda\" + 0.023*\"back\"\n",
      "Topic: 33 Words: 0.311*\"hous\" + 0.106*\"deliv\" + 0.101*\"establish\" + 0.070*\"deliveri\" + 0.058*\"creat\" + 0.037*\"onlin\" + 0.033*\"websit\" + 0.028*\"current\" + 0.027*\"kill\" + 0.024*\"lower\"\n",
      "Topic: 34 Words: 0.246*\"away\" + 0.097*\"split\" + 0.072*\"blow\" + 0.056*\"babi\" + 0.049*\"skip\" + 0.038*\"skewer\" + 0.030*\"trust\" + 0.030*\"challeng\" + 0.025*\"ventur\" + 0.022*\"browni\"\n",
      "Topic: 35 Words: 0.391*\"close\" + 0.105*\"birthday\" + 0.089*\"except\" + 0.056*\"celebr\" + 0.048*\"draft\" + 0.040*\"anymor\" + 0.032*\"hit\" + 0.026*\"competit\" + 0.024*\"sing\" + 0.022*\"rough\"\n",
      "Topic: 36 Words: 0.277*\"yummi\" + 0.091*\"moist\" + 0.090*\"hubbi\" + 0.084*\"futur\" + 0.049*\"extens\" + 0.045*\"guac\" + 0.035*\"seed\" + 0.031*\"tamal\" + 0.031*\"quinoa\" + 0.023*\"intim\"\n",
      "Topic: 37 Words: 0.155*\"lamb\" + 0.073*\"process\" + 0.047*\"mediterranean\" + 0.040*\"individu\" + 0.040*\"falafel\" + 0.039*\"golden\" + 0.038*\"risotto\" + 0.037*\"catfish\" + 0.037*\"cauliflow\" + 0.034*\"complex\"\n",
      "Topic: 38 Words: 0.124*\"sampl\" + 0.081*\"crazi\" + 0.066*\"girlfriend\" + 0.063*\"step\" + 0.056*\"grow\" + 0.056*\"east\" + 0.050*\"deserv\" + 0.038*\"compani\" + 0.034*\"condiment\" + 0.031*\"messi\"\n",
      "Topic: 39 Words: 0.222*\"buffet\" + 0.136*\"varieti\" + 0.066*\"indian\" + 0.058*\"dessert\" + 0.050*\"select\" + 0.028*\"item\" + 0.026*\"naan\" + 0.021*\"wonton\" + 0.020*\"includ\" + 0.019*\"masala\"\n",
      "Topic: 40 Words: 0.418*\"chicken\" + 0.143*\"soup\" + 0.085*\"noodl\" + 0.036*\"rice\" + 0.030*\"beef\" + 0.028*\"orang\" + 0.027*\"tofu\" + 0.026*\"bowl\" + 0.025*\"veget\" + 0.023*\"fri\"\n",
      "Topic: 41 Words: 0.091*\"oven\" + 0.075*\"bathroom\" + 0.074*\"syrup\" + 0.067*\"south\" + 0.058*\"photo\" + 0.042*\"neat\" + 0.033*\"public\" + 0.032*\"mapl\" + 0.029*\"mahi\" + 0.023*\"trash\"\n",
      "Topic: 42 Words: 0.233*\"ramen\" + 0.086*\"middl\" + 0.059*\"brew\" + 0.059*\"wood\" + 0.041*\"lemonad\" + 0.041*\"thursday\" + 0.037*\"brother\" + 0.034*\"winner\" + 0.028*\"eastern\" + 0.025*\"root\"\n",
      "Topic: 43 Words: 0.087*\"hate\" + 0.077*\"ketchup\" + 0.072*\"mustard\" + 0.055*\"king\" + 0.046*\"adult\" + 0.042*\"temperatur\" + 0.041*\"form\" + 0.040*\"stack\" + 0.038*\"altern\" + 0.035*\"text\"\n",
      "Topic: 44 Words: 0.146*\"like\" + 0.054*\"think\" + 0.033*\"look\" + 0.033*\"thing\" + 0.031*\"know\" + 0.028*\"tast\" + 0.026*\"feel\" + 0.020*\"expect\" + 0.019*\"wasn\" + 0.019*\"kind\"\n",
      "Topic: 45 Words: 0.138*\"short\" + 0.131*\"kid\" + 0.093*\"rib\" + 0.084*\"smoke\" + 0.076*\"brisket\" + 0.072*\"turkey\" + 0.060*\"side\" + 0.053*\"cooki\" + 0.052*\"bone\" + 0.038*\"platter\"\n",
      "Topic: 46 Words: 0.506*\"vega\" + 0.072*\"daughter\" + 0.049*\"learn\" + 0.032*\"discov\" + 0.030*\"subway\" + 0.027*\"sub\" + 0.027*\"last\" + 0.023*\"hospit\" + 0.019*\"squid\" + 0.018*\"décor\"\n",
      "Topic: 47 Words: 0.082*\"break\" + 0.075*\"confus\" + 0.061*\"follow\" + 0.057*\"post\" + 0.039*\"gourmet\" + 0.038*\"stomach\" + 0.037*\"asid\" + 0.033*\"find\" + 0.033*\"attempt\" + 0.031*\"concern\"\n",
      "Topic: 48 Words: 0.258*\"pork\" + 0.205*\"rice\" + 0.197*\"shrimp\" + 0.102*\"bean\" + 0.073*\"seafood\" + 0.052*\"pull\" + 0.026*\"california\" + 0.019*\"cabbag\" + 0.010*\"carrot\" + 0.008*\"plate\"\n",
      "Topic: 49 Words: 0.430*\"salad\" + 0.139*\"green\" + 0.070*\"dress\" + 0.069*\"chili\" + 0.051*\"avocado\" + 0.029*\"lemon\" + 0.028*\"fruit\" + 0.020*\"lettuc\" + 0.019*\"tomato\" + 0.014*\"terrif\"\n",
      "Topic: 50 Words: 0.129*\"comfort\" + 0.105*\"cute\" + 0.099*\"earli\" + 0.090*\"casual\" + 0.062*\"smoothi\" + 0.056*\"set\" + 0.045*\"effici\" + 0.038*\"movi\" + 0.032*\"scene\" + 0.030*\"central\"\n",
      "Topic: 51 Words: 0.132*\"year\" + 0.121*\"chang\" + 0.085*\"rememb\" + 0.068*\"forward\" + 0.067*\"origin\" + 0.062*\"past\" + 0.057*\"recent\" + 0.044*\"patti\" + 0.042*\"honey\" + 0.032*\"updat\"\n",
      "Topic: 52 Words: 0.153*\"uniqu\" + 0.138*\"tuna\" + 0.120*\"cafe\" + 0.098*\"strawberri\" + 0.044*\"chair\" + 0.041*\"strang\" + 0.041*\"chill\" + 0.032*\"whip\" + 0.028*\"social\" + 0.022*\"comfi\"\n",
      "Topic: 53 Words: 0.191*\"joint\" + 0.106*\"meet\" + 0.094*\"citi\" + 0.094*\"chicago\" + 0.084*\"dog\" + 0.037*\"oili\" + 0.035*\"oper\" + 0.029*\"tostada\" + 0.027*\"press\" + 0.021*\"fake\"\n",
      "Topic: 54 Words: 0.145*\"pick\" + 0.076*\"ladi\" + 0.076*\"explain\" + 0.066*\"speak\" + 0.062*\"question\" + 0.049*\"phone\" + 0.048*\"lose\" + 0.040*\"answer\" + 0.039*\"market\" + 0.030*\"worri\"\n",
      "Topic: 55 Words: 0.294*\"fri\" + 0.290*\"burger\" + 0.101*\"steak\" + 0.043*\"onion\" + 0.030*\"bacon\" + 0.028*\"juici\" + 0.027*\"ring\" + 0.014*\"french\" + 0.013*\"cheddar\" + 0.013*\"bun\"\n",
      "Topic: 56 Words: 0.335*\"area\" + 0.099*\"space\" + 0.086*\"seat\" + 0.053*\"outdoor\" + 0.044*\"design\" + 0.043*\"tempura\" + 0.036*\"interior\" + 0.033*\"truck\" + 0.024*\"milkshak\" + 0.024*\"gelato\"\n",
      "Topic: 57 Words: 0.143*\"italian\" + 0.125*\"pasta\" + 0.090*\"meatbal\" + 0.066*\"oliv\" + 0.052*\"tomato\" + 0.036*\"eggplant\" + 0.033*\"basil\" + 0.031*\"cart\" + 0.028*\"spaghetti\" + 0.026*\"garden\"\n",
      "Topic: 58 Words: 0.194*\"park\" + 0.099*\"japanes\" + 0.091*\"chop\" + 0.066*\"burn\" + 0.047*\"suck\" + 0.045*\"cilantro\" + 0.037*\"giant\" + 0.037*\"fail\" + 0.028*\"recip\" + 0.026*\"ticket\"\n",
      "Topic: 59 Words: 0.433*\"sandwich\" + 0.244*\"bread\" + 0.053*\"pickl\" + 0.037*\"mayo\" + 0.029*\"ball\" + 0.020*\"spread\" + 0.019*\"slice\" + 0.015*\"deli\" + 0.012*\"toast\" + 0.010*\"french\"\n",
      "Topic: 60 Words: 0.205*\"okay\" + 0.126*\"black\" + 0.089*\"corner\" + 0.071*\"particular\" + 0.052*\"doubt\" + 0.048*\"summer\" + 0.036*\"ginger\" + 0.032*\"effort\" + 0.030*\"rais\" + 0.029*\"latt\"\n",
      "Topic: 61 Words: 0.184*\"wine\" + 0.136*\"glass\" + 0.122*\"list\" + 0.084*\"bottl\" + 0.052*\"knowledg\" + 0.045*\"fabul\" + 0.034*\"pair\" + 0.032*\"win\" + 0.019*\"select\" + 0.014*\"berri\"\n",
      "Topic: 62 Words: 0.218*\"spici\" + 0.138*\"thai\" + 0.124*\"curri\" + 0.062*\"spice\" + 0.051*\"coconut\" + 0.044*\"level\" + 0.042*\"mango\" + 0.036*\"milk\" + 0.028*\"steam\" + 0.022*\"sticki\"\n",
      "Topic: 63 Words: 0.168*\"room\" + 0.134*\"din\" + 0.063*\"booth\" + 0.061*\"truli\" + 0.049*\"true\" + 0.047*\"young\" + 0.044*\"posit\" + 0.042*\"allow\" + 0.038*\"pineappl\" + 0.028*\"mark\"\n",
      "Topic: 64 Words: 0.301*\"cream\" + 0.102*\"soda\" + 0.058*\"yelper\" + 0.048*\"fountain\" + 0.039*\"earlier\" + 0.038*\"tot\" + 0.032*\"cowork\" + 0.028*\"fellow\" + 0.027*\"accord\" + 0.026*\"courteous\"\n",
      "Topic: 65 Words: 0.146*\"store\" + 0.080*\"buy\" + 0.076*\"vietnames\" + 0.072*\"protein\" + 0.064*\"straight\" + 0.054*\"addict\" + 0.035*\"hipster\" + 0.034*\"sprinkl\" + 0.033*\"flatbread\" + 0.028*\"groceri\"\n",
      "Topic: 66 Words: 0.217*\"ambianc\" + 0.133*\"valu\" + 0.110*\"ice\" + 0.080*\"bomb\" + 0.059*\"chorizo\" + 0.052*\"mother\" + 0.048*\"southern\" + 0.047*\"pure\" + 0.037*\"holi\" + 0.035*\"fashion\"\n",
      "Topic: 67 Words: 0.566*\"pizza\" + 0.097*\"crust\" + 0.076*\"slice\" + 0.057*\"top\" + 0.035*\"groupon\" + 0.021*\"madison\" + 0.016*\"sausag\" + 0.013*\"style\" + 0.011*\"ingredi\" + 0.007*\"pie\"\n",
      "Topic: 68 Words: 0.181*\"salmon\" + 0.130*\"plan\" + 0.092*\"oyster\" + 0.078*\"rush\" + 0.063*\"dine\" + 0.057*\"sister\" + 0.048*\"pesto\" + 0.040*\"heat\" + 0.031*\"cheesecak\" + 0.031*\"focus\"\n",
      "Topic: 69 Words: 0.196*\"local\" + 0.187*\"coffe\" + 0.134*\"shop\" + 0.046*\"support\" + 0.044*\"pastri\" + 0.036*\"higher\" + 0.031*\"twist\" + 0.026*\"creme\" + 0.024*\"bakeri\" + 0.021*\"display\"\n",
      "Topic: 70 Words: 0.472*\"best\" + 0.144*\"town\" + 0.069*\"phoenix\" + 0.044*\"life\" + 0.042*\"move\" + 0.024*\"buffalo\" + 0.021*\"north\" + 0.021*\"venu\" + 0.020*\"theme\" + 0.019*\"miss\"\n",
      "Topic: 71 Words: 0.187*\"wall\" + 0.109*\"pita\" + 0.098*\"hummus\" + 0.081*\"own\" + 0.068*\"yesterday\" + 0.060*\"lucki\" + 0.055*\"hole\" + 0.043*\"random\" + 0.042*\"bistro\" + 0.028*\"instant\"\n",
      "Topic: 72 Words: 0.110*\"grand\" + 0.100*\"afford\" + 0.073*\"spectacular\" + 0.065*\"upscal\" + 0.065*\"smooth\" + 0.061*\"crawfish\" + 0.059*\"app\" + 0.058*\"let\" + 0.045*\"thumb\" + 0.032*\"exceed\"\n",
      "Topic: 73 Words: 0.068*\"school\" + 0.057*\"unlik\" + 0.057*\"compliment\" + 0.050*\"english\" + 0.043*\"bitter\" + 0.041*\"culinari\" + 0.039*\"surround\" + 0.032*\"fight\" + 0.030*\"requir\" + 0.029*\"safe\"\n",
      "Topic: 74 Words: 0.075*\"rang\" + 0.067*\"featur\" + 0.064*\"boil\" + 0.052*\"eateri\" + 0.050*\"tend\" + 0.033*\"legit\" + 0.031*\"communiti\" + 0.029*\"page\" + 0.029*\"produc\" + 0.028*\"brat\"\n",
      "Topic: 75 Words: 0.255*\"star\" + 0.179*\"review\" + 0.067*\"give\" + 0.050*\"money\" + 0.048*\"write\" + 0.046*\"base\" + 0.036*\"rat\" + 0.031*\"save\" + 0.027*\"previous\" + 0.025*\"mediocr\"\n",
      "Topic: 76 Words: 0.118*\"smile\" + 0.082*\"ranch\" + 0.068*\"pretzel\" + 0.063*\"profession\" + 0.061*\"cajun\" + 0.058*\"brand\" + 0.042*\"shave\" + 0.037*\"coat\" + 0.035*\"cheer\" + 0.035*\"hurri\"\n",
      "Topic: 77 Words: 0.159*\"extra\" + 0.115*\"pay\" + 0.109*\"charg\" + 0.071*\"cost\" + 0.060*\"tini\" + 0.060*\"buck\" + 0.053*\"overpric\" + 0.042*\"ridicul\" + 0.033*\"hype\" + 0.030*\"alright\"\n",
      "Topic: 78 Words: 0.075*\"place\" + 0.073*\"good\" + 0.057*\"great\" + 0.037*\"servic\" + 0.033*\"time\" + 0.031*\"friend\" + 0.030*\"love\" + 0.024*\"nice\" + 0.024*\"come\" + 0.021*\"menu\"\n",
      "Topic: 79 Words: 0.140*\"simpli\" + 0.103*\"caus\" + 0.089*\"mild\" + 0.081*\"greek\" + 0.078*\"closer\" + 0.042*\"croissant\" + 0.036*\"butteri\" + 0.034*\"feta\" + 0.031*\"angri\" + 0.029*\"anticip\"\n",
      "Topic: 80 Words: 0.332*\"recommend\" + 0.252*\"high\" + 0.147*\"surpris\" + 0.076*\"pleasant\" + 0.040*\"desert\" + 0.030*\"invit\" + 0.019*\"sangria\" + 0.015*\"peach\" + 0.014*\"balsam\" + 0.009*\"cloth\"\n",
      "Topic: 81 Words: 0.179*\"employe\" + 0.154*\"belli\" + 0.127*\"card\" + 0.052*\"leg\" + 0.051*\"cash\" + 0.050*\"credit\" + 0.045*\"occas\" + 0.044*\"favor\" + 0.042*\"self\" + 0.029*\"steal\"\n",
      "Topic: 82 Words: 0.138*\"face\" + 0.086*\"donut\" + 0.083*\"daili\" + 0.081*\"sens\" + 0.048*\"undercook\" + 0.044*\"snow\" + 0.036*\"citrus\" + 0.031*\"bourbon\" + 0.020*\"skimpi\" + 0.019*\"visitor\"\n",
      "Topic: 83 Words: 0.315*\"lunch\" + 0.205*\"special\" + 0.134*\"week\" + 0.079*\"deal\" + 0.066*\"month\" + 0.065*\"healthi\" + 0.032*\"beat\" + 0.012*\"item\" + 0.010*\"includ\" + 0.009*\"dinner\"\n",
      "Topic: 84 Words: 0.469*\"drink\" + 0.118*\"cool\" + 0.084*\"bartend\" + 0.039*\"hang\" + 0.023*\"alcohol\" + 0.022*\"catch\" + 0.017*\"event\" + 0.017*\"bar\" + 0.017*\"gotta\" + 0.016*\"strong\"\n",
      "Topic: 85 Words: 0.145*\"medium\" + 0.116*\"rare\" + 0.111*\"duck\" + 0.080*\"deep\" + 0.060*\"slaw\" + 0.052*\"multipl\" + 0.044*\"char\" + 0.038*\"almond\" + 0.038*\"pink\" + 0.033*\"dream\"\n",
      "Topic: 86 Words: 0.178*\"crab\" + 0.142*\"mushroom\" + 0.075*\"roast\" + 0.057*\"scallop\" + 0.049*\"parmesan\" + 0.031*\"garlic\" + 0.026*\"breast\" + 0.024*\"herb\" + 0.020*\"marinara\" + 0.018*\"vinaigrett\"\n",
      "Topic: 87 Words: 0.357*\"free\" + 0.102*\"gluten\" + 0.098*\"valley\" + 0.058*\"product\" + 0.045*\"coke\" + 0.036*\"diet\" + 0.032*\"cherri\" + 0.032*\"substitut\" + 0.027*\"fulli\" + 0.024*\"join\"\n",
      "Topic: 88 Words: 0.197*\"averag\" + 0.143*\"pricey\" + 0.102*\"opinion\" + 0.087*\"worker\" + 0.052*\"mari\" + 0.051*\"bloodi\" + 0.045*\"rate\" + 0.029*\"poke\" + 0.027*\"skeptic\" + 0.023*\"earn\"\n",
      "Topic: 89 Words: 0.216*\"chocol\" + 0.189*\"cake\" + 0.138*\"date\" + 0.068*\"dessert\" + 0.064*\"dark\" + 0.044*\"filet\" + 0.027*\"unbeliev\" + 0.024*\"overlook\" + 0.021*\"lead\" + 0.021*\"nativ\"\n",
      "Topic: 90 Words: 0.400*\"happi\" + 0.398*\"hour\" + 0.083*\"juic\" + 0.030*\"chile\" + 0.018*\"vodka\" + 0.013*\"half\" + 0.009*\"runner\" + 0.007*\"karaok\" + 0.006*\"applebe\" + 0.005*\"appet\"\n",
      "Topic: 91 Words: 0.104*\"treat\" + 0.084*\"poor\" + 0.071*\"appl\" + 0.068*\"center\" + 0.051*\"board\" + 0.043*\"signatur\" + 0.037*\"replac\" + 0.034*\"natur\" + 0.034*\"situat\" + 0.031*\"industri\"\n",
      "Topic: 92 Words: 0.157*\"portion\" + 0.154*\"spot\" + 0.118*\"size\" + 0.088*\"bowl\" + 0.086*\"beef\" + 0.047*\"combo\" + 0.042*\"crave\" + 0.039*\"satisfi\" + 0.035*\"small\" + 0.035*\"solid\"\n",
      "Topic: 93 Words: 0.135*\"vibe\" + 0.089*\"rave\" + 0.074*\"ambienc\" + 0.047*\"regret\" + 0.045*\"constant\" + 0.042*\"opportun\" + 0.041*\"parent\" + 0.040*\"haha\" + 0.037*\"crack\" + 0.030*\"child\"\n",
      "Topic: 94 Words: 0.086*\"cold\" + 0.054*\"concept\" + 0.053*\"horribl\" + 0.043*\"tonight\" + 0.043*\"freez\" + 0.041*\"smell\" + 0.040*\"worst\" + 0.035*\"soggi\" + 0.030*\"aw\" + 0.025*\"throw\"\n",
      "Topic: 95 Words: 0.108*\"nacho\" + 0.087*\"sour\" + 0.076*\"contain\" + 0.074*\"jalapeno\" + 0.070*\"pasti\" + 0.063*\"soft\" + 0.059*\"pastor\" + 0.057*\"shell\" + 0.050*\"grind\" + 0.039*\"fajita\"\n",
      "Topic: 96 Words: 0.383*\"fish\" + 0.198*\"cocktail\" + 0.036*\"luck\" + 0.034*\"batter\" + 0.026*\"scratch\" + 0.025*\"tail\" + 0.025*\"that\" + 0.024*\"decis\" + 0.024*\"dead\" + 0.017*\"fishi\"\n",
      "Topic: 97 Words: 0.103*\"tabl\" + 0.083*\"server\" + 0.049*\"restaur\" + 0.036*\"experi\" + 0.033*\"servic\" + 0.031*\"waiter\" + 0.026*\"seat\" + 0.020*\"slow\" + 0.020*\"check\" + 0.020*\"host\"\n",
      "Topic: 98 Words: 0.146*\"flavor\" + 0.101*\"perfect\" + 0.062*\"cook\" + 0.034*\"crispi\" + 0.033*\"season\" + 0.032*\"light\" + 0.031*\"pepper\" + 0.027*\"sweet\" + 0.026*\"bite\" + 0.026*\"tender\"\n",
      "Topic: 99 Words: 0.154*\"white\" + 0.090*\"sausag\" + 0.087*\"homemad\" + 0.070*\"pud\" + 0.061*\"delish\" + 0.053*\"sugar\" + 0.051*\"damn\" + 0.046*\"crunch\" + 0.042*\"phenomen\" + 0.042*\"dough\"\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "review_model = models.LdaModel(review_corpus, num_topics=100, id2word=review_dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "for idx, topic in review_model.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [2:53:34<00:00, 35.55s/it]    \n"
     ]
    }
   ],
   "source": [
    "def combine_topics(cat_topics):\n",
    "    topics = {}\n",
    "    for _sub_topics in cat_topics:\n",
    "        for _topic, _value in _sub_topics:\n",
    "            if _topic in topics:\n",
    "                topics[_topic] += _value\n",
    "            else:\n",
    "                topics[_topic] = _value\n",
    "    \n",
    "    return topics\n",
    "\n",
    "all_topics = []\n",
    "cat_names = []\n",
    "for i in tqdm(range(0, len(cat_list))):\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_docs = [preprocess(text) for text in f.readlines()]\n",
    "        cat_corpus = [review_dictionary.doc2bow(doc) for doc in cat_docs]\n",
    "        cat_topics = review_model[cat_corpus]\n",
    "        all_topics.append(combine_topics(cat_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:01<00:00, 103.61it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_individual_sim = corpus_similarity([[(k, topic[k]) for k in topic] for topic in all_topics], len(review_dictionary))\n",
    "\n",
    "lda_individual_sim_df = pd.DataFrame(lda_individual_sim)\n",
    "lda_individual_sim_df.index = cat_names\n",
    "lda_individual_sim_df.columns = cat_names\n",
    "lda_individual_data = top_n(lda_individual_sim_df, 50)\n",
    "\n",
    "with open(\"display/lda_ind_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_individual_data, np.ones(lda_individual_data.shape[0]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

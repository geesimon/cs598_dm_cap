{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [25:27<00:00,  8.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before prunn:193061\n",
      "After prunn:100000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "cat_list = glob.glob (\"cuisines/*\")\n",
    "cat_size = len(cat_list)\n",
    "\n",
    "random.seed(0)\n",
    "cat_names = []\n",
    "cat_text = []\n",
    "# sample_size = min(30, cat_size)\n",
    "# cat_sample = sorted(random.sample(range(cat_size), sample_size))\n",
    "cat_sample = range(0, cat_size)\n",
    "\n",
    "count = 0\n",
    "for i in cat_sample:\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_text.append(f.read().replace(\"\\n\", \"\").replace(\"\\r\",\"\"))\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(cat_text)]\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(dictionary)))\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    b = dict(b)\n",
    "    norm_a = 0\n",
    "    norm_b = 0\n",
    "    denom = 0\n",
    "    for a_i, a_v in a:\n",
    "        norm_a += a_v * a_v\n",
    "        if a_i in b:\n",
    "            denom += a_v * b[a_i]\n",
    "    for b_i in b:\n",
    "        norm_b += b[b_i] * b[b_i]\n",
    "    \n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "#     print(norm_a)\n",
    "#     print(norm_b)\n",
    "#     print(denom)\n",
    "    \n",
    "    return denom / (norm_a * norm_b)\n",
    "\n",
    "def top_n(df, n, thresh_hold = 0.1):\n",
    "    df_count = np.zeros(df.shape)\n",
    "    df_bak = df\n",
    "    df_count[df >= thresh_hold] = 1\n",
    "    _counts = np.sum(df_count, axis=1)\n",
    "    max_index = []\n",
    "    for i in range(0, n):\n",
    "        _index = np.argmax(_counts)\n",
    "        max_index.append(_index)\n",
    "        _counts[_index] = -1\n",
    "    \n",
    "    return df.iloc[max_index][df.columns[max_index]]\n",
    "\n",
    "def format_obj(df, groups):\n",
    "    _nodes = \"nodes\"\n",
    "    _links = \"links\"\n",
    "    json_obj = {_nodes:[], _links:[]}\n",
    "    for i in range(0, len(df.columns)):\n",
    "        json_obj[_nodes].append({\"name\":df.columns[i], \"group\":groups[i]})\n",
    "    \n",
    "    for i in range(0, df.shape[0] - 1):\n",
    "        for j in range(i + 1, df.shape[0]):\n",
    "            json_obj[_links].append({\"source\":i, \"target\":j, \"value\":df.iloc[i][j]})\n",
    "    \n",
    "    return json_obj\n",
    "\n",
    "def corpus_similarity(corpus, vector_dimension):\n",
    "#     _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "#     for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#         _sim[i][i] = 1\n",
    "#         for j in range(i + 1, len(corpus)):\n",
    "#             _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#             _sim[j][i] = _sim[i][j]\n",
    "    _corpus_matrix = np.zeros([len(corpus), vector_dimension])\n",
    "    for i, row in enumerate(corpus):\n",
    "        for j, v in row:\n",
    "            _corpus_matrix[i][j] = v\n",
    "    \n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i, j] = spatial.distance.cosine(_corpus_matrix[i], _corpus_matrix[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return 1 - _sim\n",
    "\n",
    "def corpus_similarity_1(corpus):\n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        _sim[i][i] = 1\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return 1 - _sim\n",
    "\n",
    "def slice_df_by_name(df,names):\n",
    "    return df.loc[names][names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [00:04<00:00, 33.29it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 146 elements, new values have 150 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-967db7834f5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msim_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msim_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0msim_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5078\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5079\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5080\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5081\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5082\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    153\u001b[0m             raise ValueError(\n\u001b[1;32m    154\u001b[0m                 \u001b[0;34m'Length mismatch: Expected axis has {old} elements, new '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 'values have {new} elements'.format(old=old_len, new=new_len))\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 146 elements, new values have 150 elements"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "# for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#     sim[i][i] = 1\n",
    "#     for j in range(i + 1, len(corpus)):\n",
    "#         sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#         sim[j][i] = sim[i][j]\n",
    "        \n",
    "sim = corpus_similarity(corpus, len(dictionary))\n",
    "\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.index = cat_names\n",
    "sim_df.columns = cat_names\n",
    "data = top_n(sim_df, 50)\n",
    "selected_names = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(data, np.ones(data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import seaborn as sns; \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sample = 20\n",
    "# ax = sns.heatmap(data.iloc[0:sample][data.columns[0:sample]],cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [00:04<00:00, 33.51it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 146 elements, new values have 150 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-1239a090d2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtfidf_sim_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtfidf_sim_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtfidf_sim_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# tfidf_data = top_n(tfidf_sim_df, 50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5078\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5079\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5080\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5081\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5082\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    153\u001b[0m             raise ValueError(\n\u001b[1;32m    154\u001b[0m                 \u001b[0;34m'Length mismatch: Expected axis has {old} elements, new '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 'values have {new} elements'.format(old=old_len, new=new_len))\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 146 elements, new values have 150 elements"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "tfidf_sim = corpus_similarity(tfidf_corpus, len(dictionary))\n",
    "# np.zeros([len(tfidf_corpus), len(tfidf_corpus)])\n",
    "\n",
    "# for i in tqdm(range(0, len(tfidf_corpus) - 1)):\n",
    "#     tfidf_sim[i][i] = 1\n",
    "#     for j in range(i + 1, len(tfidf_corpus)):\n",
    "#         tfidf_sim[i][j] = cosine_similarity(tfidf_corpus[i], tfidf_corpus[j])\n",
    "#         tfidf_sim[j][i] = tfidf_sim[i][j]\n",
    "        \n",
    "tfidf_sim_df = pd.DataFrame(tfidf_sim)\n",
    "tfidf_sim_df.index = cat_names\n",
    "tfidf_sim_df.columns = cat_names\n",
    "# tfidf_data = top_n(tfidf_sim_df, 50)\n",
    "tfidf_data = slice_df_by_name(tfidf_sim_df, selected_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/tfidf_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_data, np.ones(tfidf_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\gensim\\models\\ldamodel.py:582: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 21.213967s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "lda_model = models.LdaModel(tfidf_corpus, num_topics = 100, id2word=dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "doc_topics = lda_model.get_document_topics(tfidf_corpus)\n",
    "print(\"done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 149/149 [15:10<00:00,  6.11s/it]\n"
     ]
    }
   ],
   "source": [
    "lda_sim = corpus_similarity(doc_topics, len(dictionary))\n",
    "\n",
    "lda_sim_df = pd.DataFrame(lda_sim)\n",
    "lda_sim_df.index = cat_names\n",
    "lda_sim_df.columns = cat_names\n",
    "# lda_data = top_n(lda_sim_df, 50)\n",
    "lda_data = slice_df_by_name(lda_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_data, np.ones(lda_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5 coherence=-1.498333\n",
      "k=7 coherence=-2.210039\n",
      "k=9 coherence=-1.753167\n",
      "k=11 coherence=-1.878003\n",
      "k=13 coherence=-2.161507\n",
      "k=15 coherence=-2.158024\n",
      "k=17 coherence=-2.020575\n",
      "k=19 coherence=-1.971740\n",
      "k=21 coherence=-1.714946\n",
      "k=23 coherence=-2.001825\n",
      "k=25 coherence=-1.957821\n",
      "k=27 coherence=-1.612905\n",
      "k=29 coherence=-1.732868\n",
      "k=31 coherence=-1.810027\n",
      "k=33 coherence=-1.767502\n",
      "k=35 coherence=-1.652618\n",
      "k=37 coherence=-1.887585\n",
      "k=39 coherence=-1.662607\n",
      "k=41 coherence=-1.734076\n",
      "k=43 coherence=-1.682676\n",
      "k=45 coherence=-1.786588\n",
      "k=47 coherence=-1.781682\n",
      "k=49 coherence=-1.677419\n"
     ]
    }
   ],
   "source": [
    "# largest_coherence = -1e20\n",
    "# best_k = 0\n",
    "# best_model = None\n",
    "# for k in range(5, 150, 2):\n",
    "#     model = models.LdaModel(tfidf_corpus, num_topics = k, id2word=dictionary)\n",
    "#     cm = models.coherencemodel.CoherenceModel(model=model, corpus=tfidf_corpus, coherence='u_mass')\n",
    "#     coherence = cm.get_coherence()\n",
    "#     print(\"k=%d coherence=%f\"%(k, coherence))\n",
    "#     if (coherence > largest_coherence):\n",
    "#         largest_coherence = coherence\n",
    "#         best_model = model\n",
    "#         best_k = k\n",
    "\n",
    "# print(\"best_k:%d\"%(best_k))\n",
    "# for idx, topic in best_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_file = \"cuisine_indices.txt\"\n",
    "matrix_file = \"cuisine_sim_matrix.csv\"\n",
    "\n",
    "with open (names_file, 'r') as f:\n",
    "    names = f.read().split(\"\\n\")\n",
    "\n",
    "demo_data = pd.read_csv(matrix_file, header=None)\n",
    "demo_data.index = names\n",
    "demo_data.columns = names\n",
    "\n",
    "with open(\"display/demo_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(demo_data, np.ones(demo_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 703473/703473 [20:43<00:00, 565.69it/s]  \n"
     ]
    }
   ],
   "source": [
    "path2reviewdump = \"reviews/reviews.dat\"\n",
    "\n",
    "with open(path2reviewdump, \"r\") as f:\n",
    "    reviews = f.readlines()\n",
    "review_docs = [preprocess(text) for text in tqdm(reviews)]\n",
    "review_dictionary = corpora.Dictionary(review_docs)\n",
    "print(\"Before prunn:%d\"%(len(review_dictionary)))\n",
    "review_dictionary.filter_extremes(no_below=15, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(review_dictionary)))\n",
    "review_corpus = [review_dictionary.doc2bow(doc) for doc in review_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8340.431562s\n",
      "Topic: 0 Words: 0.261*\"strip\" + 0.204*\"okay\" + 0.065*\"hide\" + 0.064*\"mall\" + 0.050*\"yesterday\" + 0.032*\"wild\" + 0.029*\"leftov\" + 0.025*\"cauliflow\" + 0.023*\"iron\" + 0.022*\"horchata\"\n",
      "Topic: 1 Words: 0.375*\"owner\" + 0.100*\"chop\" + 0.088*\"speak\" + 0.069*\"honey\" + 0.040*\"leg\" + 0.028*\"till\" + 0.025*\"claim\" + 0.021*\"adventur\" + 0.020*\"chow\" + 0.015*\"easier\"\n",
      "Topic: 2 Words: 0.100*\"seafood\" + 0.088*\"wall\" + 0.076*\"textur\" + 0.057*\"tini\" + 0.052*\"crunchi\" + 0.050*\"dip\" + 0.047*\"scallop\" + 0.040*\"soft\" + 0.035*\"savori\" + 0.027*\"mussel\"\n",
      "Topic: 3 Words: 0.164*\"month\" + 0.127*\"forward\" + 0.111*\"present\" + 0.072*\"hotel\" + 0.071*\"afternoon\" + 0.062*\"ball\" + 0.031*\"passion\" + 0.030*\"altern\" + 0.029*\"bear\" + 0.025*\"tip\"\n",
      "Topic: 4 Words: 0.231*\"waiter\" + 0.170*\"parti\" + 0.040*\"truck\" + 0.037*\"will\" + 0.036*\"fail\" + 0.032*\"replac\" + 0.032*\"plastic\" + 0.029*\"attempt\" + 0.029*\"awkward\" + 0.025*\"napkin\"\n",
      "Topic: 5 Words: 0.090*\"pleas\" + 0.089*\"delight\" + 0.079*\"pasti\" + 0.068*\"prime\" + 0.059*\"dont\" + 0.043*\"touch\" + 0.043*\"pure\" + 0.040*\"messi\" + 0.031*\"zero\" + 0.030*\"darn\"\n",
      "Topic: 6 Words: 0.191*\"dessert\" + 0.137*\"cream\" + 0.135*\"fantast\" + 0.100*\"chocol\" + 0.088*\"cake\" + 0.044*\"sweet\" + 0.038*\"blow\" + 0.028*\"slaw\" + 0.020*\"bore\" + 0.019*\"vanilla\"\n",
      "Topic: 7 Words: 0.143*\"salt\" + 0.138*\"pepper\" + 0.131*\"salti\" + 0.088*\"lemon\" + 0.053*\"sugar\" + 0.037*\"spoon\" + 0.029*\"creme\" + 0.026*\"effort\" + 0.026*\"tart\" + 0.022*\"custard\"\n",
      "Topic: 8 Words: 0.243*\"server\" + 0.127*\"bring\" + 0.107*\"husband\" + 0.049*\"charg\" + 0.027*\"prompt\" + 0.026*\"request\" + 0.023*\"immedi\" + 0.022*\"inform\" + 0.022*\"order\" + 0.017*\"check\"\n",
      "Topic: 9 Words: 0.136*\"line\" + 0.076*\"counter\" + 0.060*\"guy\" + 0.054*\"sign\" + 0.044*\"card\" + 0.039*\"number\" + 0.031*\"face\" + 0.030*\"worker\" + 0.028*\"stand\" + 0.027*\"cashier\"\n",
      "Topic: 10 Words: 0.245*\"light\" + 0.065*\"version\" + 0.061*\"strong\" + 0.057*\"bright\" + 0.054*\"dark\" + 0.050*\"color\" + 0.049*\"polit\" + 0.039*\"lime\" + 0.029*\"trust\" + 0.026*\"set\"\n",
      "Topic: 11 Words: 0.641*\"chees\" + 0.049*\"dog\" + 0.042*\"ranch\" + 0.026*\"york\" + 0.023*\"plain\" + 0.023*\"cheesi\" + 0.020*\"vinegar\" + 0.019*\"balsam\" + 0.019*\"calori\" + 0.017*\"bagel\"\n",
      "Topic: 12 Words: 0.429*\"taco\" + 0.154*\"green\" + 0.148*\"bean\" + 0.050*\"black\" + 0.037*\"enchilada\" + 0.019*\"fajita\" + 0.016*\"greas\" + 0.015*\"mimosa\" + 0.013*\"tamal\" + 0.011*\"curd\"\n",
      "Topic: 13 Words: 0.460*\"vega\" + 0.099*\"trip\" + 0.040*\"shell\" + 0.039*\"doubt\" + 0.033*\"stomach\" + 0.024*\"last\" + 0.023*\"martini\" + 0.023*\"remov\" + 0.022*\"freak\" + 0.018*\"power\"\n",
      "Topic: 14 Words: 0.176*\"ingredi\" + 0.116*\"uniqu\" + 0.080*\"standard\" + 0.057*\"interior\" + 0.055*\"homemad\" + 0.054*\"diner\" + 0.048*\"smoothi\" + 0.037*\"organ\" + 0.035*\"featur\" + 0.026*\"fare\"\n",
      "Topic: 15 Words: 0.288*\"warm\" + 0.205*\"lobster\" + 0.038*\"tail\" + 0.036*\"croissant\" + 0.028*\"bisqu\" + 0.020*\"grade\" + 0.019*\"resist\" + 0.018*\"purpos\" + 0.016*\"luke\" + 0.016*\"repres\"\n",
      "Topic: 16 Words: 0.447*\"star\" + 0.062*\"rat\" + 0.051*\"gravi\" + 0.040*\"biscuit\" + 0.037*\"give\" + 0.025*\"mark\" + 0.023*\"repeat\" + 0.022*\"rate\" + 0.022*\"tot\" + 0.021*\"let\"\n",
      "Topic: 17 Words: 0.075*\"cool\" + 0.059*\"patio\" + 0.059*\"drink\" + 0.051*\"music\" + 0.039*\"space\" + 0.037*\"night\" + 0.034*\"comfort\" + 0.029*\"loud\" + 0.025*\"vibe\" + 0.025*\"hang\"\n",
      "Topic: 18 Words: 0.260*\"surpris\" + 0.134*\"pleasant\" + 0.117*\"neighborhood\" + 0.072*\"chain\" + 0.063*\"frequent\" + 0.042*\"invit\" + 0.032*\"english\" + 0.027*\"whiskey\" + 0.023*\"hipster\" + 0.022*\"hire\"\n",
      "Topic: 19 Words: 0.304*\"wife\" + 0.112*\"modern\" + 0.088*\"daughter\" + 0.074*\"hummus\" + 0.073*\"fruit\" + 0.045*\"picki\" + 0.044*\"eater\" + 0.040*\"mother\" + 0.035*\"cup\" + 0.027*\"appetit\"\n",
      "Topic: 20 Words: 0.076*\"middl\" + 0.063*\"hate\" + 0.057*\"imagin\" + 0.047*\"syrup\" + 0.046*\"overwhelm\" + 0.041*\"relat\" + 0.040*\"cut\" + 0.034*\"fluffi\" + 0.033*\"trendi\" + 0.032*\"condiment\"\n",
      "Topic: 21 Words: 0.210*\"din\" + 0.153*\"room\" + 0.048*\"guest\" + 0.047*\"center\" + 0.037*\"dine\" + 0.037*\"experi\" + 0.036*\"train\" + 0.029*\"nearbi\" + 0.027*\"wear\" + 0.022*\"class\"\n",
      "Topic: 22 Words: 0.334*\"soup\" + 0.200*\"noodl\" + 0.102*\"asian\" + 0.096*\"broth\" + 0.044*\"sprout\" + 0.014*\"cranberri\" + 0.013*\"drunken\" + 0.012*\"palac\" + 0.011*\"choy\" + 0.011*\"back\"\n",
      "Topic: 23 Words: 0.353*\"fish\" + 0.349*\"shrimp\" + 0.038*\"theme\" + 0.038*\"chile\" + 0.036*\"batter\" + 0.017*\"mahi\" + 0.015*\"fishi\" + 0.013*\"anniversari\" + 0.012*\"sauc\" + 0.012*\"piec\"\n",
      "Topic: 24 Words: 0.129*\"meat\" + 0.119*\"cook\" + 0.075*\"sauc\" + 0.049*\"perfect\" + 0.044*\"season\" + 0.043*\"flavor\" + 0.039*\"tender\" + 0.027*\"medium\" + 0.027*\"rib\" + 0.023*\"pull\"\n",
      "Topic: 25 Words: 0.210*\"famili\" + 0.177*\"local\" + 0.133*\"kid\" + 0.060*\"meet\" + 0.042*\"support\" + 0.040*\"accommod\" + 0.040*\"own\" + 0.037*\"sister\" + 0.027*\"allow\" + 0.023*\"member\"\n",
      "Topic: 26 Words: 0.174*\"cocktail\" + 0.131*\"addit\" + 0.063*\"beat\" + 0.055*\"classic\" + 0.045*\"creativ\" + 0.040*\"entertain\" + 0.040*\"mile\" + 0.037*\"venu\" + 0.036*\"potenti\" + 0.026*\"current\"\n",
      "Topic: 27 Words: 0.144*\"small\" + 0.115*\"portion\" + 0.091*\"larg\" + 0.090*\"huge\" + 0.086*\"size\" + 0.067*\"share\" + 0.051*\"appet\" + 0.045*\"group\" + 0.034*\"entre\" + 0.029*\"pricey\"\n",
      "Topic: 28 Words: 0.092*\"chef\" + 0.068*\"cours\" + 0.057*\"list\" + 0.052*\"main\" + 0.050*\"ambianc\" + 0.037*\"even\" + 0.028*\"menu\" + 0.028*\"knowledg\" + 0.027*\"restaur\" + 0.026*\"truli\"\n",
      "Topic: 29 Words: 0.056*\"island\" + 0.054*\"farm\" + 0.052*\"self\" + 0.046*\"bud\" + 0.046*\"bitter\" + 0.038*\"gras\" + 0.035*\"tourist\" + 0.034*\"paradis\" + 0.030*\"hint\" + 0.027*\"blast\"\n",
      "Topic: 30 Words: 0.155*\"game\" + 0.078*\"sport\" + 0.068*\"watch\" + 0.057*\"bigger\" + 0.045*\"play\" + 0.040*\"movi\" + 0.039*\"screen\" + 0.037*\"express\" + 0.035*\"spacious\" + 0.033*\"fan\"\n",
      "Topic: 31 Words: 0.403*\"beer\" + 0.060*\"prefer\" + 0.039*\"craft\" + 0.036*\"flat\" + 0.034*\"brew\" + 0.028*\"profession\" + 0.027*\"cajun\" + 0.026*\"sens\" + 0.024*\"countri\" + 0.024*\"wide\"\n",
      "Topic: 32 Words: 0.131*\"grab\" + 0.101*\"blue\" + 0.073*\"patron\" + 0.055*\"tuesday\" + 0.055*\"put\" + 0.043*\"luck\" + 0.041*\"bell\" + 0.040*\"block\" + 0.039*\"team\" + 0.037*\"wednesday\"\n",
      "Topic: 33 Words: 0.124*\"worth\" + 0.122*\"money\" + 0.099*\"expens\" + 0.098*\"spend\" + 0.076*\"turkey\" + 0.072*\"cost\" + 0.054*\"overpric\" + 0.044*\"wast\" + 0.041*\"bomb\" + 0.036*\"total\"\n",
      "Topic: 34 Words: 0.215*\"dish\" + 0.155*\"flavor\" + 0.115*\"spici\" + 0.050*\"sauc\" + 0.042*\"spice\" + 0.034*\"sweet\" + 0.026*\"heat\" + 0.018*\"mango\" + 0.016*\"balanc\" + 0.016*\"crispi\"\n",
      "Topic: 35 Words: 0.184*\"soon\" + 0.130*\"welcom\" + 0.100*\"combin\" + 0.073*\"oyster\" + 0.067*\"citi\" + 0.039*\"west\" + 0.031*\"match\" + 0.029*\"charm\" + 0.026*\"central\" + 0.025*\"import\"\n",
      "Topic: 36 Words: 0.097*\"caus\" + 0.087*\"mustard\" + 0.085*\"vietnames\" + 0.063*\"skewer\" + 0.055*\"luckili\" + 0.054*\"tend\" + 0.052*\"diet\" + 0.037*\"flatbread\" + 0.036*\"kabob\" + 0.032*\"infus\"\n",
      "Topic: 37 Words: 0.579*\"sandwich\" + 0.097*\"tuna\" + 0.068*\"avocado\" + 0.049*\"mayo\" + 0.041*\"bread\" + 0.030*\"snack\" + 0.018*\"pastrami\" + 0.012*\"slice\" + 0.010*\"video\" + 0.009*\"cannoli\"\n",
      "Topic: 38 Words: 0.219*\"open\" + 0.143*\"walk\" + 0.102*\"close\" + 0.065*\"door\" + 0.036*\"greet\" + 0.026*\"smile\" + 0.025*\"confus\" + 0.021*\"lose\" + 0.017*\"annoy\" + 0.015*\"moment\"\n",
      "Topic: 39 Words: 0.263*\"onion\" + 0.089*\"pickl\" + 0.088*\"ring\" + 0.087*\"roast\" + 0.083*\"duck\" + 0.035*\"fountain\" + 0.034*\"gourmet\" + 0.031*\"carri\" + 0.028*\"brother\" + 0.025*\"winner\"\n",
      "Topic: 40 Words: 0.274*\"insid\" + 0.220*\"outsid\" + 0.062*\"pita\" + 0.049*\"fanci\" + 0.046*\"refresh\" + 0.045*\"gyro\" + 0.039*\"greek\" + 0.026*\"constant\" + 0.025*\"layout\" + 0.024*\"cherri\"\n",
      "Topic: 41 Words: 0.097*\"tonight\" + 0.093*\"limit\" + 0.085*\"korean\" + 0.083*\"crepe\" + 0.059*\"yelper\" + 0.054*\"somewhat\" + 0.049*\"madison\" + 0.048*\"lover\" + 0.040*\"awhil\" + 0.038*\"sear\"\n",
      "Topic: 42 Words: 0.621*\"chicken\" + 0.130*\"wing\" + 0.055*\"sauc\" + 0.036*\"sour\" + 0.018*\"crispi\" + 0.016*\"breast\" + 0.016*\"order\" + 0.012*\"sweet\" + 0.011*\"piec\" + 0.010*\"chew\"\n",
      "Topic: 43 Words: 0.095*\"separ\" + 0.087*\"cilantro\" + 0.072*\"compliment\" + 0.063*\"thursday\" + 0.062*\"smooth\" + 0.060*\"pink\" + 0.058*\"hadn\" + 0.052*\"basket\" + 0.041*\"memori\" + 0.037*\"overlook\"\n",
      "Topic: 44 Words: 0.427*\"price\" + 0.151*\"reason\" + 0.143*\"qualiti\" + 0.057*\"late\" + 0.025*\"futur\" + 0.023*\"airport\" + 0.022*\"high\" + 0.017*\"effici\" + 0.013*\"night\" + 0.010*\"upstair\"\n",
      "Topic: 45 Words: 0.065*\"plate\" + 0.061*\"bite\" + 0.049*\"couldn\" + 0.044*\"finish\" + 0.039*\"felt\" + 0.036*\"plat\" + 0.033*\"end\" + 0.029*\"leav\" + 0.028*\"hungri\" + 0.027*\"smoke\"\n",
      "Topic: 46 Words: 0.121*\"asada\" + 0.117*\"carn\" + 0.078*\"cucumb\" + 0.065*\"platter\" + 0.062*\"crunch\" + 0.061*\"mini\" + 0.060*\"donut\" + 0.046*\"unlik\" + 0.032*\"blueberri\" + 0.031*\"assort\"\n",
      "Topic: 47 Words: 0.113*\"wait\" + 0.096*\"tabl\" + 0.073*\"order\" + 0.059*\"minut\" + 0.055*\"take\" + 0.050*\"seat\" + 0.042*\"long\" + 0.037*\"come\" + 0.030*\"final\" + 0.027*\"arriv\"\n",
      "Topic: 48 Words: 0.213*\"wine\" + 0.159*\"glass\" + 0.098*\"bottl\" + 0.098*\"sampl\" + 0.082*\"salmon\" + 0.052*\"jalapeno\" + 0.037*\"win\" + 0.036*\"pour\" + 0.020*\"stumbl\" + 0.018*\"water\"\n",
      "Topic: 49 Words: 0.506*\"lunch\" + 0.082*\"compar\" + 0.054*\"crazi\" + 0.037*\"environ\" + 0.033*\"daili\" + 0.033*\"multipl\" + 0.027*\"draft\" + 0.026*\"discount\" + 0.024*\"advertis\" + 0.017*\"weekday\"\n",
      "Topic: 50 Words: 0.083*\"machin\" + 0.069*\"king\" + 0.055*\"descript\" + 0.054*\"excus\" + 0.047*\"edg\" + 0.046*\"rais\" + 0.045*\"depend\" + 0.043*\"fee\" + 0.039*\"stellar\" + 0.038*\"describ\"\n",
      "Topic: 51 Words: 0.162*\"visit\" + 0.074*\"live\" + 0.068*\"drive\" + 0.053*\"second\" + 0.041*\"phoenix\" + 0.040*\"hope\" + 0.037*\"downtown\" + 0.028*\"recent\" + 0.025*\"life\" + 0.024*\"day\"\n",
      "Topic: 52 Words: 0.291*\"option\" + 0.157*\"veggi\" + 0.087*\"vegetarian\" + 0.070*\"vegan\" + 0.044*\"peanut\" + 0.041*\"butter\" + 0.036*\"protein\" + 0.026*\"carrot\" + 0.021*\"choos\" + 0.021*\"cheesecak\"\n",
      "Topic: 53 Words: 0.210*\"shop\" + 0.076*\"post\" + 0.063*\"product\" + 0.055*\"beverag\" + 0.046*\"offic\" + 0.046*\"cheaper\" + 0.044*\"natur\" + 0.033*\"deli\" + 0.032*\"soooo\" + 0.029*\"massiv\"\n",
      "Topic: 54 Words: 0.211*\"excit\" + 0.135*\"boyfriend\" + 0.074*\"burn\" + 0.070*\"mess\" + 0.067*\"continu\" + 0.063*\"doubl\" + 0.035*\"search\" + 0.029*\"shave\" + 0.029*\"philli\" + 0.027*\"exist\"\n",
      "Topic: 55 Words: 0.185*\"stay\" + 0.093*\"dollar\" + 0.081*\"hubbi\" + 0.081*\"delish\" + 0.050*\"starter\" + 0.045*\"flight\" + 0.042*\"find\" + 0.042*\"cinnamon\" + 0.040*\"subway\" + 0.037*\"glaze\"\n",
      "Topic: 56 Words: 0.111*\"soda\" + 0.097*\"booth\" + 0.089*\"deep\" + 0.057*\"clam\" + 0.055*\"school\" + 0.047*\"twist\" + 0.042*\"favor\" + 0.039*\"listen\" + 0.037*\"discov\" + 0.033*\"can\"\n",
      "Topic: 57 Words: 0.188*\"breakfast\" + 0.128*\"coffe\" + 0.078*\"egg\" + 0.073*\"pancak\" + 0.067*\"french\" + 0.064*\"toast\" + 0.045*\"morn\" + 0.039*\"bacon\" + 0.033*\"waffl\" + 0.027*\"hash\"\n",
      "Topic: 58 Words: 0.081*\"manag\" + 0.074*\"waitress\" + 0.048*\"water\" + 0.046*\"ask\" + 0.044*\"cold\" + 0.030*\"drink\" + 0.030*\"take\" + 0.026*\"refil\" + 0.022*\"horribl\" + 0.021*\"come\"\n",
      "Topic: 59 Words: 0.203*\"style\" + 0.157*\"real\" + 0.114*\"authent\" + 0.093*\"restaur\" + 0.085*\"japanes\" + 0.073*\"tradit\" + 0.068*\"american\" + 0.051*\"cuisin\" + 0.028*\"addict\" + 0.028*\"teriyaki\"\n",
      "Topic: 60 Words: 0.234*\"bartend\" + 0.146*\"watch\" + 0.103*\"valley\" + 0.064*\"east\" + 0.059*\"buffalo\" + 0.054*\"monday\" + 0.029*\"fulli\" + 0.025*\"child\" + 0.023*\"brooklyn\" + 0.020*\"occupi\"\n",
      "Topic: 61 Words: 0.136*\"bacon\" + 0.133*\"wrap\" + 0.088*\"concept\" + 0.045*\"market\" + 0.044*\"paper\" + 0.044*\"board\" + 0.041*\"lettuc\" + 0.040*\"skin\" + 0.035*\"grind\" + 0.035*\"boil\"\n",
      "Topic: 62 Words: 0.204*\"chip\" + 0.177*\"salsa\" + 0.153*\"mexican\" + 0.071*\"tortilla\" + 0.056*\"guacamol\" + 0.028*\"shred\" + 0.026*\"quesadilla\" + 0.021*\"guac\" + 0.021*\"corn\" + 0.013*\"pico\"\n",
      "Topic: 63 Words: 0.195*\"curri\" + 0.102*\"spring\" + 0.095*\"coconut\" + 0.090*\"indian\" + 0.067*\"milk\" + 0.061*\"california\" + 0.050*\"mild\" + 0.042*\"sangria\" + 0.036*\"naan\" + 0.031*\"berri\"\n",
      "Topic: 64 Words: 0.158*\"town\" + 0.120*\"year\" + 0.110*\"chang\" + 0.096*\"glad\" + 0.077*\"rememb\" + 0.059*\"consist\" + 0.054*\"one\" + 0.054*\"past\" + 0.037*\"groupon\" + 0.034*\"notch\"\n",
      "Topic: 65 Words: 0.145*\"incred\" + 0.133*\"date\" + 0.114*\"outstand\" + 0.109*\"world\" + 0.078*\"chicago\" + 0.055*\"complimentari\" + 0.041*\"equal\" + 0.037*\"adult\" + 0.033*\"panini\" + 0.027*\"tequila\"\n",
      "Topic: 66 Words: 0.095*\"pud\" + 0.079*\"fabul\" + 0.067*\"buy\" + 0.062*\"pastor\" + 0.050*\"chorizo\" + 0.046*\"superb\" + 0.043*\"almond\" + 0.038*\"lay\" + 0.037*\"dream\" + 0.035*\"hesit\"\n",
      "Topic: 67 Words: 0.305*\"menu\" + 0.210*\"item\" + 0.048*\"chines\" + 0.039*\"tofu\" + 0.032*\"design\" + 0.022*\"signatur\" + 0.020*\"bland\" + 0.020*\"veget\" + 0.019*\"highlight\" + 0.018*\"extens\"\n",
      "Topic: 68 Words: 0.081*\"finger\" + 0.075*\"giant\" + 0.066*\"photo\" + 0.044*\"cheer\" + 0.043*\"boy\" + 0.038*\"age\" + 0.038*\"fight\" + 0.037*\"citrus\" + 0.036*\"requir\" + 0.032*\"awar\"\n",
      "Topic: 69 Words: 0.334*\"salad\" + 0.152*\"steak\" + 0.128*\"potato\" + 0.061*\"dress\" + 0.044*\"mushroom\" + 0.031*\"sweet\" + 0.026*\"side\" + 0.025*\"spinach\" + 0.021*\"bake\" + 0.021*\"mash\"\n",
      "Topic: 70 Words: 0.159*\"nacho\" + 0.075*\"handl\" + 0.066*\"sad\" + 0.063*\"compani\" + 0.063*\"secret\" + 0.046*\"bruschetta\" + 0.044*\"flour\" + 0.042*\"thrill\" + 0.039*\"delic\" + 0.037*\"manner\"\n",
      "Topic: 71 Words: 0.207*\"ramen\" + 0.144*\"chili\" + 0.112*\"complaint\" + 0.058*\"yeah\" + 0.056*\"damn\" + 0.042*\"funni\" + 0.034*\"biggest\" + 0.034*\"appeal\" + 0.031*\"dozen\" + 0.026*\"pain\"\n",
      "Topic: 72 Words: 0.108*\"valu\" + 0.096*\"provid\" + 0.088*\"desert\" + 0.087*\"cooki\" + 0.081*\"banana\" + 0.067*\"babi\" + 0.065*\"afford\" + 0.055*\"dough\" + 0.050*\"ton\" + 0.036*\"book\"\n",
      "Topic: 73 Words: 0.393*\"beef\" + 0.128*\"healthi\" + 0.123*\"corn\" + 0.044*\"ambienc\" + 0.035*\"pound\" + 0.033*\"gotta\" + 0.029*\"buddi\" + 0.026*\"hook\" + 0.024*\"substitut\" + 0.016*\"colleg\"\n",
      "Topic: 74 Words: 0.102*\"good\" + 0.084*\"like\" + 0.053*\"place\" + 0.034*\"think\" + 0.031*\"tast\" + 0.028*\"look\" + 0.027*\"come\" + 0.027*\"littl\" + 0.027*\"pretti\" + 0.026*\"tri\"\n",
      "Topic: 75 Words: 0.271*\"thai\" + 0.122*\"level\" + 0.094*\"ice\" + 0.042*\"yellow\" + 0.038*\"curri\" + 0.035*\"steakhous\" + 0.028*\"basil\" + 0.024*\"wedg\" + 0.023*\"ride\" + 0.018*\"construct\"\n",
      "Topic: 76 Words: 0.333*\"roll\" + 0.309*\"sushi\" + 0.038*\"tempura\" + 0.037*\"salmon\" + 0.028*\"chef\" + 0.021*\"fresh\" + 0.020*\"crawfish\" + 0.014*\"piec\" + 0.014*\"browni\" + 0.009*\"seven\"\n",
      "Topic: 77 Words: 0.196*\"custom\" + 0.080*\"slow\" + 0.062*\"employe\" + 0.061*\"servic\" + 0.050*\"issu\" + 0.049*\"terribl\" + 0.045*\"understand\" + 0.042*\"poor\" + 0.035*\"receiv\" + 0.034*\"complain\"\n",
      "Topic: 78 Words: 0.266*\"happi\" + 0.265*\"hour\" + 0.045*\"friday\" + 0.042*\"reserv\" + 0.041*\"birthday\" + 0.036*\"night\" + 0.031*\"drink\" + 0.030*\"saturday\" + 0.024*\"earli\" + 0.022*\"celebr\"\n",
      "Topic: 79 Words: 0.086*\"girlfriend\" + 0.083*\"posit\" + 0.072*\"alcohol\" + 0.061*\"hype\" + 0.056*\"alright\" + 0.045*\"coleslaw\" + 0.040*\"oili\" + 0.037*\"flavorless\" + 0.034*\"thumb\" + 0.033*\"everyday\"\n",
      "Topic: 80 Words: 0.396*\"locat\" + 0.119*\"park\" + 0.107*\"street\" + 0.065*\"easi\" + 0.044*\"corner\" + 0.037*\"crowd\" + 0.029*\"event\" + 0.023*\"north\" + 0.018*\"cater\" + 0.011*\"folk\"\n",
      "Topic: 81 Words: 0.217*\"rice\" + 0.204*\"pork\" + 0.062*\"fri\" + 0.033*\"dumpl\" + 0.029*\"sauc\" + 0.026*\"steam\" + 0.021*\"order\" + 0.020*\"chines\" + 0.019*\"brown\" + 0.018*\"white\"\n",
      "Topic: 82 Words: 0.135*\"bread\" + 0.068*\"tomato\" + 0.067*\"garlic\" + 0.055*\"italian\" + 0.050*\"sauc\" + 0.048*\"pasta\" + 0.025*\"oliv\" + 0.018*\"butter\" + 0.018*\"mozzarella\" + 0.018*\"parmesan\"\n",
      "Topic: 83 Words: 0.523*\"love\" + 0.195*\"awesom\" + 0.080*\"absolut\" + 0.074*\"yummi\" + 0.039*\"favorit\" + 0.037*\"cute\" + 0.009*\"sweet\" + 0.008*\"delici\" + 0.008*\"place\" + 0.006*\"serious\"\n",
      "Topic: 84 Words: 0.564*\"pizza\" + 0.097*\"crust\" + 0.074*\"top\" + 0.070*\"slice\" + 0.029*\"oven\" + 0.019*\"sauc\" + 0.011*\"white\" + 0.009*\"itali\" + 0.007*\"wood\" + 0.006*\"lighter\"\n",
      "Topic: 85 Words: 0.210*\"select\" + 0.138*\"buffet\" + 0.087*\"varieti\" + 0.048*\"station\" + 0.041*\"satisfi\" + 0.037*\"ayc\" + 0.030*\"includ\" + 0.029*\"offer\" + 0.025*\"coupon\" + 0.024*\"choos\"\n",
      "Topic: 86 Words: 0.151*\"order\" + 0.069*\"say\" + 0.056*\"tell\" + 0.051*\"know\" + 0.037*\"want\" + 0.026*\"ask\" + 0.018*\"go\" + 0.018*\"pick\" + 0.016*\"call\" + 0.014*\"girl\"\n",
      "Topic: 87 Words: 0.282*\"review\" + 0.114*\"yelp\" + 0.079*\"read\" + 0.060*\"mouth\" + 0.059*\"write\" + 0.057*\"juic\" + 0.048*\"melt\" + 0.042*\"base\" + 0.034*\"appl\" + 0.030*\"orang\"\n",
      "Topic: 88 Words: 0.322*\"free\" + 0.093*\"chipotl\" + 0.092*\"gluten\" + 0.091*\"casual\" + 0.076*\"except\" + 0.057*\"conveni\" + 0.034*\"occas\" + 0.023*\"wont\" + 0.022*\"fave\" + 0.019*\"numer\"\n",
      "Topic: 89 Words: 0.105*\"deliv\" + 0.070*\"deliveri\" + 0.060*\"caramel\" + 0.058*\"creat\" + 0.048*\"heart\" + 0.045*\"phenomen\" + 0.043*\"process\" + 0.042*\"apart\" + 0.040*\"fall\" + 0.037*\"onlin\"\n",
      "Topic: 90 Words: 0.234*\"sausag\" + 0.158*\"origin\" + 0.151*\"meatbal\" + 0.062*\"mood\" + 0.056*\"brand\" + 0.054*\"strang\" + 0.037*\"thorough\" + 0.030*\"legit\" + 0.026*\"pizzeria\" + 0.023*\"carb\"\n",
      "Topic: 91 Words: 0.190*\"bowl\" + 0.093*\"pay\" + 0.076*\"combo\" + 0.053*\"pictur\" + 0.052*\"buck\" + 0.041*\"extra\" + 0.040*\"catch\" + 0.033*\"pineappl\" + 0.027*\"bare\" + 0.024*\"mistak\"\n",
      "Topic: 92 Words: 0.206*\"crab\" + 0.130*\"sunday\" + 0.105*\"brunch\" + 0.048*\"rang\" + 0.041*\"kink\" + 0.039*\"mari\" + 0.038*\"coke\" + 0.038*\"bloodi\" + 0.037*\"benedict\" + 0.027*\"haha\"\n",
      "Topic: 93 Words: 0.076*\"great\" + 0.059*\"place\" + 0.051*\"servic\" + 0.046*\"time\" + 0.042*\"friend\" + 0.031*\"restaur\" + 0.028*\"delici\" + 0.026*\"best\" + 0.024*\"come\" + 0.024*\"definit\"\n",
      "Topic: 94 Words: 0.114*\"belli\" + 0.094*\"store\" + 0.091*\"lamb\" + 0.069*\"sell\" + 0.053*\"specialti\" + 0.052*\"pastri\" + 0.051*\"load\" + 0.033*\"purchas\" + 0.029*\"bakeri\" + 0.028*\"sick\"\n",
      "Topic: 95 Words: 0.126*\"strawberri\" + 0.114*\"question\" + 0.109*\"improv\" + 0.074*\"answer\" + 0.062*\"name\" + 0.049*\"spread\" + 0.046*\"gelato\" + 0.043*\"genuin\" + 0.038*\"bonus\" + 0.030*\"way\"\n",
      "Topic: 96 Words: 0.334*\"super\" + 0.218*\"fast\" + 0.162*\"burrito\" + 0.103*\"cheap\" + 0.085*\"margarita\" + 0.028*\"carnita\" + 0.020*\"queso\" + 0.005*\"rout\" + 0.004*\"kiss\" + 0.004*\"pushi\"\n",
      "Topic: 97 Words: 0.228*\"clean\" + 0.208*\"grill\" + 0.126*\"today\" + 0.061*\"cafe\" + 0.052*\"smell\" + 0.039*\"step\" + 0.039*\"ladi\" + 0.037*\"dirti\" + 0.032*\"bathroom\" + 0.026*\"warn\"\n",
      "Topic: 98 Words: 0.092*\"club\" + 0.086*\"heaven\" + 0.076*\"grand\" + 0.070*\"bunch\" + 0.056*\"danc\" + 0.055*\"straight\" + 0.050*\"bar\" + 0.040*\"band\" + 0.038*\"shame\" + 0.028*\"watermelon\"\n",
      "Topic: 99 Words: 0.331*\"burger\" + 0.311*\"fri\" + 0.031*\"shake\" + 0.022*\"bacon\" + 0.022*\"patti\" + 0.017*\"cheddar\" + 0.016*\"ketchup\" + 0.015*\"sweet\" + 0.014*\"top\" + 0.014*\"bun\"\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "review_model = models.LdaModel(review_corpus, num_topics=100, id2word=review_dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "for idx, topic in review_model.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [48:00<00:00, 29.22s/it] \n"
     ]
    }
   ],
   "source": [
    "def combine_topics(cat_topics):\n",
    "    topics = {}\n",
    "    for _sub_topics in cat_topics:\n",
    "        for _topic, _value in _sub_topics:\n",
    "            if _topic in topics:\n",
    "                topics[_topic] += _value\n",
    "            else:\n",
    "                topics[_topic] = _value\n",
    "    \n",
    "    return topics\n",
    "\n",
    "all_topics = []\n",
    "cat_names = []\n",
    "for i in tqdm(range(0, len(cat_list))):\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_docs = [preprocess(text) for text in f.readlines()]\n",
    "        cat_corpus = [review_dictionary.doc2bow(doc) for doc in cat_docs]\n",
    "        cat_topics = review_model[cat_corpus]\n",
    "        all_topics.append(combine_topics(cat_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:01<00:00, 105.42it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_individual_sim = corpus_similarity([[(k, topic[k]) for k in topic] for topic in all_topics], len(review_dictionary))\n",
    "\n",
    "lda_individual_sim_df = pd.DataFrame(lda_individual_sim)\n",
    "lda_individual_sim_df.index = cat_names\n",
    "lda_individual_sim_df.columns = cat_names\n",
    "lda_individual_data = top_n(lda_individual_sim_df, 50)\n",
    "\n",
    "with open(\"display/lda_ind_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_individual_data, np.ones(lda_individual_data.shape[0]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling with Gensim\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 163/163 [34:05<00:00, 10.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before prunn:193503\n",
      "After prunn:100000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "cat_list = glob.glob (\"cuisines/*\")\n",
    "cat_size = len(cat_list)\n",
    "\n",
    "random.seed(0)\n",
    "cat_names = []\n",
    "cat_text = []\n",
    "# sample_size = min(30, cat_size)\n",
    "# cat_sample = sorted(random.sample(range(cat_size), sample_size))\n",
    "cat_sample = range(0, cat_size)\n",
    "\n",
    "count = 0\n",
    "for i in cat_sample:\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_text.append(f.read().replace(\"\\n\", \"\").replace(\"\\r\",\"\"))\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(cat_text)]\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(dictionary)))\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sklearn\n",
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    b = dict(b)\n",
    "    norm_a = 0\n",
    "    norm_b = 0\n",
    "    denom = 0\n",
    "    for a_i, a_v in a:\n",
    "        norm_a += a_v * a_v\n",
    "        if a_i in b:\n",
    "            denom += a_v * b[a_i]\n",
    "    for b_i in b:\n",
    "        norm_b += b[b_i] * b[b_i]\n",
    "    \n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "    \n",
    "    return denom / (norm_a * norm_b)\n",
    "\n",
    "def top_n(df, n, thresh_hold = 0.1):\n",
    "    df_count = np.zeros(df.shape)\n",
    "    df_bak = df\n",
    "    df_count[df >= thresh_hold] = 1\n",
    "    _counts = np.sum(df_count, axis=1)\n",
    "    max_index = []\n",
    "    for i in range(0, n):\n",
    "        _index = np.argmax(_counts)\n",
    "        max_index.append(_index)\n",
    "        _counts[_index] = -1\n",
    "    \n",
    "    return df.iloc[max_index][df.columns[max_index]]\n",
    "\n",
    "def format_obj(df, groups):\n",
    "    _nodes = \"nodes\"\n",
    "    _links = \"links\"\n",
    "    json_obj = {_nodes:[], _links:[]}\n",
    "    for i in range(0, len(df.columns)):\n",
    "        json_obj[_nodes].append({\"name\":df.columns[i], \"group\":groups[i]})\n",
    "    \n",
    "    for i in range(0, df.shape[0] - 1):\n",
    "        for j in range(i + 1, df.shape[0]):\n",
    "            json_obj[_links].append({\"source\":i, \"target\":j, \"value\":float(df.iloc[i][j])})\n",
    "    \n",
    "    return json_obj\n",
    "\n",
    "def corpus2matrix(corpus, vector_dimension):\n",
    "    _corpus_matrix = np.zeros([len(corpus), vector_dimension])\n",
    "    for i, row in enumerate(corpus):\n",
    "        for j, v in row:\n",
    "            _corpus_matrix[i][j] = v\n",
    "    \n",
    "    return _corpus_matrix\n",
    "    \n",
    "def corpus_similarity(corpus, vector_dimension, distance_func = sklearn.metrics.pairwise.cosine_similarity):\n",
    "#     _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "#     for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#         _sim[i][i] = 1\n",
    "#         for j in range(i + 1, len(corpus)):\n",
    "#             _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#             _sim[j][i] = _sim[i][j]\n",
    "    _corpus_matrix = corpus2matrix(corpus, vector_dimension)\n",
    "    #Normailzation\n",
    "#     _corpus_matrix = Normalizer().transform(_corpus_matrix)    \n",
    "    return distance_func(_corpus_matrix)\n",
    "\n",
    "#     _sim = np.zeros([len(corpus), len(corpus)])\n",
    "#     for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#         for j in range(i + 1, len(corpus)):\n",
    "#             _sim[i, j] = distance_func(_corpus_matrix[i], _corpus_matrix[j])\n",
    "#             _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "#     return 1 - _sim\n",
    "\n",
    "def corpus_similarity_1(corpus):\n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        _sim[i][i] = 1\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return _sim\n",
    "\n",
    "def slice_df_by_name(df,names):\n",
    "    return df.loc[names][names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "# for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#     sim[i][i] = 1\n",
    "#     for j in range(i + 1, len(corpus)):\n",
    "#         sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#         sim[j][i] = sim[i][j]\n",
    "        \n",
    "sim = corpus_similarity(corpus, len(dictionary))\n",
    "\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.index = cat_names\n",
    "sim_df.columns = cat_names\n",
    "data = top_n(sim_df, 50)\n",
    "selected_names = data.columns\n",
    "\n",
    "with open(\"display/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(data, np.ones(data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns; \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sample = 20\n",
    "# ax = sns.heatmap(data.iloc[0:sample][data.columns[0:sample]],cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "import json\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "tfidf_sim = corpus_similarity(tfidf_corpus, len(dictionary))\n",
    "        \n",
    "tfidf_sim_df = pd.DataFrame(tfidf_sim)\n",
    "tfidf_sim_df.index = cat_names\n",
    "tfidf_sim_df.columns = cat_names\n",
    "# tfidf_data = top_n(tfidf_sim_df, 50)\n",
    "tfidf_data = slice_df_by_name(tfidf_sim_df, selected_names)\n",
    "                              \n",
    "with open(\"display/tfidf_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_data, np.ones(tfidf_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\models\\ldamodel.py:582: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 35.162400s\n",
      "\n",
      "Perplexity:  -45.70216973747614\n",
      "\n",
      "Coherence Score:  0.4342239686194873\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "num_topics = 100\n",
    "\n",
    "t0 = time()\n",
    "#lda_model = models.LdaModel(tfidf_corpus, num_topics = num_topics, id2word=dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "lda_model = models.LdaModel(tfidf_corpus, \n",
    "                            num_topics = num_topics, \n",
    "                            id2word = dictionary,\n",
    "                            random_state = 100,\n",
    "                            update_every = 1,\n",
    "                            chunksize = 100,\n",
    "                            passes = 10,\n",
    "                            alpha = 'auto',\n",
    "                            per_word_topics = True)\n",
    "\n",
    "doc_topics = lda_model.get_document_topics(tfidf_corpus)\n",
    "print(\"Training done in %fs\" % (time() - t0))\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model = lda_model, texts = processed_docs, dictionary = dictionary, coherence = 'c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "print(\"Evaluation done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '../mallet-2.0.8/bin/mallet' # update this path\n",
    "\n",
    "t0 = time()\n",
    "lda_mallet_model = gensim.models.wrappers.LdaMallet(mallet_path, corpus = tfidf_corpus, num_topics = num_topics, id2word = dictionary)\n",
    "print(\"Training done in %fs\" % (time() - t0))\n",
    "\n",
    "# Compute Coherence Score\n",
    "t0 = time()\n",
    "coherence_model_ldamallet = CoherenceModel(model = lda_mallet_model, texts = processed_docs, dictionary = dictionary, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "print(\"Evaluation done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_sim = corpus_similarity(doc_topics, num_topics)\n",
    "\n",
    "lda_sim_df = pd.DataFrame(lda_sim)\n",
    "lda_sim_df.index = cat_names\n",
    "lda_sim_df.columns = cat_names\n",
    "# lda_data = top_n(lda_sim_df, 50)\n",
    "lda_data = slice_df_by_name(lda_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_data, np.ones(lda_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, tfidf_corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest_coherence = -1e20\n",
    "# best_k = 0\n",
    "# best_model = None\n",
    "# for k in range(5, 150, 2):\n",
    "#     model = models.LdaModel(tfidf_corpus, num_topics = k, id2word=dictionary)\n",
    "#     cm = models.coherencemodel.CoherenceModel(model=model, corpus=tfidf_corpus, coherence='u_mass')\n",
    "#     coherence = cm.get_coherence()\n",
    "#     print(\"k=%d coherence=%f\"%(k, coherence))\n",
    "#     if (coherence > largest_coherence):\n",
    "#         largest_coherence = coherence\n",
    "#         best_model = model\n",
    "#         best_k = k\n",
    "\n",
    "# print(\"best_k:%d\"%(best_k))\n",
    "# for idx, topic in best_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_file = \"cuisine_indices.txt\"\n",
    "matrix_file = \"cuisine_sim_matrix.csv\"\n",
    "\n",
    "with open (names_file, 'r') as f:\n",
    "    names = f.read().split(\"\\n\")\n",
    "\n",
    "demo_data = pd.read_csv(matrix_file, header=None)\n",
    "demo_data.index = names\n",
    "demo_data.columns = names\n",
    "\n",
    "with open(\"display/demo_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(demo_data, np.ones(demo_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path2reviewdump = \"reviews/reviews.dat\"\n",
    "\n",
    "# with open(path2reviewdump, \"r\") as f:\n",
    "#     reviews = f.readlines()\n",
    "# review_docs = [preprocess(text) for text in tqdm(reviews)]\n",
    "# review_dictionary = corpora.Dictionary(review_docs)\n",
    "# print(\"Before prunn:%d\"%(len(review_dictionary)))\n",
    "# review_dictionary.filter_extremes(no_below=15, no_above = 0.5)\n",
    "# print(\"After prunn:%d\"%(len(review_dictionary)))\n",
    "# review_corpus = [review_dictionary.doc2bow(doc) for doc in review_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time\n",
    "\n",
    "# t0 = time()\n",
    "# review_model = models.LdaModel(review_corpus, num_topics=100, id2word=review_dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "# print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "# for idx, topic in review_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_topics(cat_topics):\n",
    "#     topics = {}\n",
    "#     for _sub_topics in cat_topics:\n",
    "#         for _topic, _value in _sub_topics:\n",
    "#             if _topic in topics:\n",
    "#                 topics[_topic] += _value\n",
    "#             else:\n",
    "#                 topics[_topic] = _value\n",
    "    \n",
    "#     return topics\n",
    "\n",
    "# all_topics = []\n",
    "# cat_names = []\n",
    "# for i in tqdm(range(0, len(cat_list))):\n",
    "#     cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "#     with open(cat_list[i]) as f:\n",
    "#         cat_docs = [preprocess(text) for text in f.readlines()]\n",
    "#         cat_corpus = [review_dictionary.doc2bow(doc) for doc in cat_docs]\n",
    "#         cat_topics = review_model[cat_corpus]\n",
    "#         all_topics.append(combine_topics(cat_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lda_individual_sim = corpus_similarity([[(k, topic[k]) for k in topic] for topic in all_topics], len(review_dictionary))\n",
    "\n",
    "# lda_individual_sim_df = pd.DataFrame(lda_individual_sim)\n",
    "# lda_individual_sim_df.index = cat_names\n",
    "# lda_individual_sim_df.columns = cat_names\n",
    "# lda_individual_data = top_n(lda_individual_sim_df, 50)\n",
    "\n",
    "# with open(\"display/lda_ind_output.json\", \"w\") as f:\n",
    "#     f.write(json.dumps(format_obj(lda_individual_data, np.ones(lda_individual_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, Birch\n",
    "\n",
    "# X = corpus2matrix(tfidf_corpus, len(dictionary))\n",
    "X = corpus2matrix(doc_topics, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "cluster_size = 10\n",
    "Norm_X = Normalizer().transform(X)\n",
    "\n",
    "best_score = -1\n",
    "best_k = -1\n",
    "for k in range(2, 10):\n",
    "    y_pred = KMeans(n_clusters = k).fit_predict(Norm_X)\n",
    "    _score = metrics.silhouette_score(Norm_X, y_pred, metric='euclidean')\n",
    "    if _score > best_score:\n",
    "        best_k = k\n",
    "        best_score = _score\n",
    "\n",
    "y_pred = KMeans(n_clusters = best_k).fit_predict(Norm_X)\n",
    "\n",
    "clusters = dict()\n",
    "for i in range(0, len(y_pred)):\n",
    "    if y_pred[i] in clusters:\n",
    "        clusters[y_pred[i]].append(cat_names[i])\n",
    "    else:\n",
    "        clusters[y_pred[i]] = [cat_names[i]]\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Birch(branching_factor=50, compute_labels=True, n_clusters=10).fit_predict(X)\n",
    "\n",
    "clusters = dict()\n",
    "for i in range(0, len(y_pred)):\n",
    "    if y_pred[i] in clusters:\n",
    "        clusters[y_pred[i]].append(cat_names[i])\n",
    "    else:\n",
    "        clusters[y_pred[i]] = [cat_names[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

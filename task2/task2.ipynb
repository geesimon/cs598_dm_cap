{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling with Gensim\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 147/147 [31:05<00:00, 10.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before prunn:193047\n",
      "After prunn:100000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "cat_list = sorted(glob.glob (\"cuisines/*\"))\n",
    "cat_size = len(cat_list)\n",
    "\n",
    "random.seed(0)\n",
    "cat_names = []\n",
    "cat_text = []\n",
    "# sample_size = min(30, cat_size)\n",
    "# cat_sample = sorted(random.sample(range(cat_size), sample_size))\n",
    "cat_sample = range(0, cat_size)\n",
    "\n",
    "count = 0\n",
    "for i in cat_sample:\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_text.append(f.read().replace(\"\\n\", \"\").replace(\"\\r\",\"\"))\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(cat_text)]\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(dictionary)))\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sklearn\n",
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    b = dict(b)\n",
    "    norm_a = 0\n",
    "    norm_b = 0\n",
    "    denom = 0\n",
    "    for a_i, a_v in a:\n",
    "        norm_a += a_v * a_v\n",
    "        if a_i in b:\n",
    "            denom += a_v * b[a_i]\n",
    "    for b_i in b:\n",
    "        norm_b += b[b_i] * b[b_i]\n",
    "    \n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "    \n",
    "    return denom / (norm_a * norm_b)\n",
    "\n",
    "def top_n(df, n, thresh_hold = 0.1):\n",
    "    df_count = np.zeros(df.shape)\n",
    "    df_bak = df\n",
    "    df_count[df >= thresh_hold] = 1\n",
    "    _counts = np.sum(df_count, axis=1)\n",
    "    max_index = []\n",
    "    for i in range(0, n):\n",
    "        _index = np.argmax(_counts)\n",
    "        max_index.append(_index)\n",
    "        _counts[_index] = -1\n",
    "    \n",
    "    return df.iloc[max_index][df.columns[max_index]]\n",
    "\n",
    "def slice_df_by_name(df,names):\n",
    "    return df.loc[names][names]\n",
    "\n",
    "def format_obj(df, groups):\n",
    "    _nodes = \"nodes\"\n",
    "    _links = \"links\"\n",
    "    json_obj = {_nodes:[], _links:[]}\n",
    "    sorted_names = []\n",
    "    name2gid = dict()\n",
    "    for g in range(0, len(groups)):\n",
    "        for name in groups[g]:\n",
    "            name2gid[name] = g\n",
    "            if name in df.columns:\n",
    "                sorted_names.append(name)\n",
    "    \n",
    "    df = slice_df_by_name(df, sorted_names)\n",
    "    for c_name in df.columns:\n",
    "        json_obj[_nodes].append({\"name\": c_name, \"group\":name2gid[c_name]})\n",
    "    \n",
    "    for i in range(0, df.shape[0] - 1):\n",
    "        for j in range(i + 1, df.shape[0]):\n",
    "            json_obj[_links].append({\"source\":i, \"target\":j, \"value\":float(df.iloc[i][j])})\n",
    "    \n",
    "    return json_obj\n",
    "\n",
    "def corpus2matrix(corpus, vector_dimension):\n",
    "    _corpus_matrix = np.zeros([len(corpus), vector_dimension])\n",
    "    for i, row in enumerate(corpus):\n",
    "        for j, v in row:\n",
    "            _corpus_matrix[i][j] = v\n",
    "    \n",
    "    return _corpus_matrix\n",
    "    \n",
    "def corpus_similarity(corpus, vector_dimension, distance_func = sklearn.metrics.pairwise.cosine_similarity):\n",
    "    _corpus_matrix = corpus2matrix(corpus, vector_dimension)\n",
    "    #Normailzation\n",
    "#     _corpus_matrix = Normalizer().transform(_corpus_matrix)    \n",
    "    return distance_func(_corpus_matrix)\n",
    "\n",
    "\n",
    "def corpus_similarity_1(corpus):\n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        _sim[i][i] = 1\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return _sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, Birch, DBSCAN\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "def kmean_predict(X, n_clusters):\n",
    "    return KMeans(n_clusters = n_clusters).fit_predict(X)\n",
    "\n",
    "def birch_predict(X, n_clusters):\n",
    "    return Birch(compute_labels = True, n_clusters = n_clusters).fit_predict(X)\n",
    "\n",
    "def dbscan_predict(X, n_clusters):\n",
    "    return DBSCAN(eps=n_clusters * 0.01, min_samples=1).fit_predict(X)\n",
    "\n",
    "cluster_method = {\"kmean\": kmean_predict,\n",
    "                 \"birch\": birch_predict,\n",
    "                 \"dbscan\": dbscan_predict}\n",
    "\n",
    "def get_cluster(features_list, feature_dimension, names, num_cluster = -1, method = \"kmean\", verbose = False):\n",
    "    X = corpus2matrix(features_list, feature_dimension)\n",
    "    Norm_X = Normalizer().transform(X)\n",
    "    \n",
    "    if num_cluster < 0:\n",
    "        best_score = -1\n",
    "        best_k = -1\n",
    "        for k in range(2, 100):\n",
    "            y_pred = cluster_method[method](Norm_X, k)\n",
    "            _score = metrics.silhouette_score(Norm_X, y_pred, metric='euclidean')\n",
    "            if verbose:\n",
    "                print(_score)\n",
    "            if _score > best_score:\n",
    "                best_k = k\n",
    "                best_score = _score\n",
    "        if verbose:\n",
    "            print(\"Best k:%d\"%(best_k))\n",
    "    else:\n",
    "        best_k = num_cluster\n",
    "        \n",
    "    y_pred = cluster_method[method](Norm_X, best_k)\n",
    "    clusters = dict()\n",
    "    name2cluster = dict()\n",
    "    for i in range(0, len(y_pred)):\n",
    "        name2cluster[names[i]] = y_pred[i]\n",
    "        if y_pred[i] in clusters:\n",
    "            clusters[y_pred[i]].append(names[i])\n",
    "        else:\n",
    "            clusters[y_pred[i]] = [names[i]]\n",
    "\n",
    "    return (clusters, name2cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "        \n",
    "sim = corpus_similarity(corpus, len(dictionary))\n",
    "sim_clusters, i = get_cluster(corpus, len(dictionary), cat_names, 10, method='birch')\n",
    "\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.index = cat_names\n",
    "sim_df.columns = cat_names\n",
    "\n",
    "sim_df_50 = top_n(sim_df, 50)\n",
    "selected_names = sim_df_50.columns\n",
    "\n",
    "with open(\"display/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(sim_df, sim_clusters)))\n",
    "\n",
    "with open(\"display/output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(sim_df_50, sim_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns; \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sample = 20\n",
    "# ax = sns.heatmap(data.iloc[0:sample][data.columns[0:sample]],cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "import json\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "tfidf_sim = corpus_similarity(tfidf_corpus, len(dictionary))\n",
    "tfidf_sim_clusters, i = get_cluster(tfidf_corpus, len(dictionary), cat_names, 10, method='birch')\n",
    "\n",
    "tfidf_sim_df = pd.DataFrame(tfidf_sim)\n",
    "tfidf_sim_df.index = cat_names\n",
    "tfidf_sim_df.columns = cat_names\n",
    "#tfidf_sim_df_50 = top_n(tfidf_sim_df, 50)\n",
    "tfidf_sim_df_50 = slice_df_by_name(tfidf_sim_df, selected_names)\n",
    "                              \n",
    "with open(\"display/tfidf_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_sim_df, tfidf_sim_clusters)))\n",
    "with open(\"display/tfidf_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_sim_df_50, tfidf_sim_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\models\\ldamodel.py:582: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done in 29.780834s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# # Enable logging for gensim - optional\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "num_topics = 100\n",
    "\n",
    "t0 = time()\n",
    "lda_model = models.LdaModel(tfidf_corpus, \n",
    "                            num_topics = num_topics, \n",
    "                            id2word = dictionary,\n",
    "                            random_state = 100,\n",
    "                            eval_every=5, \n",
    "                            alpha='auto', \n",
    "                            gamma_threshold=0.01)\n",
    "# lda_model = models.LdaModel(tfidf_corpus, \n",
    "#                             num_topics = num_topics, \n",
    "#                             id2word = dictionary,\n",
    "#                             random_state = 100,\n",
    "#                             update_every = 1,\n",
    "#                             chunksize = 100,\n",
    "#                             passes = 10,\n",
    "#                             alpha = 'auto')\n",
    "\n",
    "doc_topics = lda_model[tfidf_corpus]\n",
    "print(\"Training done in %fs\" % (time() - t0))\n",
    "\n",
    "# t0 = time()\n",
    "# # Compute Perplexity\n",
    "# print('\\nPerplexity: ', lda_model.log_perplexity(tfidf_corpus))  # a measure of how good the model is. lower the better.\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model = lda_model, texts = processed_docs, dictionary = dictionary, coherence = 'c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_lda)\n",
    "# print(\"Evaluation done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '../mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\geesi\\AppData\\Local\\Temp\\c53edf_corpus.txt --output C:\\Users\\geesi\\AppData\\Local\\Temp\\c53edf_corpus.mallet' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-559d8dad39fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlda_mallet_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaMallet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmallet_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmallet_doc_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_mallet_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training done in %fs\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \"\"\"\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;34m'--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\models\\wrappers\\ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[1;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfcorpusmallet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"converting temporary corpus to MALLET format with %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m   1804\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1805\u001b[0m             \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1806\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1807\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1808\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '../mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\geesi\\AppData\\Local\\Temp\\c53edf_corpus.txt --output C:\\Users\\geesi\\AppData\\Local\\Temp\\c53edf_corpus.mallet' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "#Dosen't work\n",
    "\n",
    "mallet_path = '../mallet-2.0.8/bin/mallet'\n",
    "\n",
    "t0 = time()\n",
    "lda_mallet_model = models.wrappers.LdaMallet(mallet_path, corpus = corpus, num_topics = num_topics, id2word = dictionary)\n",
    "mallet_doc_topics = lda_mallet_model[corpus]\n",
    "print(\"Training done in %fs\" % (time() - t0))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# t0 = time()\n",
    "# coherence_model_ldamallet = CoherenceModel(model = lda_mallet_model, texts = processed_docs, dictionary = dictionary, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "# print(\"Evaluation done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_sim = corpus_similarity(doc_topics, num_topics)\n",
    "lda_sim_clusters, i = get_cluster(doc_topics, num_topics, cat_names, 10, method='birch')\n",
    "\n",
    "lda_sim_df = pd.DataFrame(lda_sim)\n",
    "lda_sim_df.index = cat_names\n",
    "lda_sim_df.columns = cat_names\n",
    "# lda_sim_df_50 = top_n(lda_sim_df, 50)\n",
    "lda_sim_df_50 = slice_df_by_name(lda_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_sim_df, lda_sim_clusters)))\n",
    "with open(\"display/lda_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_sim_df_50, lda_sim_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mallet_sim = corpus_similarity(mallet_doc_topics, num_topics)\n",
    "lda_mallet_sim_clusters, i = get_cluster(mallet_doc_topics, num_topics, cat_names, 10, method='birch')\n",
    "\n",
    "lda_mallet_sim_df = pd.DataFrame(lda_mallet_sim)\n",
    "lda_mallet_sim_df.index = cat_names\n",
    "lda_mallet_sim_df.columns = cat_names\n",
    "# lda_mallet_sim_df_50 = top_n(lda_sim_df, 50)\n",
    "lda_mallet_sim_df_50 = slice_df_by_name(lda_mallet_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_mallet_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_mallet_sim_df, lda_mallet_sim_clusters)))\n",
    "with open(\"display/lda_mallet_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_mallet_sim_df_50, lda_mallet_sim_clusters)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, tfidf_corpus, dictionary)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest_coherence = -1e20\n",
    "# best_k = 0\n",
    "# best_model = None\n",
    "# for k in range(5, 150, 2):\n",
    "#     model = models.LdaModel(tfidf_corpus, num_topics = k, id2word=dictionary)\n",
    "#     cm = models.coherencemodel.CoherenceModel(model=model, corpus=tfidf_corpus, coherence='u_mass')\n",
    "#     coherence = cm.get_coherence()\n",
    "#     print(\"k=%d coherence=%f\"%(k, coherence))\n",
    "#     if (coherence > largest_coherence):\n",
    "#         largest_coherence = coherence\n",
    "#         best_model = model\n",
    "#         best_k = k\n",
    "\n",
    "# print(\"best_k:%d\"%(best_k))\n",
    "# for idx, topic in best_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_file = \"cuisine_indices.txt\"\n",
    "matrix_file = \"cuisine_sim_matrix.csv\"\n",
    "\n",
    "with open (names_file, 'r') as f:\n",
    "    names = f.read().split(\"\\n\")\n",
    "\n",
    "demo_data = pd.read_csv(matrix_file, header=None)\n",
    "demo_data.index = names\n",
    "demo_data.columns = names\n",
    "\n",
    "with open(\"display/demo_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(demo_data, np.ones(demo_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path2reviewdump = \"reviews/reviews.dat\"\n",
    "\n",
    "# with open(path2reviewdump, \"r\") as f:\n",
    "#     reviews = f.readlines()\n",
    "# review_docs = [preprocess(text) for text in tqdm(reviews)]\n",
    "# review_dictionary = corpora.Dictionary(review_docs)\n",
    "# print(\"Before prunn:%d\"%(len(review_dictionary)))\n",
    "# review_dictionary.filter_extremes(no_below=15, no_above = 0.5)\n",
    "# print(\"After prunn:%d\"%(len(review_dictionary)))\n",
    "# review_corpus = [review_dictionary.doc2bow(doc) for doc in review_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time\n",
    "\n",
    "# t0 = time()\n",
    "# review_model = models.LdaModel(review_corpus, num_topics=100, id2word=review_dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "# print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "# for idx, topic in review_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_topics(cat_topics):\n",
    "#     topics = {}\n",
    "#     for _sub_topics in cat_topics:\n",
    "#         for _topic, _value in _sub_topics:\n",
    "#             if _topic in topics:\n",
    "#                 topics[_topic] += _value\n",
    "#             else:\n",
    "#                 topics[_topic] = _value\n",
    "    \n",
    "#     return topics\n",
    "\n",
    "# all_topics = []\n",
    "# cat_names = []\n",
    "# for i in tqdm(range(0, len(cat_list))):\n",
    "#     cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "#     with open(cat_list[i]) as f:\n",
    "#         cat_docs = [preprocess(text) for text in f.readlines()]\n",
    "#         cat_corpus = [review_dictionary.doc2bow(doc) for doc in cat_docs]\n",
    "#         cat_topics = review_model[cat_corpus]\n",
    "#         all_topics.append(combine_topics(cat_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lda_individual_sim = corpus_similarity([[(k, topic[k]) for k in topic] for topic in all_topics], len(review_dictionary))\n",
    "\n",
    "# lda_individual_sim_df = pd.DataFrame(lda_individual_sim)\n",
    "# lda_individual_sim_df.index = cat_names\n",
    "# lda_individual_sim_df.columns = cat_names\n",
    "# lda_individual_data = top_n(lda_individual_sim_df, 50)\n",
    "\n",
    "# with open(\"display/lda_ind_output.json\", \"w\") as f:\n",
    "#     f.write(json.dumps(format_obj(lda_individual_data, np.ones(lda_individual_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_clusters(cluster1, cluster2, title):\n",
    "    children_name = 'children'\n",
    "    name_name = 'name'\n",
    "    value_name = 'value'\n",
    "    color_name = 'color'    \n",
    "\n",
    "    _out = {name_name: title, children_name:[]}\n",
    "\n",
    "#     name2cluster1 = dict()\n",
    "#     for _group_id in range(0, len(cluster1)):\n",
    "#         for _name in cluster1[_group_id]:\n",
    "#             name2cluster1[_name]= _group_id\n",
    "\n",
    "    name2cluster2 = dict()            \n",
    "    for _group_id in range(0, len(cluster2)):\n",
    "        for _name in cluster2[_group_id]:\n",
    "            name2cluster2[_name]= _group_id\n",
    "                \n",
    "    for _group_id in range(0, len(cluster1)):\n",
    "        _out[children_name].append({name_name: 'Cluster ' + str(_group_id), children_name:[]})\n",
    "        for _name_id in range(0, len(cluster1[_group_id])):\n",
    "            _out[children_name][_group_id][children_name].append({name_name: cluster1[_group_id][_name_id],\n",
    "                                                                  \"cluster1\": _group_id,\n",
    "                                                                  \"cluster2\": name2cluster2[cluster1[_group_id][_name_id]]})\n",
    "        \n",
    "    return _out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011378078210452251\n",
      "0.003850765112039594\n",
      "0.010662896382978255\n",
      "0.014981799839011857\n",
      "0.022610534852843683\n",
      "0.026237686346998645\n",
      "0.029960348669318116\n",
      "0.033899416099357566\n",
      "0.03551696751816912\n",
      "0.03917860720836795\n",
      "0.04718412338058862\n",
      "0.05041446346895162\n",
      "0.05078252266347051\n",
      "0.05227559563814271\n",
      "0.057986542201610196\n",
      "0.060536693255543374\n",
      "0.0626983631025126\n",
      "0.06404943960399567\n",
      "0.06599528064134054\n",
      "0.06762236169467138\n",
      "0.06964551663689311\n",
      "0.07651489269521856\n",
      "0.07843838229623068\n",
      "0.08139279856582178\n",
      "0.08285414032962153\n",
      "0.08536080864179174\n",
      "0.08669187647881872\n",
      "0.08776420026661425\n",
      "0.0862399557676451\n",
      "0.08718554784390245\n",
      "0.08775172628637501\n",
      "0.08787410511443243\n",
      "0.09156873710906564\n",
      "0.09298957102847667\n",
      "0.09323794225926917\n",
      "0.09442402205968554\n",
      "0.09541197219784264\n",
      "0.09741884462592951\n",
      "0.0975220898648321\n",
      "0.09779616596208755\n",
      "0.0972760667483393\n",
      "0.0974918756640594\n",
      "0.09787922888186702\n",
      "0.09824206539266712\n",
      "0.09913780157971411\n",
      "0.09858119116336637\n",
      "0.09899471427796168\n",
      "0.09961804474477945\n",
      "0.10020222251293709\n",
      "0.10069474691047346\n",
      "0.09985669241012639\n",
      "0.099243133260884\n",
      "0.09966425061195576\n",
      "0.10086979143632324\n",
      "0.10139556658154225\n",
      "0.10181120558676306\n",
      "0.10056288824241023\n",
      "0.10041177150160574\n",
      "0.1076370976116281\n",
      "0.1079348829259738\n",
      "0.10932350748377484\n",
      "0.10878207379599411\n",
      "0.10872395320594266\n",
      "0.10825268436005715\n",
      "0.10723392902283754\n",
      "0.10561617191340515\n",
      "0.10946569814191622\n",
      "0.11243022759212151\n",
      "0.11229050774754874\n",
      "0.11138678170363324\n",
      "0.11130950161577514\n",
      "0.11097091882539853\n",
      "0.11122388448132263\n",
      "0.1104863156435631\n",
      "0.1100021453028554\n",
      "0.10896344156617638\n",
      "0.11517353107634933\n",
      "0.114361105781393\n",
      "0.11311777416421437\n",
      "0.11389815988509099\n",
      "0.11297143657433359\n",
      "0.11757780317971073\n",
      "0.11788706301749126\n",
      "0.11783228143580139\n",
      "0.11769944536684744\n",
      "0.12353284978607061\n",
      "0.12275129093528481\n",
      "0.12195744573654628\n",
      "0.12655415717416746\n",
      "0.12493602589886671\n",
      "0.12331482771963692\n",
      "0.12300303869278849\n",
      "0.12365777466377206\n",
      "0.12203054658059971\n",
      "0.12061427274836295\n",
      "0.11860092793136155\n",
      "0.11662965222045427\n",
      "0.11928223072104728\n",
      "Best k:90\n"
     ]
    }
   ],
   "source": [
    "sim_clusters_2, i = get_cluster(tfidf_corpus, len(dictionary), cat_names, 2)\n",
    "sim_clusters, i = get_cluster(corpus, len(dictionary), cat_names)\n",
    "with open(\"display/cluster_kmean_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dump_clusters(sim_clusters, sim_clusters_2, \"Cuisine Clustering by KMean\")))\n",
    "    \n",
    "sim_clusters_2, i = get_cluster(tfidf_corpus, len(dictionary), cat_names, 2, method='birch')\n",
    "sim_clusters, i = get_cluster(tfidf_corpus, len(dictionary), cat_names, method='birch')\n",
    "with open(\"display/cluster_birch_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dump_clusters(sim_clusters, sim_clusters_2, \"Cuisine Clustering by Birch\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

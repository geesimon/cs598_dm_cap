{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling with Gensim\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 147/147 [29:22<00:00,  7.92s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before prunn:193047\n",
      "After prunn:100000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "cat_list = sorted(glob.glob (\"cuisines/*\"))\n",
    "cat_size = len(cat_list)\n",
    "\n",
    "random.seed(0)\n",
    "cat_names = []\n",
    "cat_text = []\n",
    "# sample_size = min(30, cat_size)\n",
    "# cat_sample = sorted(random.sample(range(cat_size), sample_size))\n",
    "cat_sample = range(0, cat_size)\n",
    "\n",
    "count = 0\n",
    "for i in cat_sample:\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_text.append(f.read().replace(\"\\n\", \"\").replace(\"\\r\",\"\"))\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(cat_text)]\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(dictionary)))\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sklearn\n",
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    b = dict(b)\n",
    "    norm_a = 0\n",
    "    norm_b = 0\n",
    "    denom = 0\n",
    "    for a_i, a_v in a:\n",
    "        norm_a += a_v * a_v\n",
    "        if a_i in b:\n",
    "            denom += a_v * b[a_i]\n",
    "    for b_i in b:\n",
    "        norm_b += b[b_i] * b[b_i]\n",
    "    \n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "    \n",
    "    return denom / (norm_a * norm_b)\n",
    "\n",
    "def top_n(df, n, thresh_hold = 0.1):\n",
    "    df_count = np.zeros(df.shape)\n",
    "    df_bak = df\n",
    "    df_count[df >= thresh_hold] = 1\n",
    "    _counts = np.sum(df_count, axis=1)\n",
    "    max_index = []\n",
    "    for i in range(0, n):\n",
    "        _index = np.argmax(_counts)\n",
    "        max_index.append(_index)\n",
    "        _counts[_index] = -1\n",
    "    \n",
    "    return df.iloc[max_index][df.columns[max_index]]\n",
    "\n",
    "def slice_df_by_name(df,names):\n",
    "    return df.loc[names][names]\n",
    "\n",
    "def format_obj(df, groups):\n",
    "    _nodes = \"nodes\"\n",
    "    _links = \"links\"\n",
    "    json_obj = {_nodes:[], _links:[]}\n",
    "    sorted_names = []\n",
    "    name2gid = dict()\n",
    "    for g in range(0, len(groups)):\n",
    "        for name in groups[g]:\n",
    "            name2gid[name] = g\n",
    "            if name in df.columns:\n",
    "                sorted_names.append(name)\n",
    "    \n",
    "    df = slice_df_by_name(df, sorted_names)\n",
    "    for c_name in df.columns:\n",
    "        json_obj[_nodes].append({\"name\": c_name, \"group\":name2gid[c_name]})\n",
    "    \n",
    "    for i in range(0, df.shape[0] - 1):\n",
    "        for j in range(i + 1, df.shape[0]):\n",
    "            json_obj[_links].append({\"source\":i, \"target\":j, \"value\":float(df.iloc[i][j])})\n",
    "    \n",
    "    return json_obj\n",
    "\n",
    "def corpus2matrix(corpus, vector_dimension):\n",
    "    _corpus_matrix = np.zeros([len(corpus), vector_dimension])\n",
    "    for i, row in enumerate(corpus):\n",
    "        for j, v in row:\n",
    "            _corpus_matrix[i][j] = v\n",
    "    \n",
    "    return _corpus_matrix\n",
    "    \n",
    "def corpus_similarity(corpus, vector_dimension, distance_func = sklearn.metrics.pairwise.cosine_similarity):\n",
    "    _corpus_matrix = corpus2matrix(corpus, vector_dimension)\n",
    "    #Normailzation\n",
    "#     _corpus_matrix = Normalizer().transform(_corpus_matrix)    \n",
    "    return distance_func(_corpus_matrix)\n",
    "\n",
    "\n",
    "def corpus_similarity_1(corpus):\n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        _sim[i][i] = 1\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return _sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, Birch, DBSCAN\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "def kmean_predict(X, n_clusters):\n",
    "    return KMeans(n_clusters = n_clusters).fit_predict(X)\n",
    "\n",
    "def birch_predict(X, n_clusters):\n",
    "    return Birch(compute_labels = True, n_clusters = n_clusters).fit_predict(X)\n",
    "\n",
    "def dbscan_predict(X, n_clusters):\n",
    "    return DBSCAN(eps=n_clusters * 0.01, min_samples=1).fit_predict(X)\n",
    "\n",
    "cluster_method = {\"kmean\": kmean_predict,\n",
    "                 \"birch\": birch_predict,\n",
    "                 \"dbscan\": dbscan_predict}\n",
    "\n",
    "def get_cluster(features_list, feature_dimension, names, num_cluster = -1, method = \"kmean\", verbose = False):\n",
    "    if type(features_list) == np.ndarray:\n",
    "        X = features_list\n",
    "    else:\n",
    "        X = corpus2matrix(features_list, feature_dimension)\n",
    "    \n",
    "    Norm_X = Normalizer().transform(X)\n",
    "    \n",
    "    if num_cluster < 0:\n",
    "        best_score = -1\n",
    "        best_k = -1\n",
    "        for k in range(2, 100):\n",
    "            y_pred = cluster_method[method](Norm_X, k)\n",
    "            _score = metrics.silhouette_score(Norm_X, y_pred, metric='euclidean')\n",
    "#             _score = metrics.calinski_harabasz_score(Norm_X, y_pred) \n",
    "            if verbose:\n",
    "                print(_score)\n",
    "            if _score > best_score:\n",
    "                best_k = k\n",
    "                best_score = _score\n",
    "        if verbose:\n",
    "            print(\"Best k:%d\"%(best_k))\n",
    "    else:\n",
    "        best_k = num_cluster\n",
    "        \n",
    "    y_pred = cluster_method[method](Norm_X, best_k)\n",
    "    clusters = dict()\n",
    "    name2cluster = dict()\n",
    "    for i in range(0, len(y_pred)):\n",
    "        name2cluster[names[i]] = y_pred[i]\n",
    "        if y_pred[i] in clusters:\n",
    "            clusters[y_pred[i]].append(names[i])\n",
    "        else:\n",
    "            clusters[y_pred[i]] = [names[i]]\n",
    "\n",
    "    return (clusters, name2cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = corpus_similarity(corpus, len(dictionary))\n",
    "sim_clusters, i = get_cluster(corpus, len(dictionary), cat_names, 10, method='birch')\n",
    "\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.index = cat_names\n",
    "sim_df.columns = cat_names\n",
    "\n",
    "sim_df_50 = top_n(sim_df, 50)\n",
    "selected_names = sim_df_50.columns\n",
    "\n",
    "with open(\"display/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(sim_df, sim_clusters)))\n",
    "\n",
    "with open(\"display/output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(sim_df_50, sim_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns; \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sample = 20\n",
    "# ax = sns.heatmap(data.iloc[0:sample][data.columns[0:sample]],cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "import json\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "tfidf_sim = corpus_similarity(tfidf_corpus, len(dictionary))\n",
    "tfidf_sim_clusters, i = get_cluster(tfidf_corpus, len(dictionary), cat_names, 10, method='birch')\n",
    "\n",
    "tfidf_sim_df = pd.DataFrame(tfidf_sim)\n",
    "tfidf_sim_df.index = cat_names\n",
    "tfidf_sim_df.columns = cat_names\n",
    "#tfidf_sim_df_50 = top_n(tfidf_sim_df, 50)\n",
    "tfidf_sim_df_50 = slice_df_by_name(tfidf_sim_df, selected_names)\n",
    "                              \n",
    "with open(\"display/tfidf_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_sim_df, tfidf_sim_clusters)))\n",
    "with open(\"display/tfidf_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_sim_df_50, tfidf_sim_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/gensim/models/ldamodel.py:582: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done in 24.541629s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "num_topics = 100\n",
    "\n",
    "t0 = time()\n",
    "lda_model = models.LdaModel(tfidf_corpus, \n",
    "                            num_topics = num_topics, \n",
    "                            id2word = dictionary,\n",
    "                            random_state = 100,\n",
    "                            eval_every=5, \n",
    "                            alpha='auto', \n",
    "                            gamma_threshold=0.01)\n",
    "# lda_model = models.LdaModel(tfidf_corpus, \n",
    "#                             num_topics = num_topics, \n",
    "#                             id2word = dictionary,\n",
    "#                             random_state = 100,\n",
    "#                             update_every = 1,\n",
    "#                             chunksize = 100,\n",
    "#                             passes = 10,\n",
    "#                             alpha = 'auto')\n",
    "\n",
    "doc_topics = lda_model[tfidf_corpus]\n",
    "print(\"Training done in %fs\" % (time() - t0))\n",
    "\n",
    "# t0 = time()\n",
    "# # Compute Perplexity\n",
    "# print('\\nPerplexity: ', lda_model.log_perplexity(tfidf_corpus))  # a measure of how good the model is. lower the better.\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model = lda_model, texts = processed_docs, dictionary = dictionary, coherence = 'c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_lda)\n",
    "# print(\"Evaluation done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done in 1238.064361s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "mallet_path = \"..\" + os.sep + \"mallet-2.0.8\"+ os.sep + \"bin\" + os.sep +\"mallet\"\n",
    "t0 = time()\n",
    "lda_mallet_model = models.wrappers.LdaMallet(mallet_path, \n",
    "                                             corpus = corpus, \n",
    "                                             num_topics = num_topics, \n",
    "                                             id2word = dictionary)\n",
    "mallet_doc_topics = lda_mallet_model[corpus]\n",
    "print(\"Training done in %fs\" % (time() - t0))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# t0 = time()\n",
    "# coherence_model_ldamallet = CoherenceModel(model = lda_mallet_model, texts = processed_docs, dictionary = dictionary, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "# print(\"Evaluation done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_sim = corpus_similarity(doc_topics, num_topics)\n",
    "lda_sim_clusters, i = get_cluster(doc_topics, num_topics, cat_names, 10, method='birch')\n",
    "\n",
    "lda_sim_df = pd.DataFrame(lda_sim)\n",
    "lda_sim_df.index = cat_names\n",
    "lda_sim_df.columns = cat_names\n",
    "# lda_sim_df_50 = top_n(lda_sim_df, 50)\n",
    "lda_sim_df_50 = slice_df_by_name(lda_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_sim_df, lda_sim_clusters)))\n",
    "with open(\"display/lda_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_sim_df_50, lda_sim_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mallet_sim = corpus_similarity(mallet_doc_topics, num_topics)\n",
    "lda_mallet_sim_clusters, i = get_cluster(mallet_doc_topics, num_topics, cat_names, 10, method='birch')\n",
    "\n",
    "lda_mallet_sim_df = pd.DataFrame(lda_mallet_sim)\n",
    "lda_mallet_sim_df.index = cat_names\n",
    "lda_mallet_sim_df.columns = cat_names\n",
    "# lda_mallet_sim_df_50 = top_n(lda_sim_df, 50)\n",
    "lda_mallet_sim_df_50 = slice_df_by_name(lda_mallet_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_mallet_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_mallet_sim_df, lda_mallet_sim_clusters)))\n",
    "with open(\"display/lda_mallet_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_mallet_sim_df_50, lda_mallet_sim_clusters)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, tfidf_corpus, dictionary)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest_coherence = -1e20\n",
    "# best_k = 0\n",
    "# best_model = None\n",
    "# for k in range(5, 150, 2):\n",
    "#     model = models.LdaModel(tfidf_corpus, num_topics = k, id2word=dictionary)\n",
    "#     cm = models.coherencemodel.CoherenceModel(model=model, corpus=tfidf_corpus, coherence='u_mass')\n",
    "#     coherence = cm.get_coherence()\n",
    "#     print(\"k=%d coherence=%f\"%(k, coherence))\n",
    "#     if (coherence > largest_coherence):\n",
    "#         largest_coherence = coherence\n",
    "#         best_model = model\n",
    "#         best_k = k\n",
    "\n",
    "# print(\"best_k:%d\"%(best_k))\n",
    "# for idx, topic in best_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names_file = \"cuisine_indices.txt\"\n",
    "# matrix_file = \"cuisine_sim_matrix.csv\"\n",
    "\n",
    "# with open (names_file, 'r') as f:\n",
    "#     names = f.read().split(\"\\n\")\n",
    "\n",
    "# demo_data = pd.read_csv(matrix_file, header=None)\n",
    "# demo_data.index = names\n",
    "# demo_data.columns = names\n",
    "\n",
    "# with open(\"display/demo_output.json\", \"w\") as f:\n",
    "#     f.write(json.dumps(format_obj(demo_data, np.ones(demo_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path2reviewdump = \"reviews/reviews.dat\"\n",
    "\n",
    "# with open(path2reviewdump, \"r\") as f:\n",
    "#     reviews = f.readlines()\n",
    "# review_docs = [preprocess(text) for text in tqdm(reviews)]\n",
    "# review_dictionary = corpora.Dictionary(review_docs)\n",
    "# print(\"Before prunn:%d\"%(len(review_dictionary)))\n",
    "# review_dictionary.filter_extremes(no_below=15, no_above = 0.5)\n",
    "# print(\"After prunn:%d\"%(len(review_dictionary)))\n",
    "# review_corpus = [review_dictionary.doc2bow(doc) for doc in review_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time\n",
    "\n",
    "# t0 = time()\n",
    "# review_model = models.LdaModel(review_corpus, num_topics=100, id2word=review_dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "# print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "# for idx, topic in review_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_topics(cat_topics):\n",
    "#     topics = {}\n",
    "#     for _sub_topics in cat_topics:\n",
    "#         for _topic, _value in _sub_topics:\n",
    "#             if _topic in topics:\n",
    "#                 topics[_topic] += _value\n",
    "#             else:\n",
    "#                 topics[_topic] = _value\n",
    "    \n",
    "#     return topics\n",
    "\n",
    "# all_topics = []\n",
    "# cat_names = []\n",
    "# for i in tqdm(range(0, len(cat_list))):\n",
    "#     cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "#     with open(cat_list[i]) as f:\n",
    "#         cat_docs = [preprocess(text) for text in f.readlines()]\n",
    "#         cat_corpus = [review_dictionary.doc2bow(doc) for doc in cat_docs]\n",
    "#         cat_topics = review_model[cat_corpus]\n",
    "#         all_topics.append(combine_topics(cat_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lda_individual_sim = corpus_similarity([[(k, topic[k]) for k in topic] for topic in all_topics], len(review_dictionary))\n",
    "\n",
    "# lda_individual_sim_df = pd.DataFrame(lda_individual_sim)\n",
    "# lda_individual_sim_df.index = cat_names\n",
    "# lda_individual_sim_df.columns = cat_names\n",
    "# lda_individual_data = top_n(lda_individual_sim_df, 50)\n",
    "\n",
    "# with open(\"display/lda_ind_output.json\", \"w\") as f:\n",
    "#     f.write(json.dumps(format_obj(lda_individual_data, np.ones(lda_individual_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_clusters(cluster1, cluster2, title):\n",
    "    children_name = 'children'\n",
    "    name_name = 'name'\n",
    "    value_name = 'value'\n",
    "    color_name = 'color'    \n",
    "\n",
    "    _out = {name_name: title, children_name:[]}\n",
    "\n",
    "#     name2cluster1 = dict()\n",
    "#     for _group_id in range(0, len(cluster1)):\n",
    "#         for _name in cluster1[_group_id]:\n",
    "#             name2cluster1[_name]= _group_id\n",
    "\n",
    "    name2cluster2 = dict()            \n",
    "    for _group_id in range(0, len(cluster2)):\n",
    "        for _name in cluster2[_group_id]:\n",
    "            name2cluster2[_name]= _group_id\n",
    "                \n",
    "    for _group_id in range(0, len(cluster1)):\n",
    "        _out[children_name].append({name_name: str(_group_id), children_name:[]})\n",
    "        for _name_id in range(0, len(cluster1[_group_id])):\n",
    "            _out[children_name][_group_id][children_name].append({name_name: cluster1[_group_id][_name_id],\n",
    "                                                                  \"cluster1\": _group_id,\n",
    "                                                                  \"cluster2\": name2cluster2[cluster1[_group_id][_name_id]]})\n",
    "        \n",
    "    return _out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering done in 19.013090s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (79). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (80). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (81). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (82). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (83). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (84). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (85). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (86). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (87). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (88). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (89). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (90). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (91). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (92). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (93). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (94). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (95). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (96). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering done in 1.275685s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (97). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (98). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n",
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/sklearn/cluster/birch.py:629: ConvergenceWarning: Number of subclusters found (78) by Birch is less than (99). Decrease the threshold.\n",
      "  % (len(centroids), self.n_clusters), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "num_cluster = 10\n",
    "sim_clusters_kmean_small, i = get_cluster(mallet_doc_topics, num_topics, cat_names, num_cluster)\n",
    "sim_clusters_kmean, i = get_cluster(mallet_doc_topics, num_topics, cat_names)\n",
    "with open(\"display/cluster_kmean_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dump_clusters(sim_clusters_kmean, sim_clusters_kmean_small, \"Cuisine Clustering by KMean\")))\n",
    "print(\"Clustering done in %fs\" % (time() - t0))\n",
    "  \n",
    "t0 = time()\n",
    "sim_clusters_birch_small, i = get_cluster(mallet_doc_topics, num_topics, cat_names, num_cluster, method='birch')\n",
    "sim_clusters_birch, i = get_cluster(mallet_doc_topics, num_topics, cat_names, method='birch')\n",
    "with open(\"display/cluster_birch_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dump_clusters(sim_clusters_birch, sim_clusters_birch_small, \"Cuisine Clustering by Birch\")))\n",
    "print(\"Clustering done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rest_reviews.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-799ad28fa27d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rest_reviews.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mreview_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rest_reviews.txt'"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "with open(\"rest_reviews.txt\") as f:\n",
    "    review_docs = f.readlines()\n",
    "\n",
    "processed_review_docs = [preprocess(doc) for doc in tqdm(review_docs)]\n",
    "d2v_docs = [TaggedDocument(doc, [i]) for i, doc in tqdm(enumerate(processed_review_docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_len = 100\n",
    "%time d2v_model = Doc2Vec(d2v_docs, vector_size=vector_len, workers=4)\n",
    "d2v = np.array([d2v_model.infer_vector(doc) for doc in tqdm(processed_docs)])\n",
    "d2v_sim = sklearn.metrics.pairwise.cosine_similarity(d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_sim_clusters, i = get_cluster(d2v, vector_len, cat_names, 10, method='birch')\n",
    "\n",
    "d2v_sim_df = pd.DataFrame(d2v_sim)\n",
    "d2v_sim_df.index = cat_names\n",
    "d2v_sim_df.columns = cat_names  \n",
    "# lda_mallet_sim_df_50 = top_n(lda_sim_df, 50)\n",
    "d2v_sim_df_50 = slice_df_by_name(d2v_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/d2v_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(d2v_sim_df, d2v_sim_clusters)))\n",
    "with open(\"display/d2v_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(d2v_sim_df_50, d2v_sim_clusters)))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "  1%|â–Œ                                                                                 | 1/163 [00:03<09:10,  3.40s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4e8257cc8574>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mcat_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mprocessed_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Before prunn:%d\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4e8257cc8574>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mcat_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mprocessed_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Before prunn:%d\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4e8257cc8574>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatize_stemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4e8257cc8574>\u001b[0m in \u001b[0;36mlemmatize_stemming\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize_stemming\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'v'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\nltk\\stem\\snowball.py\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m   1500\u001b[0m         \u001b[1;31m# STEP 1b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__step1b_suffixes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1502\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1503\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"eed\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"eedly\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "cat_list = glob.glob (\"cuisines/*\")\n",
    "cat_size = len(cat_list)\n",
    "\n",
    "random.seed(0)\n",
    "cat_names = []\n",
    "cat_text = []\n",
    "# sample_size = min(30, cat_size)\n",
    "# cat_sample = sorted(random.sample(range(cat_size), sample_size))\n",
    "cat_sample = range(0, cat_size)\n",
    "\n",
    "count = 0\n",
    "for i in cat_sample:\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_text.append(f.read().replace(\"\\n\", \"\").replace(\"\\r\",\"\"))\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(cat_text)]\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(dictionary)))\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    b = dict(b)\n",
    "    norm_a = 0\n",
    "    norm_b = 0\n",
    "    denom = 0\n",
    "    for a_i, a_v in a:\n",
    "        norm_a += a_v * a_v\n",
    "        if a_i in b:\n",
    "            denom += a_v * b[a_i]\n",
    "    for b_i in b:\n",
    "        norm_b += b[b_i] * b[b_i]\n",
    "    \n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "#     print(norm_a)\n",
    "#     print(norm_b)\n",
    "#     print(denom)\n",
    "    \n",
    "    return denom / (norm_a * norm_b)\n",
    "\n",
    "def top_n(df, n, thresh_hold = 0.1):\n",
    "    df_count = np.zeros(df.shape)\n",
    "    df_bak = df\n",
    "    df_count[df >= thresh_hold] = 1\n",
    "    _counts = np.sum(df_count, axis=1)\n",
    "    max_index = []\n",
    "    for i in range(0, n):\n",
    "        _index = np.argmax(_counts)\n",
    "        max_index.append(_index)\n",
    "        _counts[_index] = -1\n",
    "    \n",
    "    return df.iloc[max_index][df.columns[max_index]]\n",
    "\n",
    "def format_obj(df, groups):\n",
    "    _nodes = \"nodes\"\n",
    "    _links = \"links\"\n",
    "    json_obj = {_nodes:[], _links:[]}\n",
    "    for i in range(0, len(df.columns)):\n",
    "        json_obj[_nodes].append({\"name\":df.columns[i], \"group\":groups[i]})\n",
    "    \n",
    "    for i in range(0, df.shape[0] - 1):\n",
    "        for j in range(i + 1, df.shape[0]):\n",
    "            json_obj[_links].append({\"source\":i, \"target\":j, \"value\":df.iloc[i][j]})\n",
    "    \n",
    "    return json_obj\n",
    "\n",
    "def corpus_similarity(corpus, vector_dimension):\n",
    "#     _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "#     for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#         _sim[i][i] = 1\n",
    "#         for j in range(i + 1, len(corpus)):\n",
    "#             _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#             _sim[j][i] = _sim[i][j]\n",
    "    _corpus_matrix = np.zeros([len(corpus), vector_dimension])\n",
    "    for i, row in enumerate(corpus):\n",
    "        for j, v in row:\n",
    "            _corpus_matrix[i][j] = v\n",
    "    \n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i, j] = spatial.distance.cosine(_corpus_matrix[i], _corpus_matrix[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return 1 - _sim\n",
    "\n",
    "def corpus_similarity_1(corpus):\n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        _sim[i][i] = 1\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return 1 - _sim\n",
    "\n",
    "def slice_df_by_name(df,names):\n",
    "    return df.loc[names][names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "# for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#     sim[i][i] = 1\n",
    "#     for j in range(i + 1, len(corpus)):\n",
    "#         sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#         sim[j][i] = sim[i][j]\n",
    "        \n",
    "sim = corpus_similarity(corpus, len(dictionary))\n",
    "\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.index = cat_names\n",
    "sim_df.columns = cat_names\n",
    "data = top_n(sim_df, 50)\n",
    "selected_names = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(data, np.ones(data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns; \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sample = 20\n",
    "# ax = sns.heatmap(data.iloc[0:sample][data.columns[0:sample]],cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "tfidf_sim = corpus_similarity(tfidf_corpus, len(dictionary))\n",
    "# np.zeros([len(tfidf_corpus), len(tfidf_corpus)])\n",
    "\n",
    "# for i in tqdm(range(0, len(tfidf_corpus) - 1)):\n",
    "#     tfidf_sim[i][i] = 1\n",
    "#     for j in range(i + 1, len(tfidf_corpus)):\n",
    "#         tfidf_sim[i][j] = cosine_similarity(tfidf_corpus[i], tfidf_corpus[j])\n",
    "#         tfidf_sim[j][i] = tfidf_sim[i][j]\n",
    "        \n",
    "tfidf_sim_df = pd.DataFrame(tfidf_sim)\n",
    "tfidf_sim_df.index = cat_names\n",
    "tfidf_sim_df.columns = cat_names\n",
    "# tfidf_data = top_n(tfidf_sim_df, 50)\n",
    "tfidf_data = slice_df_by_name(tfidf_sim_df, selected_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/tfidf_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_data, np.ones(tfidf_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "lda_model = models.LdaModel(tfidf_corpus, num_topics = 100, id2word=dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "doc_topics = lda_model.get_document_topics(tfidf_corpus)\n",
    "print(\"done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_sim = corpus_similarity(doc_topics, len(dictionary))\n",
    "\n",
    "lda_sim_df = pd.DataFrame(lda_sim)\n",
    "lda_sim_df.index = cat_names\n",
    "lda_sim_df.columns = cat_names\n",
    "# lda_data = top_n(lda_sim_df, 50)\n",
    "lda_data = slice_df_by_name(lda_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_data, np.ones(lda_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest_coherence = -1e20\n",
    "# best_k = 0\n",
    "# best_model = None\n",
    "# for k in range(5, 150, 2):\n",
    "#     model = models.LdaModel(tfidf_corpus, num_topics = k, id2word=dictionary)\n",
    "#     cm = models.coherencemodel.CoherenceModel(model=model, corpus=tfidf_corpus, coherence='u_mass')\n",
    "#     coherence = cm.get_coherence()\n",
    "#     print(\"k=%d coherence=%f\"%(k, coherence))\n",
    "#     if (coherence > largest_coherence):\n",
    "#         largest_coherence = coherence\n",
    "#         best_model = model\n",
    "#         best_k = k\n",
    "\n",
    "# print(\"best_k:%d\"%(best_k))\n",
    "# for idx, topic in best_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_file = \"cuisine_indices.txt\"\n",
    "matrix_file = \"cuisine_sim_matrix.csv\"\n",
    "\n",
    "with open (names_file, 'r') as f:\n",
    "    names = f.read().split(\"\\n\")\n",
    "\n",
    "demo_data = pd.read_csv(matrix_file, header=None)\n",
    "demo_data.index = names\n",
    "demo_data.columns = names\n",
    "\n",
    "with open(\"display/demo_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(demo_data, np.ones(demo_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2reviewdump = \"reviews/reviews.dat\"\n",
    "\n",
    "with open(path2reviewdump, \"r\") as f:\n",
    "    reviews = f.readlines()\n",
    "review_docs = [preprocess(text) for text in tqdm(reviews)]\n",
    "review_dictionary = corpora.Dictionary(review_docs)\n",
    "print(\"Before prunn:%d\"%(len(review_dictionary)))\n",
    "review_dictionary.filter_extremes(no_below=15, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(review_dictionary)))\n",
    "review_corpus = [review_dictionary.doc2bow(doc) for doc in review_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "review_model = models.LdaModel(review_corpus, num_topics=100, id2word=review_dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "for idx, topic in review_model.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_topics(cat_topics):\n",
    "    topics = {}\n",
    "    for _sub_topics in cat_topics:\n",
    "        for _topic, _value in _sub_topics:\n",
    "            if _topic in topics:\n",
    "                topics[_topic] += _value\n",
    "            else:\n",
    "                topics[_topic] = _value\n",
    "    \n",
    "    return topics\n",
    "\n",
    "all_topics = []\n",
    "cat_names = []\n",
    "for i in tqdm(range(0, len(cat_list))):\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_docs = [preprocess(text) for text in f.readlines()]\n",
    "        cat_corpus = [review_dictionary.doc2bow(doc) for doc in cat_docs]\n",
    "        cat_topics = review_model[cat_corpus]\n",
    "        all_topics.append(combine_topics(cat_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_individual_sim = corpus_similarity([[(k, topic[k]) for k in topic] for topic in all_topics], len(review_dictionary))\n",
    "\n",
    "lda_individual_sim_df = pd.DataFrame(lda_individual_sim)\n",
    "lda_individual_sim_df.index = cat_names\n",
    "lda_individual_sim_df.columns = cat_names\n",
    "lda_individual_data = top_n(lda_individual_sim_df, 50)\n",
    "\n",
    "with open(\"display/lda_ind_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_individual_data, np.ones(lda_individual_data.shape[0]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

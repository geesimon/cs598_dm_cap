{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "cat_list = glob.glob (\"cuisines/*\")\n",
    "cat_size = len(cat_list)\n",
    "\n",
    "random.seed(0)\n",
    "cat_names = []\n",
    "cat_text = []\n",
    "# sample_size = min(30, cat_size)\n",
    "# cat_sample = sorted(random.sample(range(cat_size), sample_size))\n",
    "cat_sample = range(0, cat_size)\n",
    "\n",
    "count = 0\n",
    "for i in cat_sample:\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_text.append(f.read().replace(\"\\n\", \"\").replace(\"\\r\",\"\"))\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(cat_text)]\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(dictionary)))\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    b = dict(b)\n",
    "    norm_a = 0\n",
    "    norm_b = 0\n",
    "    denom = 0\n",
    "    for a_i, a_v in a:\n",
    "        norm_a += a_v * a_v\n",
    "        if a_i in b:\n",
    "            denom += a_v * b[a_i]\n",
    "    for b_i in b:\n",
    "        norm_b += b[b_i] * b[b_i]\n",
    "    \n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "#     print(norm_a)\n",
    "#     print(norm_b)\n",
    "#     print(denom)\n",
    "    \n",
    "    return denom / (norm_a * norm_b)\n",
    "\n",
    "def top_n(df, n, thresh_hold = 0.1):\n",
    "    df_count = np.zeros(df.shape)\n",
    "    df_bak = df\n",
    "    df_count[df >= thresh_hold] = 1\n",
    "    _counts = np.sum(df_count, axis=1)\n",
    "    max_index = []\n",
    "    for i in range(0, n):\n",
    "        _index = np.argmax(_counts)\n",
    "        max_index.append(_index)\n",
    "        _counts[_index] = -1\n",
    "    \n",
    "    return df.iloc[max_index][df.columns[max_index]]\n",
    "\n",
    "def format_obj(df, groups):\n",
    "    _nodes = \"nodes\"\n",
    "    _links = \"links\"\n",
    "    json_obj = {_nodes:[], _links:[]}\n",
    "    for i in range(0, len(df.columns)):\n",
    "        json_obj[_nodes].append({\"name\":df.columns[i], \"group\":groups[i]})\n",
    "    \n",
    "    for i in range(0, df.shape[0] - 1):\n",
    "        for j in range(i + 1, df.shape[0]):\n",
    "            json_obj[_links].append({\"source\":i, \"target\":j, \"value\":df.iloc[i][j]})\n",
    "    \n",
    "    return json_obj\n",
    "\n",
    "def corpus_similarity(corpus):\n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        _sim[i][i] = 1\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return _sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "# for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#     sim[i][i] = 1\n",
    "#     for j in range(i + 1, len(corpus)):\n",
    "#         sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#         sim[j][i] = sim[i][j]\n",
    "        \n",
    "sim = corpus_similarity(corpus)\n",
    "\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.index = cat_names\n",
    "sim_df.columns = cat_names\n",
    "data = top_n(sim_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(data, np.ones(data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = 20\n",
    "ax = sns.heatmap(data.iloc[0:sample][data.columns[0:sample]],cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "model = TfidfModel(corpus)\n",
    "tfidf_corpus = model[corpus]\n",
    "\n",
    "tfidf_sim = np.zeros([len(tfidf_corpus), len(tfidf_corpus)])\n",
    "\n",
    "for i in tqdm(range(0, len(tfidf_corpus) - 1)):\n",
    "    tfidf_sim[i][i] = 1\n",
    "    for j in range(i + 1, len(tfidf_corpus)):\n",
    "        tfidf_sim[i][j] = cosine_similarity(tfidf_corpus[i], tfidf_corpus[j])\n",
    "        tfidf_sim[j][i] = tfidf_sim[i][j]\n",
    "        \n",
    "\n",
    "tfidf_sim_df = pd.DataFrame(tfidf_sim)\n",
    "tfidf_sim_df.index = cat_names\n",
    "tfidf_sim_df.columns = cat_names\n",
    "tfidf_data = top_n(tfidf_sim_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/tfidf_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_data, np.ones(tfidf_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "lda_model = models.LdaModel(tfidf_corpus, num_topics = 100, id2word=dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "doc_topics = lda_model.get_document_topics(tfidf_corpus)\n",
    "print(\"done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_sim = corpus_similarity(doc_topics)\n",
    "\n",
    "lda_sim_df = pd.DataFrame(lda_sim)\n",
    "lda_sim_df.index = cat_names\n",
    "lda_sim_df.columns = cat_names\n",
    "lda_data = top_n(lda_sim_df, 50)\n",
    "\n",
    "with open(\"display/lda_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_data, np.ones(lda_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

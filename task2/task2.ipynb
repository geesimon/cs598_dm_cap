{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [25:27<00:00,  8.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before prunn:193061\n",
      "After prunn:100000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "cat_list = glob.glob (\"cuisines/*\")\n",
    "cat_size = len(cat_list)\n",
    "\n",
    "random.seed(0)\n",
    "cat_names = []\n",
    "cat_text = []\n",
    "# sample_size = min(30, cat_size)\n",
    "# cat_sample = sorted(random.sample(range(cat_size), sample_size))\n",
    "cat_sample = range(0, cat_size)\n",
    "\n",
    "count = 0\n",
    "for i in cat_sample:\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_text.append(f.read().replace(\"\\n\", \"\").replace(\"\\r\",\"\"))\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(cat_text)]\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(dictionary)))\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    b = dict(b)\n",
    "    norm_a = 0\n",
    "    norm_b = 0\n",
    "    denom = 0\n",
    "    for a_i, a_v in a:\n",
    "        norm_a += a_v * a_v\n",
    "        if a_i in b:\n",
    "            denom += a_v * b[a_i]\n",
    "    for b_i in b:\n",
    "        norm_b += b[b_i] * b[b_i]\n",
    "    \n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "#     print(norm_a)\n",
    "#     print(norm_b)\n",
    "#     print(denom)\n",
    "    \n",
    "    return denom / (norm_a * norm_b)\n",
    "\n",
    "def top_n(df, n, thresh_hold = 0.1):\n",
    "    df_count = np.zeros(df.shape)\n",
    "    df_bak = df\n",
    "    df_count[df >= thresh_hold] = 1\n",
    "    _counts = np.sum(df_count, axis=1)\n",
    "    max_index = []\n",
    "    for i in range(0, n):\n",
    "        _index = np.argmax(_counts)\n",
    "        max_index.append(_index)\n",
    "        _counts[_index] = -1\n",
    "    \n",
    "    return df.iloc[max_index][df.columns[max_index]]\n",
    "\n",
    "def format_obj(df, groups):\n",
    "    _nodes = \"nodes\"\n",
    "    _links = \"links\"\n",
    "    json_obj = {_nodes:[], _links:[]}\n",
    "    for i in range(0, len(df.columns)):\n",
    "        json_obj[_nodes].append({\"name\":df.columns[i], \"group\":groups[i]})\n",
    "    \n",
    "    for i in range(0, df.shape[0] - 1):\n",
    "        for j in range(i + 1, df.shape[0]):\n",
    "            json_obj[_links].append({\"source\":i, \"target\":j, \"value\":df.iloc[i][j]})\n",
    "    \n",
    "    return json_obj\n",
    "\n",
    "def corpus_similarity(corpus):\n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        _sim[i][i] = 1\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return _sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 149/149 [00:27<00:00,  5.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "# for i in tqdm(range(0, len(corpus) - 1)):\n",
    "#     sim[i][i] = 1\n",
    "#     for j in range(i + 1, len(corpus)):\n",
    "#         sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "#         sim[j][i] = sim[i][j]\n",
    "        \n",
    "sim = corpus_similarity(corpus)\n",
    "\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.index = cat_names\n",
    "sim_df.columns = cat_names\n",
    "data = top_n(sim_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(data, np.ones(data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns; \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = 20\n",
    "ax = sns.heatmap(data.iloc[0:sample][data.columns[0:sample]],cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 149/149 [11:20<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "model = TfidfModel(corpus)\n",
    "tfidf_corpus = model[corpus]\n",
    "\n",
    "tfidf_sim = np.zeros([len(tfidf_corpus), len(tfidf_corpus)])\n",
    "\n",
    "for i in tqdm(range(0, len(tfidf_corpus) - 1)):\n",
    "    tfidf_sim[i][i] = 1\n",
    "    for j in range(i + 1, len(tfidf_corpus)):\n",
    "        tfidf_sim[i][j] = cosine_similarity(tfidf_corpus[i], tfidf_corpus[j])\n",
    "        tfidf_sim[j][i] = tfidf_sim[i][j]\n",
    "        \n",
    "\n",
    "tfidf_sim_df = pd.DataFrame(tfidf_sim)\n",
    "tfidf_sim_df.index = cat_names\n",
    "tfidf_sim_df.columns = cat_names\n",
    "tfidf_data = top_n(tfidf_sim_df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"display/tfidf_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_data, np.ones(tfidf_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\gensim\\models\\ldamodel.py:582: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 21.213967s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "lda_model = models.LdaModel(tfidf_corpus, num_topics = 100, id2word=dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "doc_topics = lda_model.get_document_topics(tfidf_corpus)\n",
    "print(\"done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 149/149 [15:10<00:00,  6.11s/it]\n"
     ]
    }
   ],
   "source": [
    "lda_sim = corpus_similarity(doc_topics)\n",
    "\n",
    "lda_sim_df = pd.DataFrame(lda_sim)\n",
    "lda_sim_df.index = cat_names\n",
    "lda_sim_df.columns = cat_names\n",
    "lda_data = top_n(lda_sim_df, 50)\n",
    "\n",
    "with open(\"display/lda_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_data, np.ones(lda_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(29,\n",
       "  '0.000*\"prompf\" + 0.000*\"prooo\" + 0.000*\"prohibitivli\" + 0.000*\"prokkimchi\" + 0.000*\"promiscu\" + 0.000*\"professori\" + 0.000*\"proviso\" + 0.000*\"proteinsi\" + 0.000*\"probobl\" + 0.000*\"prosinexpens\"'),\n",
       " (32,\n",
       "  '0.000*\"prompf\" + 0.000*\"prooo\" + 0.000*\"prohibitivli\" + 0.000*\"prokkimchi\" + 0.000*\"promiscu\" + 0.000*\"professori\" + 0.000*\"proviso\" + 0.000*\"proteinsi\" + 0.000*\"probobl\" + 0.000*\"prosinexpens\"'),\n",
       " (55,\n",
       "  '0.000*\"bellagio\" + 0.000*\"kobe\" + 0.000*\"mastro\" + 0.000*\"wellington\" + 0.000*\"fogo\" + 0.000*\"wagyu\" + 0.000*\"brazilian\" + 0.000*\"delmonico\" + 0.000*\"porterhous\" + 0.000*\"outback\"'),\n",
       " (87,\n",
       "  '0.000*\"rula\" + 0.000*\"casey\" + 0.000*\"ireland\" + 0.000*\"boxti\" + 0.000*\"guin\" + 0.000*\"irishmen\" + 0.000*\"shepherd\" + 0.000*\"smithwick\" + 0.000*\"mcmullan\" + 0.000*\"fibber\"'),\n",
       " (8,\n",
       "  '0.001*\"firefli\" + 0.000*\"paella\" + 0.000*\"julian\" + 0.000*\"serrano\" + 0.000*\"manchego\" + 0.000*\"brava\" + 0.000*\"cevich\" + 0.000*\"croqueta\" + 0.000*\"empanada\" + 0.000*\"patata\"'),\n",
       " (18,\n",
       "  '0.000*\"havana\" + 0.000*\"ropa\" + 0.000*\"lechon\" + 0.000*\"vieja\" + 0.000*\"halo\" + 0.000*\"toston\" + 0.000*\"plantain\" + 0.000*\"benedict\" + 0.000*\"maduro\" + 0.000*\"mimita\"'),\n",
       " (81,\n",
       "  '0.001*\"bosa\" + 0.000*\"boba\" + 0.000*\"saigon\" + 0.000*\"banh\" + 0.000*\"baha\" + 0.000*\"firefli\" + 0.000*\"babystack\" + 0.000*\"vermicelli\" + 0.000*\"fondu\" + 0.000*\"benedict\"'),\n",
       " (53,\n",
       "  '0.000*\"snoh\" + 0.000*\"zipp\" + 0.000*\"hobe\" + 0.000*\"stromboli\" + 0.000*\"hooter\" + 0.000*\"hashbrown\" + 0.000*\"wazuzu\" + 0.000*\"tamal\" + 0.000*\"monti\" + 0.000*\"benedict\"'),\n",
       " (43,\n",
       "  '0.001*\"lingonberri\" + 0.000*\"swedish\" + 0.000*\"huli\" + 0.000*\"firehous\" + 0.000*\"joynt\" + 0.000*\"pepin\" + 0.000*\"katsu\" + 0.000*\"mamma\" + 0.000*\"sonic\" + 0.000*\"tonkatsu\"'),\n",
       " (36,\n",
       "  '0.000*\"flay\" + 0.000*\"bobbi\" + 0.000*\"hanni\" + 0.000*\"peppermil\" + 0.000*\"tamal\" + 0.000*\"lucill\" + 0.000*\"cornbread\" + 0.000*\"lolo\" + 0.000*\"culver\" + 0.000*\"kisra\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(-1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5 coherence=-1.498333\n",
      "k=7 coherence=-2.210039\n",
      "k=9 coherence=-1.753167\n",
      "k=11 coherence=-1.878003\n",
      "k=13 coherence=-2.161507\n",
      "k=15 coherence=-2.158024\n",
      "k=17 coherence=-2.020575\n",
      "k=19 coherence=-1.971740\n",
      "k=21 coherence=-1.714946\n",
      "k=23 coherence=-2.001825\n",
      "k=25 coherence=-1.957821\n",
      "k=27 coherence=-1.612905\n",
      "k=29 coherence=-1.732868\n",
      "k=31 coherence=-1.810027\n",
      "k=33 coherence=-1.767502\n",
      "k=35 coherence=-1.652618\n",
      "k=37 coherence=-1.887585\n",
      "k=39 coherence=-1.662607\n",
      "k=41 coherence=-1.734076\n",
      "k=43 coherence=-1.682676\n",
      "k=45 coherence=-1.786588\n",
      "k=47 coherence=-1.781682\n",
      "k=49 coherence=-1.677419\n"
     ]
    }
   ],
   "source": [
    "largest_coherence = -1e20\n",
    "best_k = 0\n",
    "best_model = None\n",
    "for k in range(5, 150, 2):\n",
    "    model = models.LdaModel(tfidf_corpus, num_topics = k, id2word=dictionary)\n",
    "    cm = models.coherencemodel.CoherenceModel(model=model, corpus=tfidf_corpus, coherence='u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "    print(\"k=%d coherence=%f\"%(k, coherence))\n",
    "    if (coherence > largest_coherence):\n",
    "        largest_coherence = coherence\n",
    "        best_model = model\n",
    "        best_k = k\n",
    "\n",
    "print(\"best_k:%d\"%(best_k))\n",
    "for idx, topic in best_model.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

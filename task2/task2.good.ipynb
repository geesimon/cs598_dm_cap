{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling with Gensim\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 147/147 [25:12<00:00,  7.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before prunn:193047\n",
      "After prunn:100000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "cat_list = sorted(glob.glob (\"cuisines/*\"))\n",
    "cat_size = len(cat_list)\n",
    "\n",
    "random.seed(0)\n",
    "cat_names = []\n",
    "cat_text = []\n",
    "# sample_size = min(30, cat_size)\n",
    "# cat_sample = sorted(random.sample(range(cat_size), sample_size))\n",
    "cat_sample = range(0, cat_size)\n",
    "\n",
    "count = 0\n",
    "for i in cat_sample:\n",
    "    cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "    with open(cat_list[i]) as f:\n",
    "        cat_text.append(f.read().replace(\"\\n\", \"\").replace(\"\\r\",\"\"))\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(cat_text)]\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(\"Before prunn:%d\"%(len(dictionary)))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5)\n",
    "print(\"After prunn:%d\"%(len(dictionary)))\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sklearn\n",
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    b = dict(b)\n",
    "    norm_a = 0\n",
    "    norm_b = 0\n",
    "    denom = 0\n",
    "    for a_i, a_v in a:\n",
    "        norm_a += a_v * a_v\n",
    "        if a_i in b:\n",
    "            denom += a_v * b[a_i]\n",
    "    for b_i in b:\n",
    "        norm_b += b[b_i] * b[b_i]\n",
    "    \n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "    \n",
    "    return denom / (norm_a * norm_b)\n",
    "\n",
    "def top_n(df, n, thresh_hold = 0.1):\n",
    "    df_count = np.zeros(df.shape)\n",
    "    df_bak = df\n",
    "    df_count[df >= thresh_hold] = 1\n",
    "    _counts = np.sum(df_count, axis=1)\n",
    "    max_index = []\n",
    "    for i in range(0, n):\n",
    "        _index = np.argmax(_counts)\n",
    "        max_index.append(_index)\n",
    "        _counts[_index] = -1\n",
    "    \n",
    "    return df.iloc[max_index][df.columns[max_index]]\n",
    "\n",
    "def slice_df_by_name(df,names):\n",
    "    return df.loc[names][names]\n",
    "\n",
    "def format_obj(df, groups):\n",
    "    _nodes = \"nodes\"\n",
    "    _links = \"links\"\n",
    "    json_obj = {_nodes:[], _links:[]}\n",
    "    sorted_names = []\n",
    "    name2gid = dict()\n",
    "    for g in range(0, len(groups)):\n",
    "        for name in groups[g]:\n",
    "            name2gid[name] = g\n",
    "            if name in df.columns:\n",
    "                sorted_names.append(name)\n",
    "    \n",
    "    df = slice_df_by_name(df, sorted_names)\n",
    "    for c_name in df.columns:\n",
    "        json_obj[_nodes].append({\"name\": c_name, \"group\":name2gid[c_name]})\n",
    "    \n",
    "    for i in range(0, df.shape[0] - 1):\n",
    "        for j in range(i + 1, df.shape[0]):\n",
    "            json_obj[_links].append({\"source\":i, \"target\":j, \"value\":float(df.iloc[i][j])})\n",
    "    \n",
    "    return json_obj\n",
    "\n",
    "def corpus2matrix(corpus, vector_dimension):\n",
    "    _corpus_matrix = np.zeros([len(corpus), vector_dimension])\n",
    "    for i, row in enumerate(corpus):\n",
    "        for j, v in row:\n",
    "            _corpus_matrix[i][j] = v\n",
    "    \n",
    "    return _corpus_matrix\n",
    "    \n",
    "def corpus_similarity(corpus, vector_dimension, distance_func = sklearn.metrics.pairwise.cosine_similarity):\n",
    "    _corpus_matrix = corpus2matrix(corpus, vector_dimension)\n",
    "    #Normailzation\n",
    "#     _corpus_matrix = Normalizer().transform(_corpus_matrix)    \n",
    "    return distance_func(_corpus_matrix)\n",
    "\n",
    "\n",
    "def corpus_similarity_1(corpus):\n",
    "    _sim = np.zeros([len(corpus), len(corpus)])\n",
    "\n",
    "    for i in tqdm(range(0, len(corpus) - 1)):\n",
    "        _sim[i][i] = 1\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            _sim[i][j] = cosine_similarity(corpus[i], corpus[j])\n",
    "            _sim[j][i] = _sim[i][j]\n",
    "    \n",
    "    return _sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, Birch, DBSCAN\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "def kmean_predict(X, n_clusters):\n",
    "    return KMeans(n_clusters = n_clusters).fit_predict(X)\n",
    "\n",
    "def birch_predict(X, n_clusters):\n",
    "    return Birch(compute_labels = True, n_clusters = n_clusters).fit_predict(X)\n",
    "\n",
    "def dbscan_predict(X, n_clusters):\n",
    "    return DBSCAN(eps=n_clusters * 0.01, min_samples=1).fit_predict(X)\n",
    "\n",
    "cluster_method = {\"kmean\": kmean_predict,\n",
    "                 \"birch\": birch_predict,\n",
    "                 \"dbscan\": dbscan_predict}\n",
    "\n",
    "def get_cluster(features_list, feature_dimension, names, num_cluster = -1, method = \"kmean\", verbose = False):\n",
    "    if type(d2v) == np.ndarray:\n",
    "        X = features_list\n",
    "    else:\n",
    "        X = corpus2matrix(features_list, feature_dimension)\n",
    "    \n",
    "    Norm_X = Normalizer().transform(X)\n",
    "    \n",
    "    if num_cluster < 0:\n",
    "        best_score = -1\n",
    "        best_k = -1\n",
    "        for k in range(2, 100):\n",
    "            y_pred = cluster_method[method](Norm_X, k)\n",
    "            _score = metrics.silhouette_score(Norm_X, y_pred, metric='euclidean')\n",
    "#             _score = metrics.calinski_harabasz_score(Norm_X, y_pred) \n",
    "            if verbose:\n",
    "                print(_score)\n",
    "            if _score > best_score:\n",
    "                best_k = k\n",
    "                best_score = _score\n",
    "        if verbose:\n",
    "            print(\"Best k:%d\"%(best_k))\n",
    "    else:\n",
    "        best_k = num_cluster\n",
    "        \n",
    "    y_pred = cluster_method[method](Norm_X, best_k)\n",
    "    clusters = dict()\n",
    "    name2cluster = dict()\n",
    "    for i in range(0, len(y_pred)):\n",
    "        name2cluster[names[i]] = y_pred[i]\n",
    "        if y_pred[i] in clusters:\n",
    "            clusters[y_pred[i]].append(names[i])\n",
    "        else:\n",
    "            clusters[y_pred[i]] = [names[i]]\n",
    "\n",
    "    return (clusters, name2cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "        \n",
    "sim = corpus_similarity(corpus, len(dictionary))\n",
    "sim_clusters, i = get_cluster(corpus, len(dictionary), cat_names, 10, method='birch')\n",
    "\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.index = cat_names\n",
    "sim_df.columns = cat_names\n",
    "\n",
    "sim_df_50 = top_n(sim_df, 50)\n",
    "selected_names = sim_df_50.columns\n",
    "\n",
    "with open(\"display/output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(sim_df, sim_clusters)))\n",
    "\n",
    "with open(\"display/output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(sim_df_50, sim_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns; \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sample = 20\n",
    "# ax = sns.heatmap(data.iloc[0:sample][data.columns[0:sample]],cmap=\"YlGnBu\", xticklabels=True, yticklabels=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "import json\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "tfidf_sim = corpus_similarity(tfidf_corpus, len(dictionary))\n",
    "tfidf_sim_clusters, i = get_cluster(tfidf_corpus, len(dictionary), cat_names, 10, method='birch')\n",
    "\n",
    "tfidf_sim_df = pd.DataFrame(tfidf_sim)\n",
    "tfidf_sim_df.index = cat_names\n",
    "tfidf_sim_df.columns = cat_names\n",
    "#tfidf_sim_df_50 = top_n(tfidf_sim_df, 50)\n",
    "tfidf_sim_df_50 = slice_df_by_name(tfidf_sim_df, selected_names)\n",
    "                              \n",
    "with open(\"display/tfidf_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_sim_df, tfidf_sim_clusters)))\n",
    "with open(\"display/tfidf_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(tfidf_sim_df_50, tfidf_sim_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_py3\\lib\\site-packages\\gensim\\models\\ldamodel.py:582: RuntimeWarning: overflow encountered in exp2\n",
      "  perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done in 20.272974s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "num_topics = 100\n",
    "\n",
    "t0 = time()\n",
    "lda_model = models.LdaModel(tfidf_corpus, \n",
    "                            num_topics = num_topics, \n",
    "                            id2word = dictionary,\n",
    "                            random_state = 100,\n",
    "                            eval_every=5, \n",
    "                            alpha='auto', \n",
    "                            gamma_threshold=0.01)\n",
    "# lda_model = models.LdaModel(tfidf_corpus, \n",
    "#                             num_topics = num_topics, \n",
    "#                             id2word = dictionary,\n",
    "#                             random_state = 100,\n",
    "#                             update_every = 1,\n",
    "#                             chunksize = 100,\n",
    "#                             passes = 10,\n",
    "#                             alpha = 'auto')\n",
    "\n",
    "doc_topics = lda_model[tfidf_corpus]\n",
    "print(\"Training done in %fs\" % (time() - t0))\n",
    "\n",
    "# t0 = time()\n",
    "# # Compute Perplexity\n",
    "# print('\\nPerplexity: ', lda_model.log_perplexity(tfidf_corpus))  # a measure of how good the model is. lower the better.\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model = lda_model, texts = processed_docs, dictionary = dictionary, coherence = 'c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_lda)\n",
    "# print(\"Evaluation done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done in 1650.143629s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "mallet_path = \"..\" + os.sep + \"mallet-2.0.8\"+ os.sep + \"bin\" + os.sep +\"mallet\"\n",
    "t0 = time()\n",
    "lda_mallet_model = models.wrappers.LdaMallet(mallet_path, \n",
    "                                             corpus = corpus, \n",
    "                                             num_topics = num_topics, \n",
    "                                             id2word = dictionary)\n",
    "mallet_doc_topics = lda_mallet_model[corpus]\n",
    "print(\"Training done in %fs\" % (time() - t0))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# t0 = time()\n",
    "# coherence_model_ldamallet = CoherenceModel(model = lda_mallet_model, texts = processed_docs, dictionary = dictionary, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "# print(\"Evaluation done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alda_sim = corpus_similarity(doc_topics, num_topics)\n",
    "lda_sim_clusters, i = get_cluster(doc_topics, num_topics, cat_names, 10, method='birch')\n",
    "\n",
    "lda_sim_df = pd.DataFrame(lda_sim)\n",
    "lda_sim_df.index = cat_names\n",
    "lda_sim_df.columns = cat_names\n",
    "# lda_sim_df_50 = top_n(lda_sim_df, 50)\n",
    "lda_sim_df_50 = slice_df_by_name(lda_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_sim_df, lda_sim_clusters)))\n",
    "with open(\"display/lda_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_sim_df_50, lda_sim_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mallet_sim = corpus_similarity(mallet_doc_topics, num_topics)\n",
    "lda_mallet_sim_clusters, i = get_cluster(mallet_doc_topics, num_topics, cat_names, 10, method='birch')\n",
    "\n",
    "lda_mallet_sim_df = pd.DataFrame(lda_mallet_sim)\n",
    "lda_mallet_sim_df.index = cat_names\n",
    "lda_mallet_sim_df.columns = cat_names\n",
    "# lda_mallet_sim_df_50 = top_n(lda_sim_df, 50)\n",
    "lda_mallet_sim_df_50 = slice_df_by_name(lda_mallet_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/lda_mallet_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_mallet_sim_df, lda_mallet_sim_clusters)))\n",
    "with open(\"display/lda_mallet_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(lda_mallet_sim_df_50, lda_mallet_sim_clusters)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, tfidf_corpus, dictionary)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest_coherence = -1e20\n",
    "# best_k = 0\n",
    "# best_model = None\n",
    "# for k in range(5, 150, 2):\n",
    "#     model = models.LdaModel(tfidf_corpus, num_topics = k, id2word=dictionary)\n",
    "#     cm = models.coherencemodel.CoherenceModel(model=model, corpus=tfidf_corpus, coherence='u_mass')\n",
    "#     coherence = cm.get_coherence()\n",
    "#     print(\"k=%d coherence=%f\"%(k, coherence))\n",
    "#     if (coherence > largest_coherence):\n",
    "#         largest_coherence = coherence\n",
    "#         best_model = model\n",
    "#         best_k = k\n",
    "\n",
    "# print(\"best_k:%d\"%(best_k))\n",
    "# for idx, topic in best_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names_file = \"cuisine_indices.txt\"\n",
    "# matrix_file = \"cuisine_sim_matrix.csv\"\n",
    "\n",
    "# with open (names_file, 'r') as f:\n",
    "#     names = f.read().split(\"\\n\")\n",
    "\n",
    "# demo_data = pd.read_csv(matrix_file, header=None)\n",
    "# demo_data.index = names\n",
    "# demo_data.columns = names\n",
    "\n",
    "# with open(\"display/demo_output.json\", \"w\") as f:\n",
    "#     f.write(json.dumps(format_obj(demo_data, np.ones(demo_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path2reviewdump = \"reviews/reviews.dat\"\n",
    "\n",
    "# with open(path2reviewdump, \"r\") as f:\n",
    "#     reviews = f.readlines()\n",
    "# review_docs = [preprocess(text) for text in tqdm(reviews)]\n",
    "# review_dictionary = corpora.Dictionary(review_docs)\n",
    "# print(\"Before prunn:%d\"%(len(review_dictionary)))\n",
    "# review_dictionary.filter_extremes(no_below=15, no_above = 0.5)\n",
    "# print(\"After prunn:%d\"%(len(review_dictionary)))\n",
    "# review_corpus = [review_dictionary.doc2bow(doc) for doc in review_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import time\n",
    "\n",
    "# t0 = time()\n",
    "# review_model = models.LdaModel(review_corpus, num_topics=100, id2word=review_dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "# print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "# for idx, topic in review_model.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_topics(cat_topics):\n",
    "#     topics = {}\n",
    "#     for _sub_topics in cat_topics:\n",
    "#         for _topic, _value in _sub_topics:\n",
    "#             if _topic in topics:\n",
    "#                 topics[_topic] += _value\n",
    "#             else:\n",
    "#                 topics[_topic] = _value\n",
    "    \n",
    "#     return topics\n",
    "\n",
    "# all_topics = []\n",
    "# cat_names = []\n",
    "# for i in tqdm(range(0, len(cat_list))):\n",
    "#     cat_names.append(cat_list[i].replace(\"\\\\\", \"/\").split('/')[-1][:-4].replace(\"_\",\" \"))\n",
    "#     with open(cat_list[i]) as f:\n",
    "#         cat_docs = [preprocess(text) for text in f.readlines()]\n",
    "#         cat_corpus = [review_dictionary.doc2bow(doc) for doc in cat_docs]\n",
    "#         cat_topics = review_model[cat_corpus]\n",
    "#         all_topics.append(combine_topics(cat_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lda_individual_sim = corpus_similarity([[(k, topic[k]) for k in topic] for topic in all_topics], len(review_dictionary))\n",
    "\n",
    "# lda_individual_sim_df = pd.DataFrame(lda_individual_sim)\n",
    "# lda_individual_sim_df.index = cat_names\n",
    "# lda_individual_sim_df.columns = cat_names\n",
    "# lda_individual_data = top_n(lda_individual_sim_df, 50)\n",
    "\n",
    "# with open(\"display/lda_ind_output.json\", \"w\") as f:\n",
    "#     f.write(json.dumps(format_obj(lda_individual_data, np.ones(lda_individual_data.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_clusters(cluster1, cluster2, title):\n",
    "    children_name = 'children'\n",
    "    name_name = 'name'\n",
    "    value_name = 'value'\n",
    "    color_name = 'color'    \n",
    "\n",
    "    _out = {name_name: title, children_name:[]}\n",
    "\n",
    "#     name2cluster1 = dict()\n",
    "#     for _group_id in range(0, len(cluster1)):\n",
    "#         for _name in cluster1[_group_id]:\n",
    "#             name2cluster1[_name]= _group_id\n",
    "\n",
    "    name2cluster2 = dict()            \n",
    "    for _group_id in range(0, len(cluster2)):\n",
    "        for _name in cluster2[_group_id]:\n",
    "            name2cluster2[_name]= _group_id\n",
    "                \n",
    "    for _group_id in range(0, len(cluster1)):\n",
    "        _out[children_name].append({name_name: str(_group_id), children_name:[]})\n",
    "        for _name_id in range(0, len(cluster1[_group_id])):\n",
    "            _out[children_name][_group_id][children_name].append({name_name: cluster1[_group_id][_name_id],\n",
    "                                                                  \"cluster1\": _group_id,\n",
    "                                                                  \"cluster2\": name2cluster2[cluster1[_group_id][_name_id]]})\n",
    "        \n",
    "    return _out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "num_cluster = 10\n",
    "sim_clusters_kmean_small, i = get_cluster(mallet_doc_topics, num_topics, cat_names, num_cluster)\n",
    "sim_clusters_kmean, i = get_cluster(mallet_doc_topics, num_topics, cat_names)\n",
    "with open(\"display/cluster_kmean_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dump_clusters(sim_clusters_kmean, sim_clusters_kmean_small, \"Cuisine Clustering by KMean\")))\n",
    "print(\"Clustering done in %fs\" % (time() - t0))\n",
    "  \n",
    "t0 = time()\n",
    "sim_clusters_birch_small, i = get_cluster(mallet_doc_topics, num_topics, cat_names, num_cluster, method='birch')\n",
    "sim_clusters_birch, i = get_cluster(mallet_doc_topics, num_topics, cat_names, method='birch')\n",
    "with open(\"display/cluster_birch_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dump_clusters(sim_clusters_birch, sim_clusters_birch_small, \"Cuisine Clustering by Birch\")))\n",
    "print(\"Clustering done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "vector_len = 200\n",
    "d2v_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_docs)]\n",
    "d2v_model = Doc2Vec(d2v_docs, vector_size=vector_len, workers=4)\n",
    "\n",
    "d2v = np.array([d2v_model.docvecs[i] for i in range(0, len(d2v_model.docvecs))])\n",
    "\n",
    "d2v_sim = sklearn.metrics.pairwise.cosine_similarity(d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_sim_clusters, i = get_cluster(d2v, vector_len, cat_names, 10, method='birch')\n",
    "\n",
    "d2v_sim_df = pd.DataFrame(d2v_sim)\n",
    "d2v_sim_df.index = cat_names\n",
    "d2v_sim_df.columns = cat_names\n",
    "# lda_mallet_sim_df_50 = top_n(lda_sim_df, 50)\n",
    "d2v_sim_df_50 = slice_df_by_name(d2v_sim_df, selected_names)\n",
    "\n",
    "with open(\"display/d2v_output.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(d2v_sim_df, d2v_sim_clusters)))\n",
    "with open(\"display/d2v_output_50.json\", \"w\") as f:\n",
    "    f.write(json.dumps(format_obj(d2v_sim_df_50, d2v_sim_clusters)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1626406\n",
      "0.19083329\n",
      "0.18491527\n",
      "0.17983446\n",
      "0.17873819\n",
      "0.17034803\n",
      "0.17219\n",
      "0.16337727\n",
      "0.14083004\n",
      "0.11862921\n",
      "0.13388829\n",
      "0.12908001\n",
      "0.13525268\n",
      "0.13806993\n",
      "0.12947792\n",
      "0.13661696\n",
      "0.14072403\n",
      "0.13844796\n",
      "0.13813818\n",
      "0.13141593\n",
      "0.1220346\n",
      "0.11984437\n",
      "0.1295713\n",
      "0.1295371\n",
      "0.12695064\n",
      "0.13532527\n",
      "0.1334181\n",
      "0.12471841\n",
      "0.12492941\n",
      "0.13343571\n",
      "0.15290906\n",
      "0.13799869\n",
      "0.13610584\n",
      "0.13701025\n",
      "0.1277633\n",
      "0.12849717\n",
      "0.13896547\n",
      "0.12844686\n",
      "0.14122948\n",
      "0.13422623\n",
      "0.13505155\n",
      "0.14346586\n",
      "0.14419034\n",
      "0.13484779\n",
      "0.13644804\n",
      "0.14638242\n",
      "0.1374259\n",
      "0.13539498\n",
      "0.1508247\n",
      "0.13588268\n",
      "0.13589351\n",
      "0.1498769\n",
      "0.13319418\n",
      "0.13402417\n",
      "0.14225596\n",
      "0.14619489\n",
      "0.14711191\n",
      "0.1251058\n",
      "0.12668662\n",
      "0.1437225\n",
      "0.13284649\n",
      "0.14569567\n",
      "0.14724608\n",
      "0.13944815\n",
      "0.14370216\n",
      "0.1277525\n",
      "0.1501313\n",
      "0.13705862\n",
      "0.12619251\n",
      "0.13964447\n",
      "0.1326814\n",
      "0.13356207\n",
      "0.13546489\n",
      "0.1312189\n",
      "0.13347234\n",
      "0.12927599\n",
      "0.12460005\n",
      "0.12790564\n",
      "0.13476914\n",
      "0.12282319\n",
      "0.13192022\n",
      "0.12587492\n",
      "0.12161904\n",
      "0.12882684\n",
      "0.13257973\n",
      "0.12428298\n",
      "0.1317847\n",
      "0.12744819\n",
      "0.1228672\n",
      "0.123336144\n",
      "0.11806366\n",
      "0.121874884\n",
      "0.113776684\n",
      "0.11608874\n",
      "0.11217884\n",
      "0.11796728\n",
      "0.11350173\n",
      "0.112284474\n",
      "Best k:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({1: ['Afghan',\n",
       "   'Arabian',\n",
       "   'Bagels',\n",
       "   'Bakeries',\n",
       "   'Barbeque',\n",
       "   'Beer, Wine & Spirits',\n",
       "   'Brasseries',\n",
       "   'Breakfast & Brunch',\n",
       "   'Bubble Tea',\n",
       "   'Burgers',\n",
       "   'Cafeteria',\n",
       "   'Caterers',\n",
       "   'Cheese Shops',\n",
       "   'Cheesesteaks',\n",
       "   'Coffee & Tea',\n",
       "   'Comfort Food',\n",
       "   'Creperies',\n",
       "   'Delis',\n",
       "   'Desserts',\n",
       "   'Diners',\n",
       "   'Do-It-Yourself Food',\n",
       "   'Donuts',\n",
       "   'Drugstores',\n",
       "   'Ethnic Food',\n",
       "   'Fast Food',\n",
       "   'Food Stands',\n",
       "   'Food Trucks',\n",
       "   'Fruits & Veggies',\n",
       "   'Gelato',\n",
       "   'Greek',\n",
       "   'Halal',\n",
       "   'Health Markets',\n",
       "   'Hot Dogs',\n",
       "   'Ice Cream & Frozen Yogurt',\n",
       "   'Internet Cafes',\n",
       "   'Juice Bars & Smoothies',\n",
       "   'Kosher',\n",
       "   'Lebanese',\n",
       "   'Meat Shops',\n",
       "   'Mediterranean',\n",
       "   'Middle Eastern',\n",
       "   'Persian-Iranian',\n",
       "   'Pizza',\n",
       "   'Polish',\n",
       "   'Pretzels',\n",
       "   'Russian',\n",
       "   'Salvadoran',\n",
       "   'Sandwiches',\n",
       "   'Scandinavian',\n",
       "   'Shaved Ice',\n",
       "   'Soup',\n",
       "   'Southern',\n",
       "   'Specialty Food',\n",
       "   'Street Vendors',\n",
       "   'Szechuan',\n",
       "   'Tea Rooms',\n",
       "   'Turkish',\n",
       "   'Vegetarian',\n",
       "   'Venezuelan'],\n",
       "  0: ['African',\n",
       "   'Asian Fusion',\n",
       "   'Brazilian',\n",
       "   'Buffets',\n",
       "   'Cambodian',\n",
       "   'Cantonese',\n",
       "   'Chinese',\n",
       "   'Cuban',\n",
       "   'Dim Sum',\n",
       "   'Ethiopian',\n",
       "   'Filipino',\n",
       "   'Food Delivery Services',\n",
       "   'Gluten-Free',\n",
       "   'Hawaiian',\n",
       "   'Himalayan-Nepalese',\n",
       "   'Hot Pot',\n",
       "   'Indian',\n",
       "   'Indonesian',\n",
       "   'Japanese',\n",
       "   'Karaoke',\n",
       "   'Korean',\n",
       "   'Laotian',\n",
       "   'Malaysian',\n",
       "   'Mongolian',\n",
       "   'Pakistani',\n",
       "   'Ramen',\n",
       "   'Shanghainese',\n",
       "   'Singaporean',\n",
       "   'Sushi Bars',\n",
       "   'Taiwanese',\n",
       "   'Thai',\n",
       "   'Vegan',\n",
       "   'Vietnamese'],\n",
       "  2: ['American (New)',\n",
       "   'American (Traditional)',\n",
       "   'Arcades',\n",
       "   'Argentine',\n",
       "   'Bars',\n",
       "   'Basque',\n",
       "   'Belgian',\n",
       "   'Bistros',\n",
       "   'Breweries',\n",
       "   'British',\n",
       "   'Butcher',\n",
       "   'Cafes',\n",
       "   'Cajun-Creole',\n",
       "   'Canadian (New)',\n",
       "   'Caribbean',\n",
       "   'Champagne Bars',\n",
       "   'Chicken Wings',\n",
       "   'Cocktail Bars',\n",
       "   'Cultural Center',\n",
       "   'Distilleries',\n",
       "   'Dive Bars',\n",
       "   'Fish & Chips',\n",
       "   'Fondue',\n",
       "   'Food Court',\n",
       "   'French',\n",
       "   'Gastropubs',\n",
       "   'German',\n",
       "   'Hookah Bars',\n",
       "   'Irish',\n",
       "   'Italian',\n",
       "   'Latin American',\n",
       "   'Live-Raw Food',\n",
       "   'Local Flavor',\n",
       "   'Lounges',\n",
       "   'Mexican',\n",
       "   'Modern European',\n",
       "   'Moroccan',\n",
       "   'Peruvian',\n",
       "   'Piano Bars',\n",
       "   'Pool Halls',\n",
       "   'Portuguese',\n",
       "   'Pubs',\n",
       "   'Salad',\n",
       "   'Scottish',\n",
       "   'Seafood',\n",
       "   'Soul Food',\n",
       "   'Spanish',\n",
       "   'Sports Bars',\n",
       "   'Steakhouses',\n",
       "   'Tapas-Small Plates',\n",
       "   'Tapas Bars',\n",
       "   'Tex-Mex',\n",
       "   'Ukrainian',\n",
       "   'Wine Bars',\n",
       "   'Wineries']},\n",
       " {'Afghan': 1,\n",
       "  'African': 0,\n",
       "  'American (New)': 2,\n",
       "  'American (Traditional)': 2,\n",
       "  'Arabian': 1,\n",
       "  'Arcades': 2,\n",
       "  'Argentine': 2,\n",
       "  'Asian Fusion': 0,\n",
       "  'Bagels': 1,\n",
       "  'Bakeries': 1,\n",
       "  'Barbeque': 1,\n",
       "  'Bars': 2,\n",
       "  'Basque': 2,\n",
       "  'Beer, Wine & Spirits': 1,\n",
       "  'Belgian': 2,\n",
       "  'Bistros': 2,\n",
       "  'Brasseries': 1,\n",
       "  'Brazilian': 0,\n",
       "  'Breakfast & Brunch': 1,\n",
       "  'Breweries': 2,\n",
       "  'British': 2,\n",
       "  'Bubble Tea': 1,\n",
       "  'Buffets': 0,\n",
       "  'Burgers': 1,\n",
       "  'Butcher': 2,\n",
       "  'Cafes': 2,\n",
       "  'Cafeteria': 1,\n",
       "  'Cajun-Creole': 2,\n",
       "  'Cambodian': 0,\n",
       "  'Canadian (New)': 2,\n",
       "  'Cantonese': 0,\n",
       "  'Caribbean': 2,\n",
       "  'Caterers': 1,\n",
       "  'Champagne Bars': 2,\n",
       "  'Cheese Shops': 1,\n",
       "  'Cheesesteaks': 1,\n",
       "  'Chicken Wings': 2,\n",
       "  'Chinese': 0,\n",
       "  'Cocktail Bars': 2,\n",
       "  'Coffee & Tea': 1,\n",
       "  'Comfort Food': 1,\n",
       "  'Creperies': 1,\n",
       "  'Cuban': 0,\n",
       "  'Cultural Center': 2,\n",
       "  'Delis': 1,\n",
       "  'Desserts': 1,\n",
       "  'Dim Sum': 0,\n",
       "  'Diners': 1,\n",
       "  'Distilleries': 2,\n",
       "  'Dive Bars': 2,\n",
       "  'Do-It-Yourself Food': 1,\n",
       "  'Donuts': 1,\n",
       "  'Drugstores': 1,\n",
       "  'Ethiopian': 0,\n",
       "  'Ethnic Food': 1,\n",
       "  'Fast Food': 1,\n",
       "  'Filipino': 0,\n",
       "  'Fish & Chips': 2,\n",
       "  'Fondue': 2,\n",
       "  'Food Court': 2,\n",
       "  'Food Delivery Services': 0,\n",
       "  'Food Stands': 1,\n",
       "  'Food Trucks': 1,\n",
       "  'French': 2,\n",
       "  'Fruits & Veggies': 1,\n",
       "  'Gastropubs': 2,\n",
       "  'Gelato': 1,\n",
       "  'German': 2,\n",
       "  'Gluten-Free': 0,\n",
       "  'Greek': 1,\n",
       "  'Halal': 1,\n",
       "  'Hawaiian': 0,\n",
       "  'Health Markets': 1,\n",
       "  'Himalayan-Nepalese': 0,\n",
       "  'Hookah Bars': 2,\n",
       "  'Hot Dogs': 1,\n",
       "  'Hot Pot': 0,\n",
       "  'Ice Cream & Frozen Yogurt': 1,\n",
       "  'Indian': 0,\n",
       "  'Indonesian': 0,\n",
       "  'Internet Cafes': 1,\n",
       "  'Irish': 2,\n",
       "  'Italian': 2,\n",
       "  'Japanese': 0,\n",
       "  'Juice Bars & Smoothies': 1,\n",
       "  'Karaoke': 0,\n",
       "  'Korean': 0,\n",
       "  'Kosher': 1,\n",
       "  'Laotian': 0,\n",
       "  'Latin American': 2,\n",
       "  'Lebanese': 1,\n",
       "  'Live-Raw Food': 2,\n",
       "  'Local Flavor': 2,\n",
       "  'Lounges': 2,\n",
       "  'Malaysian': 0,\n",
       "  'Meat Shops': 1,\n",
       "  'Mediterranean': 1,\n",
       "  'Mexican': 2,\n",
       "  'Middle Eastern': 1,\n",
       "  'Modern European': 2,\n",
       "  'Mongolian': 0,\n",
       "  'Moroccan': 2,\n",
       "  'Pakistani': 0,\n",
       "  'Persian-Iranian': 1,\n",
       "  'Peruvian': 2,\n",
       "  'Piano Bars': 2,\n",
       "  'Pizza': 1,\n",
       "  'Polish': 1,\n",
       "  'Pool Halls': 2,\n",
       "  'Portuguese': 2,\n",
       "  'Pretzels': 1,\n",
       "  'Pubs': 2,\n",
       "  'Ramen': 0,\n",
       "  'Russian': 1,\n",
       "  'Salad': 2,\n",
       "  'Salvadoran': 1,\n",
       "  'Sandwiches': 1,\n",
       "  'Scandinavian': 1,\n",
       "  'Scottish': 2,\n",
       "  'Seafood': 2,\n",
       "  'Shanghainese': 0,\n",
       "  'Shaved Ice': 1,\n",
       "  'Singaporean': 0,\n",
       "  'Soul Food': 2,\n",
       "  'Soup': 1,\n",
       "  'Southern': 1,\n",
       "  'Spanish': 2,\n",
       "  'Specialty Food': 1,\n",
       "  'Sports Bars': 2,\n",
       "  'Steakhouses': 2,\n",
       "  'Street Vendors': 1,\n",
       "  'Sushi Bars': 0,\n",
       "  'Szechuan': 1,\n",
       "  'Taiwanese': 0,\n",
       "  'Tapas-Small Plates': 2,\n",
       "  'Tapas Bars': 2,\n",
       "  'Tea Rooms': 1,\n",
       "  'Tex-Mex': 2,\n",
       "  'Thai': 0,\n",
       "  'Turkish': 1,\n",
       "  'Ukrainian': 2,\n",
       "  'Vegan': 0,\n",
       "  'Vegetarian': 1,\n",
       "  'Venezuelan': 1,\n",
       "  'Vietnamese': 0,\n",
       "  'Wine Bars': 2,\n",
       "  'Wineries': 2})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cluster(d2v, vector_len, cat_names, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

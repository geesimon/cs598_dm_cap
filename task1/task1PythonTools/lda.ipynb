{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dm_cap_mkl_py3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\geesi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "np.random.seed(2018)\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text, min_len = 4):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['I', 'wanted', 'ice', 'cream', 'so', 'my', 'husband', 'pulled', 'off', 'the', 'highway', 'at', 'this', \"Culver's.\", 'We', 'opted', 'to', 'get', 'the', 'flavor', 'of', 'the', 'day', '(cookie', 'dough', 'craving).', 'I', 'was', 'disappointed', 'to', 'find', 'that', 'it', 'was', 'a', 'chocolate', 'custard', 'with', 'TONS', 'of', 'cookie', 'dough', 'chunks', 'because', 'I', 'like', 'custard', 'and', 'not', 'just', 'the', 'mix-ins,', 'etc.', 'but', 'it', 'was', 'overall', 'a', 'decent', 'sundae.', \"I'll\", 'just', 'stick', 'to', 'vanilla', 'next', 'time!\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['want', 'cream', 'husband', 'pull', 'highway', 'culver', 'opt', 'flavor', 'cooki', 'dough', 'crave', 'disappoint', 'chocol', 'custard', 'ton', 'cooki', 'dough', 'chunk', 'like', 'custard', 'overal', 'decent', 'sunda', 'stick', 'vanilla', 'time']\n"
     ]
    }
   ],
   "source": [
    "sample_file = 'rest_review_sample_100000.txt'\n",
    "documents = []\n",
    "\n",
    "with open (sample_file, 'r') as f:\n",
    "    documents = f.readlines()\n",
    "        \n",
    "doc_sample = documents[1]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100181/100181 [02:42<00:00, 614.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "processed_docs = [preprocess(text) for text in tqdm(documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.020*\"good\" + 0.014*\"like\" + 0.012*\"great\" + 0.011*\"place\" + 0.009*\"sauc\" + 0.009*\"order\" + 0.009*\"salad\" + 0.009*\"pizza\" + 0.008*\"love\" + 0.008*\"chees\"\n",
      "Topic: 1 \n",
      "Words: 0.030*\"pizza\" + 0.019*\"order\" + 0.014*\"good\" + 0.011*\"breakfast\" + 0.011*\"time\" + 0.010*\"thai\" + 0.009*\"like\" + 0.009*\"come\" + 0.008*\"place\" + 0.006*\"great\"\n",
      "Topic: 2 \n",
      "Words: 0.015*\"good\" + 0.012*\"go\" + 0.010*\"time\" + 0.010*\"restaur\" + 0.010*\"place\" + 0.010*\"order\" + 0.010*\"like\" + 0.010*\"come\" + 0.009*\"servic\" + 0.008*\"chicken\"\n",
      "Topic: 3 \n",
      "Words: 0.020*\"order\" + 0.018*\"place\" + 0.014*\"time\" + 0.014*\"wait\" + 0.014*\"come\" + 0.013*\"servic\" + 0.012*\"good\" + 0.011*\"tabl\" + 0.010*\"like\" + 0.010*\"drink\"\n",
      "Topic: 4 \n",
      "Words: 0.024*\"burger\" + 0.019*\"place\" + 0.016*\"like\" + 0.016*\"good\" + 0.011*\"fri\" + 0.009*\"come\" + 0.009*\"friend\" + 0.009*\"beer\" + 0.008*\"nice\" + 0.008*\"great\"\n",
      "Topic: 5 \n",
      "Words: 0.013*\"place\" + 0.012*\"sushi\" + 0.011*\"restaur\" + 0.010*\"roll\" + 0.009*\"good\" + 0.009*\"menu\" + 0.008*\"like\" + 0.008*\"dish\" + 0.008*\"love\" + 0.007*\"great\"\n",
      "Topic: 6 \n",
      "Words: 0.045*\"great\" + 0.024*\"place\" + 0.018*\"good\" + 0.016*\"taco\" + 0.015*\"servic\" + 0.014*\"time\" + 0.013*\"love\" + 0.010*\"like\" + 0.009*\"friend\" + 0.009*\"best\"\n",
      "Topic: 7 \n",
      "Words: 0.018*\"order\" + 0.015*\"good\" + 0.012*\"come\" + 0.012*\"like\" + 0.012*\"fri\" + 0.011*\"chicken\" + 0.009*\"rice\" + 0.009*\"sauc\" + 0.009*\"place\" + 0.008*\"tast\"\n",
      "Topic: 8 \n",
      "Words: 0.022*\"sandwich\" + 0.016*\"great\" + 0.014*\"buffet\" + 0.013*\"place\" + 0.012*\"servic\" + 0.011*\"good\" + 0.011*\"price\" + 0.008*\"time\" + 0.008*\"lunch\" + 0.008*\"go\"\n",
      "Topic: 9 \n",
      "Words: 0.031*\"place\" + 0.030*\"good\" + 0.013*\"like\" + 0.010*\"price\" + 0.009*\"time\" + 0.009*\"mexican\" + 0.008*\"taco\" + 0.008*\"chicken\" + 0.008*\"servic\" + 0.008*\"come\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.008*\"thai\" + 0.006*\"noodl\" + 0.005*\"place\" + 0.005*\"chicken\" + 0.005*\"good\" + 0.005*\"order\" + 0.005*\"ramen\" + 0.004*\"great\" + 0.004*\"servic\" + 0.004*\"rice\"\n",
      "Topic: 1 Word: 0.006*\"burger\" + 0.005*\"order\" + 0.004*\"time\" + 0.004*\"wait\" + 0.004*\"come\" + 0.004*\"minut\" + 0.004*\"place\" + 0.004*\"drink\" + 0.004*\"servic\" + 0.004*\"tabl\"\n",
      "Topic: 2 Word: 0.006*\"buffet\" + 0.004*\"good\" + 0.004*\"great\" + 0.003*\"drink\" + 0.003*\"time\" + 0.003*\"place\" + 0.003*\"breakfast\" + 0.003*\"come\" + 0.003*\"like\" + 0.003*\"vega\"\n",
      "Topic: 3 Word: 0.006*\"sushi\" + 0.005*\"burger\" + 0.004*\"pancak\" + 0.004*\"place\" + 0.004*\"good\" + 0.004*\"like\" + 0.004*\"fri\" + 0.004*\"chicken\" + 0.004*\"order\" + 0.004*\"roll\"\n",
      "Topic: 4 Word: 0.024*\"pizza\" + 0.008*\"great\" + 0.007*\"sandwich\" + 0.006*\"crust\" + 0.006*\"love\" + 0.006*\"best\" + 0.006*\"awesom\" + 0.006*\"burger\" + 0.006*\"place\" + 0.006*\"good\"\n",
      "Topic: 5 Word: 0.013*\"taco\" + 0.006*\"salsa\" + 0.006*\"burrito\" + 0.005*\"mexican\" + 0.005*\"great\" + 0.005*\"good\" + 0.005*\"chicken\" + 0.005*\"place\" + 0.005*\"wing\" + 0.005*\"chip\"\n",
      "Topic: 6 Word: 0.005*\"steak\" + 0.004*\"great\" + 0.004*\"good\" + 0.004*\"order\" + 0.003*\"fri\" + 0.003*\"perfect\" + 0.003*\"servic\" + 0.003*\"burger\" + 0.003*\"come\" + 0.003*\"restaur\"\n",
      "Topic: 7 Word: 0.008*\"sushi\" + 0.005*\"roll\" + 0.005*\"great\" + 0.004*\"good\" + 0.004*\"place\" + 0.004*\"order\" + 0.004*\"love\" + 0.003*\"restaur\" + 0.003*\"servic\" + 0.003*\"best\"\n",
      "Topic: 8 Word: 0.004*\"order\" + 0.004*\"place\" + 0.004*\"time\" + 0.004*\"sandwich\" + 0.004*\"good\" + 0.004*\"servic\" + 0.003*\"like\" + 0.003*\"come\" + 0.003*\"go\" + 0.003*\"great\"\n",
      "Topic: 9 Word: 0.012*\"great\" + 0.007*\"servic\" + 0.007*\"good\" + 0.006*\"place\" + 0.006*\"friend\" + 0.006*\"atmospher\" + 0.006*\"love\" + 0.005*\"nice\" + 0.005*\"staff\" + 0.005*\"price\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

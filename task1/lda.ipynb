{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# from nltk.stem.porter import *\n",
    "# import numpy as np\n",
    "# import nltk\n",
    "\n",
    "# np.random.seed(2018)\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# from time import time\n",
    "# np.random.seed(2018)\n",
    "\n",
    "# #logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# #lda_model = gensim.models.LdaModel(bow_corpus, num_topics=10, id2word=dictionary,  eval_every=5, iterations = 1000, alpha='auto', gamma_threshold=0.01)\n",
    "# t0 = time()\n",
    "# #lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, iterations = 1000, passes = 2)\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, num_topics=10, id2word=dictionary,  eval_every=5, iterations = 1000, alpha='auto', gamma_threshold=0.01)\n",
    "# print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "# for idx, topic in lda_model.print_topics(-1):\n",
    "#     print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:19<00:00, 717.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 54.840080s\n",
      "done in 53.405018s\n",
      "k=5 coherence=-8.711588\n",
      "done in 55.879316s\n",
      "k=7 coherence=-9.012033\n",
      "done in 55.771752s\n",
      "k=9 coherence=-8.386204\n",
      "done in 53.025692s\n",
      "k=11 coherence=-9.758425\n",
      "done in 54.100503s\n",
      "k=13 coherence=-7.879161\n",
      "done in 55.230710s\n",
      "k=15 coherence=-8.563793\n",
      "done in 54.732833s\n",
      "k=17 coherence=-9.184393\n",
      "done in 53.434371s\n",
      "k=19 coherence=-9.459875\n",
      "done in 54.366743s\n",
      "k=21 coherence=-8.239192\n",
      "done in 55.524129s\n",
      "k=23 coherence=-6.813123\n",
      "done in 54.958368s\n",
      "k=25 coherence=-8.404542\n",
      "done in 55.031952s\n",
      "k=27 coherence=-7.494700\n",
      "done in 54.308684s\n",
      "k=29 coherence=-8.648878\n",
      "done in 55.207020s\n",
      "k=31 coherence=-7.119518\n",
      "done in 74.247778s\n",
      "k=33 coherence=-7.831742\n",
      "done in 1106.163820s\n",
      "k=35 coherence=-7.199657\n",
      "done in 58.562465s\n",
      "k=37 coherence=-7.412367\n",
      "done in 66.677724s\n",
      "k=39 coherence=-7.469745\n",
      "done in 65.118296s\n",
      "k=41 coherence=-7.468898\n",
      "done in 65.331245s\n",
      "k=43 coherence=-6.461854\n",
      "done in 72.429250s\n",
      "k=45 coherence=-7.173412\n",
      "done in 69.824019s\n",
      "k=47 coherence=-7.043966\n",
      "done in 70.762374s\n",
      "k=49 coherence=-6.323688\n",
      "done in 70.012990s\n",
      "k=51 coherence=-7.131176\n",
      "done in 69.949732s\n",
      "k=53 coherence=-6.103705\n",
      "done in 69.659272s\n",
      "k=55 coherence=-6.543207\n",
      "done in 71.047445s\n",
      "k=57 coherence=-6.268963\n",
      "done in 71.933576s\n",
      "k=59 coherence=-6.069249\n",
      "done in 72.130097s\n",
      "k=61 coherence=-6.082221\n",
      "done in 70.314358s\n",
      "k=63 coherence=-5.887149\n",
      "done in 79.040825s\n",
      "k=65 coherence=-6.256523\n",
      "done in 72.512707s\n",
      "k=67 coherence=-6.298812\n",
      "done in 61.332944s\n",
      "k=69 coherence=-6.297968\n",
      "done in 62.022715s\n",
      "k=71 coherence=-6.261041\n",
      "done in 66.806908s\n",
      "k=73 coherence=-6.335642\n",
      "done in 90.294826s\n",
      "k=75 coherence=-6.336505\n",
      "done in 94.626681s\n",
      "k=77 coherence=-6.050592\n",
      "done in 91.593441s\n",
      "k=79 coherence=-5.910567\n",
      "done in 78.114729s\n",
      "k=81 coherence=-5.876420\n",
      "done in 79.212896s\n",
      "k=83 coherence=-5.759697\n",
      "done in 79.512523s\n",
      "k=85 coherence=-5.811315\n",
      "done in 80.736456s\n",
      "k=87 coherence=-5.734602\n",
      "done in 81.761635s\n",
      "k=89 coherence=-5.803246\n",
      "done in 82.506235s\n",
      "k=91 coherence=-5.841822\n",
      "done in 83.278917s\n",
      "k=93 coherence=-5.893402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dm_cap_py3/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 81.404716s\n",
      "k=95 coherence=-6.098757\n",
      "done in 72.649066s\n",
      "k=97 coherence=-5.859996\n",
      "done in 73.923305s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 119/27804 [00:00<00:23, 1179.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=99 coherence=-6.032273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27804/27804 [00:33<00:00, 822.53it/s] \n",
      "  1%|          | 71/10912 [00:00<00:15, 686.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 16.170151s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10912/10912 [00:14<00:00, 762.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 7.126101s\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def get_corpus(file_name, use_tfidf = True):\n",
    "    documents = []\n",
    "\n",
    "    with open (file_name, 'r') as f:\n",
    "        documents = f.readlines()\n",
    "\n",
    "    processed_docs = [preprocess(text) for text in tqdm(documents)]\n",
    "\n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    if use_tfidf:\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        corpus = tfidf[corpus]\n",
    "        \n",
    "    return (corpus, dictionary)\n",
    "\n",
    "def save_corpus(corpus, file_name):\n",
    "    with open(file_name + '.txt', 'w') as f:\n",
    "        for doc in tqdm(corpus):\n",
    "            words = []\n",
    "            for i,v in doc:\n",
    "                words.append(dictionary[i])\n",
    "            f.write(' '.join(words) + '\\n')\n",
    "        \n",
    "def get_lda_topic_model(corpus, dictionary, num_topics = 10):\n",
    "    t0 = time()\n",
    "    #lda_model = models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary) #Bad!\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary,  eval_every=5, alpha='auto', gamma_threshold=0.01)\n",
    "    #lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary,  eval_every=5, iterations = 1000, alpha='auto', gamma_threshold=0.01)\n",
    "    #lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary,  eval_every=5, alpha='auto') #Good enough\n",
    "    #lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    \n",
    "    return lda_model\n",
    "\n",
    "def get_lsi_topic_model(corpus, dictionary, num_topics = 10):\n",
    "    t0 = time()\n",
    "    lsi_model = models.LsiModel(corpus, num_topics = num_topics, id2word = dictionary)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    \n",
    "    return lsi_model\n",
    "\n",
    "files = ['rest_review_sample_100000', 'categories/Chinese_pos', 'categories/Chinese_neg','categories/Chinese', 'categories/Mexican']\n",
    "(corpus, dictionary) = get_corpus(files[0] + \".txt\")\n",
    "save_corpus(corpus, files[0])\n",
    "model_1 = get_lda_topic_model(corpus, dictionary, num_topics=20)\n",
    "#model_1_1 = get_lsi_topic_model(corpus, dictionary, num_topics=20)\n",
    "largest_coherence = -1e20\n",
    "best_k = 0\n",
    "for k in range(5, 100, 2):\n",
    "    model = get_lda_topic_model(corpus, dictionary, num_topics=k)    \n",
    "    cm = models.coherencemodel.CoherenceModel(model=model, corpus=corpus, coherence='u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "    print(\"k=%d coherence=%f\"%(k, coherence))\n",
    "    if (coherence > largest_coherence):\n",
    "        largest_coherence = coherence\n",
    "        model_1 = model\n",
    "        best_k = k\n",
    "    \n",
    "corpus, dictionary = get_corpus(files[1] + \".txt\")\n",
    "save_corpus(corpus, files[1])\n",
    "model_2 = get_lda_topic_model(corpus, dictionary)\n",
    "\n",
    "corpus, dictionary = get_corpus(files[2] + \".txt\")\n",
    "save_corpus(corpus, files[2])\n",
    "model_3 = get_lda_topic_model(corpus, dictionary)\n",
    "# model_4 = get_topic_model(files[3] + \".txt\")\n",
    "# model_5 = get_topic_model(files[4] + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock_Colors = ['black', 'maroon', 'red', 'purple', 'fuchsia', 'green', 'lime', 'olive', 'navy', 'blue', 'teal', 'aqua',\n",
    "               'black', 'maroon', 'red', 'purple', 'fuchsia', 'green', 'lime', 'olive']\n",
    "\n",
    "def get_topic_json(model, title, compare_words):\n",
    "    topic = model.show_topics(-1, formatted=False)\n",
    "    children_name = 'children'\n",
    "    name_name = 'name'\n",
    "    value_name = 'value'\n",
    "    color_name = 'color'    \n",
    "\n",
    "    topic_out = {name_name: title, children_name:[]}\n",
    "\n",
    "    for i in range(0, len(topic)):\n",
    "        topic_out[children_name].append({name_name: 'Topic ' + str(i), children_name:[]})\n",
    "        max_weight = topic[i][1][0][1]\n",
    "        for j in range(0, len(topic[i][1])):\n",
    "            topic_out[children_name][i][children_name].append({name_name:topic[i][1][j][0],\n",
    "                                                               value_name:\"{0:.2f}\".format(topic[i][1][j][1]/max_weight),\n",
    "                                                               color_name: Stock_Colors[i],\n",
    "                                                               'new_word': not (topic[i][1][j][0] in compare_words)})\n",
    "        \n",
    "    return topic_out\n",
    "\n",
    "\n",
    "def get_topic_words(model):\n",
    "    topic = model.show_topics(-1, formatted=False)\n",
    "    all_words = {}\n",
    "    for i in range(0, len(topic)):\n",
    "        for j in range(0, len(topic[i][1])):\n",
    "            if topic[i][1][j][0] in all_words:\n",
    "                all_words[topic[i][1][j][0]] += 1\n",
    "            else:\n",
    "                all_words[topic[i][1][j][0]] = 1\n",
    "    \n",
    "    return all_words\n",
    "    \n",
    "# with open(files[0] + '.json', 'w') as f:\n",
    "#     f.write(json.dumps(get_topic_json(model_1, '100000_Samples', {})))\n",
    "\n",
    "# pos_words = get_topic_words(model_2)\n",
    "# neg_words = get_topic_words(model_3)\n",
    "\n",
    "# topic2 = get_topic_json(model_2, 'Positive', neg_words)\n",
    "# topic3 = get_topic_json(model_3, 'Negative', pos_words)\n",
    "# topic = {'name':'Chinese Restaurant Reviews', 'children':[topic2, topic3]}\n",
    "# with open('compare' + '.json', 'w') as f:\n",
    "#     f.write(json.dumps(topic))\n",
    "    \n",
    "# topic4 = get_topic_json(model_4, 'Chinese')\n",
    "# topic5 = get_topic_json(model_5, 'Mexican')\n",
    "# topic = {'name':'Chinese vs. Mexican', 'children':[topic2, topic3]}\n",
    "# with open('compare_cn_mx' + '.json', 'w') as f:\n",
    "#     f.write(json.dumps(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Words: 0.008*\"excus\" + 0.008*\"manag\" + 0.008*\"tabl\" + 0.007*\"apolog\" + 0.007*\"wait\" + 0.007*\"minut\" + 0.006*\"order\" + 0.006*\"time\" + 0.006*\"come\" + 0.006*\"approach\"\n",
      "Topic: 1 Words: 0.056*\"rib\" + 0.021*\"unbeliev\" + 0.019*\"short\" + 0.018*\"tonight\" + 0.018*\"disgust\" + 0.018*\"profession\" + 0.017*\"stumbl\" + 0.016*\"gross\" + 0.016*\"marin\" + 0.015*\"caramel\"\n",
      "Topic: 2 Words: 0.040*\"groupon\" + 0.023*\"benedict\" + 0.018*\"close\" + 0.017*\"wont\" + 0.011*\"tail\" + 0.010*\"chilli\" + 0.010*\"scene\" + 0.009*\"golden\" + 0.008*\"lean\" + 0.008*\"cave\"\n",
      "Topic: 3 Words: 0.037*\"buffalo\" + 0.036*\"falafel\" + 0.027*\"higher\" + 0.025*\"condiment\" + 0.019*\"competit\" + 0.018*\"spanish\" + 0.018*\"hesit\" + 0.016*\"complex\" + 0.015*\"wast\" + 0.014*\"smell\"\n",
      "Topic: 4 Words: 0.041*\"airport\" + 0.036*\"outstand\" + 0.031*\"chipotl\" + 0.028*\"excel\" + 0.026*\"bomb\" + 0.020*\"knowledg\" + 0.020*\"pricey\" + 0.015*\"winner\" + 0.013*\"blend\" + 0.013*\"great\"\n",
      "Topic: 5 Words: 0.065*\"burger\" + 0.031*\"ring\" + 0.023*\"onion\" + 0.019*\"fri\" + 0.015*\"hubbi\" + 0.015*\"everyday\" + 0.014*\"constant\" + 0.014*\"kid\" + 0.014*\"mcdonald\" + 0.014*\"adult\"\n",
      "Topic: 6 Words: 0.015*\"soon\" + 0.012*\"anytim\" + 0.012*\"flatbread\" + 0.010*\"crack\" + 0.009*\"mint\" + 0.008*\"mojito\" + 0.008*\"feet\" + 0.007*\"cider\" + 0.007*\"occupi\" + 0.007*\"burger\"\n",
      "Topic: 7 Words: 0.027*\"pasti\" + 0.026*\"brand\" + 0.023*\"health\" + 0.020*\"bang\" + 0.019*\"wellington\" + 0.018*\"pleasur\" + 0.015*\"listen\" + 0.014*\"penni\" + 0.014*\"brooklyn\" + 0.014*\"stack\"\n",
      "Topic: 8 Words: 0.041*\"brisket\" + 0.020*\"pastor\" + 0.019*\"steak\" + 0.019*\"philli\" + 0.013*\"meat\" + 0.013*\"cheesesteak\" + 0.013*\"side\" + 0.013*\"chees\" + 0.011*\"potato\" + 0.011*\"sandwich\"\n",
      "Topic: 9 Words: 0.019*\"music\" + 0.018*\"king\" + 0.017*\"answer\" + 0.016*\"multipl\" + 0.016*\"danc\" + 0.015*\"question\" + 0.015*\"recip\" + 0.014*\"soooo\" + 0.014*\"jalapeno\" + 0.013*\"tequila\"\n",
      "Topic: 10 Words: 0.019*\"coupon\" + 0.012*\"mike\" + 0.012*\"poke\" + 0.011*\"yelp\" + 0.011*\"eater\" + 0.011*\"wash\" + 0.010*\"http\" + 0.010*\"picki\" + 0.010*\"quarter\" + 0.009*\"horchata\"\n",
      "Topic: 11 Words: 0.020*\"genuin\" + 0.017*\"skinni\" + 0.017*\"buddi\" + 0.014*\"kudo\" + 0.014*\"worthi\" + 0.013*\"papa\" + 0.013*\"tamal\" + 0.012*\"cafeteria\" + 0.011*\"devil\" + 0.011*\"funki\"\n",
      "Topic: 12 Words: 0.026*\"finger\" + 0.020*\"can\" + 0.018*\"design\" + 0.016*\"mesa\" + 0.015*\"separ\" + 0.014*\"expand\" + 0.012*\"remodel\" + 0.010*\"rais\" + 0.010*\"dri\" + 0.010*\"fishi\"\n",
      "Topic: 13 Words: 0.056*\"truffl\" + 0.041*\"environ\" + 0.034*\"notch\" + 0.031*\"cashier\" + 0.025*\"parmesan\" + 0.025*\"squar\" + 0.025*\"spoon\" + 0.023*\"mayo\" + 0.021*\"train\" + 0.017*\"hotdog\"\n",
      "Topic: 14 Words: 0.051*\"truck\" + 0.033*\"east\" + 0.023*\"valley\" + 0.021*\"coast\" + 0.021*\"prici\" + 0.020*\"hipster\" + 0.020*\"pound\" + 0.019*\"fli\" + 0.016*\"itali\" + 0.016*\"ticket\"\n",
      "Topic: 15 Words: 0.051*\"deliveri\" + 0.028*\"deliv\" + 0.027*\"subway\" + 0.024*\"sandwich\" + 0.022*\"lover\" + 0.021*\"mediterranean\" + 0.018*\"bakeri\" + 0.018*\"pizza\" + 0.015*\"cowork\" + 0.015*\"calzon\"\n",
      "Topic: 16 Words: 0.026*\"chorizo\" + 0.021*\"sticki\" + 0.020*\"nutella\" + 0.018*\"thumb\" + 0.017*\"mother\" + 0.016*\"sooo\" + 0.015*\"box\" + 0.012*\"sooner\" + 0.011*\"steep\" + 0.011*\"brais\"\n",
      "Topic: 17 Words: 0.076*\"tapa\" + 0.052*\"strawberri\" + 0.033*\"cevich\" + 0.029*\"fluffi\" + 0.028*\"regist\" + 0.023*\"plaza\" + 0.023*\"bonus\" + 0.022*\"refresh\" + 0.020*\"comfi\" + 0.020*\"franchis\"\n",
      "Topic: 18 Words: 0.018*\"scallop\" + 0.017*\"flight\" + 0.012*\"southern\" + 0.010*\"anim\" + 0.008*\"pain\" + 0.008*\"burger\" + 0.008*\"wise\" + 0.008*\"portillo\" + 0.008*\"doubl\" + 0.007*\"complement\"\n",
      "Topic: 19 Words: 0.031*\"italian\" + 0.025*\"pasta\" + 0.017*\"oliv\" + 0.016*\"eggplant\" + 0.015*\"pizza\" + 0.013*\"hook\" + 0.013*\"garden\" + 0.011*\"mozzarella\" + 0.010*\"salad\" + 0.010*\"parm\"\n",
      "Topic: 20 Words: 0.062*\"pizza\" + 0.027*\"crust\" + 0.014*\"slice\" + 0.013*\"pepperoni\" + 0.012*\"slider\" + 0.012*\"top\" + 0.011*\"burger\" + 0.010*\"cheeseburg\" + 0.009*\"chees\" + 0.009*\"milkshak\"\n",
      "Topic: 21 Words: 0.046*\"cajun\" + 0.035*\"twice\" + 0.032*\"altern\" + 0.027*\"texa\" + 0.025*\"madison\" + 0.025*\"sugar\" + 0.024*\"powder\" + 0.021*\"will\" + 0.017*\"devour\" + 0.017*\"western\"\n",
      "Topic: 22 Words: 0.018*\"ayc\" + 0.010*\"mussel\" + 0.010*\"sake\" + 0.009*\"skewer\" + 0.005*\"menu\" + 0.005*\"beach\" + 0.005*\"order\" + 0.005*\"like\" + 0.005*\"shrimp\" + 0.005*\"tomorrow\"\n",
      "Topic: 23 Words: 0.050*\"vegan\" + 0.033*\"vegetarian\" + 0.018*\"section\" + 0.017*\"wick\" + 0.014*\"custard\" + 0.013*\"option\" + 0.012*\"queso\" + 0.011*\"energi\" + 0.011*\"substitut\" + 0.011*\"everytim\"\n",
      "Topic: 24 Words: 0.016*\"bloodi\" + 0.015*\"mari\" + 0.013*\"terrif\" + 0.009*\"mountain\" + 0.007*\"chris\" + 0.007*\"steakhous\" + 0.006*\"recogn\" + 0.006*\"walnut\" + 0.005*\"advis\" + 0.005*\"brunch\"\n",
      "Topic: 25 Words: 0.027*\"pull\" + 0.027*\"improv\" + 0.025*\"avocado\" + 0.024*\"cilantro\" + 0.021*\"pork\" + 0.017*\"barbecu\" + 0.015*\"arugula\" + 0.015*\"edamam\" + 0.011*\"pico\" + 0.011*\"random\"\n",
      "Topic: 26 Words: 0.065*\"game\" + 0.046*\"ketchup\" + 0.037*\"watch\" + 0.032*\"beer\" + 0.031*\"japanes\" + 0.024*\"courteous\" + 0.023*\"footbal\" + 0.019*\"english\" + 0.019*\"sport\" + 0.017*\"bright\"\n",
      "Topic: 27 Words: 0.037*\"fast\" + 0.030*\"burger\" + 0.028*\"patti\" + 0.024*\"katsu\" + 0.018*\"teriyaki\" + 0.018*\"clean\" + 0.017*\"cheap\" + 0.017*\"terribl\" + 0.015*\"aioli\" + 0.015*\"bowl\"\n",
      "Topic: 28 Words: 0.046*\"crepe\" + 0.025*\"pud\" + 0.014*\"crawfish\" + 0.013*\"partner\" + 0.012*\"chocol\" + 0.012*\"underwhelm\" + 0.011*\"puff\" + 0.011*\"current\" + 0.010*\"their\" + 0.009*\"lentil\"\n",
      "Topic: 29 Words: 0.024*\"discount\" + 0.023*\"machin\" + 0.022*\"snack\" + 0.017*\"nois\" + 0.017*\"movi\" + 0.014*\"america\" + 0.013*\"ampl\" + 0.012*\"tap\" + 0.012*\"pleas\" + 0.011*\"soda\"\n",
      "Topic: 30 Words: 0.078*\"taco\" + 0.038*\"margarita\" + 0.027*\"crab\" + 0.024*\"leg\" + 0.021*\"delish\" + 0.016*\"guac\" + 0.015*\"fish\" + 0.015*\"carnita\" + 0.013*\"bell\" + 0.010*\"skillet\"\n",
      "Topic: 31 Words: 0.030*\"belli\" + 0.019*\"card\" + 0.013*\"negat\" + 0.012*\"credit\" + 0.012*\"crunch\" + 0.012*\"coleslaw\" + 0.011*\"phone\" + 0.011*\"pork\" + 0.010*\"nasti\" + 0.009*\"gift\"\n",
      "Topic: 32 Words: 0.082*\"meatbal\" + 0.035*\"favor\" + 0.035*\"undercook\" + 0.025*\"pie\" + 0.022*\"pint\" + 0.019*\"lech\" + 0.019*\"apart\" + 0.018*\"carb\" + 0.018*\"stomach\" + 0.017*\"awar\"\n",
      "Topic: 33 Words: 0.030*\"pickl\" + 0.030*\"blueberri\" + 0.028*\"team\" + 0.024*\"pineappl\" + 0.024*\"martini\" + 0.020*\"spread\" + 0.018*\"class\" + 0.017*\"ordinari\" + 0.016*\"advic\" + 0.015*\"wild\"\n",
      "Topic: 34 Words: 0.075*\"burrito\" + 0.058*\"taco\" + 0.047*\"asada\" + 0.043*\"carn\" + 0.016*\"mexican\" + 0.014*\"salsa\" + 0.013*\"quinoa\" + 0.012*\"meat\" + 0.011*\"bean\" + 0.009*\"mapl\"\n",
      "Topic: 35 Words: 0.046*\"healthi\" + 0.018*\"takeout\" + 0.017*\"gotta\" + 0.014*\"tonkatsu\" + 0.014*\"offic\" + 0.013*\"passion\" + 0.010*\"setup\" + 0.009*\"upgrad\" + 0.009*\"strike\" + 0.008*\"option\"\n",
      "Topic: 36 Words: 0.053*\"organ\" + 0.039*\"onlin\" + 0.034*\"regret\" + 0.034*\"goat\" + 0.029*\"wheat\" + 0.026*\"pride\" + 0.024*\"skimp\" + 0.024*\"pumpkin\" + 0.023*\"drip\" + 0.023*\"hurri\"\n",
      "Topic: 37 Words: 0.020*\"caesar\" + 0.017*\"prime\" + 0.014*\"shave\" + 0.013*\"snow\" + 0.013*\"espresso\" + 0.009*\"salad\" + 0.008*\"save\" + 0.007*\"jar\" + 0.007*\"visitor\" + 0.006*\"upbeat\"\n",
      "Topic: 38 Words: 0.079*\"pancak\" + 0.021*\"hide\" + 0.016*\"closer\" + 0.016*\"syrup\" + 0.014*\"own\" + 0.012*\"cornbread\" + 0.011*\"west\" + 0.011*\"upscal\" + 0.011*\"let\" + 0.011*\"famili\"\n",
      "Topic: 39 Words: 0.068*\"breakfast\" + 0.051*\"coffe\" + 0.035*\"egg\" + 0.022*\"toast\" + 0.021*\"hash\" + 0.021*\"biscuit\" + 0.019*\"gravi\" + 0.019*\"bacon\" + 0.016*\"omelet\" + 0.014*\"brown\"\n",
      "Topic: 40 Words: 0.012*\"club\" + 0.012*\"island\" + 0.010*\"octopus\" + 0.009*\"luck\" + 0.009*\"swiss\" + 0.009*\"children\" + 0.009*\"ricotta\" + 0.009*\"jack\" + 0.008*\"burger\" + 0.008*\"chees\"\n",
      "Topic: 41 Words: 0.104*\"nacho\" + 0.058*\"grit\" + 0.048*\"empanada\" + 0.037*\"effici\" + 0.034*\"spectacular\" + 0.032*\"holi\" + 0.031*\"last\" + 0.024*\"crappi\" + 0.024*\"gold\" + 0.022*\"darn\"\n",
      "Topic: 42 Words: 0.068*\"fajita\" + 0.052*\"tuesday\" + 0.040*\"road\" + 0.036*\"curd\" + 0.036*\"waitstaff\" + 0.024*\"bare\" + 0.024*\"tongu\" + 0.023*\"butt\" + 0.022*\"karaok\" + 0.021*\"joke\"\n",
      "Topic: 43 Words: 0.032*\"bartend\" + 0.021*\"wednesday\" + 0.020*\"entertain\" + 0.018*\"guy\" + 0.017*\"band\" + 0.016*\"mexico\" + 0.016*\"smile\" + 0.015*\"pool\" + 0.015*\"fabul\" + 0.014*\"calori\"\n",
      "Topic: 44 Words: 0.014*\"attitud\" + 0.009*\"manag\" + 0.008*\"ask\" + 0.007*\"order\" + 0.007*\"say\" + 0.007*\"fix\" + 0.006*\"tabl\" + 0.006*\"tell\" + 0.006*\"custom\" + 0.006*\"browni\"\n",
      "Topic: 45 Words: 0.045*\"bruschetta\" + 0.041*\"ambienc\" + 0.037*\"trendi\" + 0.032*\"kabob\" + 0.031*\"mimosa\" + 0.028*\"board\" + 0.027*\"pop\" + 0.023*\"postino\" + 0.019*\"depend\" + 0.019*\"success\"\n",
      "Topic: 46 Words: 0.037*\"downtown\" + 0.029*\"modern\" + 0.029*\"relax\" + 0.025*\"patio\" + 0.024*\"outdoor\" + 0.020*\"hang\" + 0.019*\"cut\" + 0.019*\"hospit\" + 0.018*\"great\" + 0.018*\"ambianc\"\n",
      "Topic: 47 Words: 0.031*\"smoothi\" + 0.013*\"sangria\" + 0.013*\"venu\" + 0.010*\"loung\" + 0.009*\"defin\" + 0.009*\"bitter\" + 0.009*\"push\" + 0.007*\"attract\" + 0.006*\"skill\" + 0.006*\"place\"\n",
      "Topic: 48 Words: 0.056*\"korean\" + 0.029*\"gourmet\" + 0.023*\"mongolian\" + 0.022*\"muffin\" + 0.020*\"monday\" + 0.019*\"cupcak\" + 0.016*\"weak\" + 0.014*\"commerci\" + 0.014*\"grass\" + 0.013*\"crazi\"\n",
      "Topic: 49 Words: 0.032*\"salsa\" + 0.025*\"mexican\" + 0.025*\"taco\" + 0.024*\"chip\" + 0.018*\"guacamol\" + 0.015*\"enchilada\" + 0.015*\"bean\" + 0.014*\"tortilla\" + 0.010*\"chile\" + 0.010*\"green\"\n",
      "Topic: 50 Words: 0.033*\"min\" + 0.022*\"secret\" + 0.018*\"brother\" + 0.012*\"prior\" + 0.010*\"knife\" + 0.010*\"meati\" + 0.008*\"intend\" + 0.008*\"redeem\" + 0.008*\"renov\" + 0.008*\"blacken\"\n",
      "Topic: 51 Words: 0.040*\"casual\" + 0.039*\"torta\" + 0.037*\"scratch\" + 0.029*\"vacat\" + 0.027*\"rotat\" + 0.025*\"shit\" + 0.022*\"worri\" + 0.021*\"quich\" + 0.021*\"decis\" + 0.019*\"afraid\"\n",
      "Topic: 52 Words: 0.054*\"shake\" + 0.041*\"waffl\" + 0.023*\"phenomen\" + 0.021*\"burger\" + 0.014*\"risotto\" + 0.012*\"fri\" + 0.010*\"trio\" + 0.010*\"rough\" + 0.009*\"intrigu\" + 0.009*\"chicken\"\n",
      "Topic: 53 Words: 0.033*\"bun\" + 0.020*\"fianc\" + 0.018*\"boy\" + 0.017*\"mediocr\" + 0.017*\"store\" + 0.016*\"wifi\" + 0.016*\"candi\" + 0.016*\"grand\" + 0.015*\"comparison\" + 0.013*\"central\"\n",
      "Topic: 54 Words: 0.090*\"wing\" + 0.080*\"pizza\" + 0.023*\"oven\" + 0.017*\"screen\" + 0.016*\"didnt\" + 0.016*\"draft\" + 0.016*\"boil\" + 0.015*\"cent\" + 0.015*\"fire\" + 0.015*\"flat\"\n",
      "Topic: 55 Words: 0.017*\"brew\" + 0.010*\"feta\" + 0.009*\"indoor\" + 0.008*\"cauliflow\" + 0.007*\"beer\" + 0.006*\"breast\" + 0.006*\"delay\" + 0.006*\"gumbo\" + 0.006*\"disorgan\" + 0.005*\"chees\"\n",
      "Topic: 56 Words: 0.030*\"turkey\" + 0.022*\"dirti\" + 0.017*\"velvet\" + 0.016*\"smoke\" + 0.014*\"knock\" + 0.012*\"tray\" + 0.012*\"poor\" + 0.010*\"smaller\" + 0.010*\"edibl\" + 0.009*\"bathroom\"\n",
      "Topic: 57 Words: 0.024*\"addict\" + 0.014*\"starv\" + 0.014*\"verd\" + 0.013*\"draw\" + 0.013*\"bore\" + 0.010*\"resort\" + 0.009*\"south\" + 0.009*\"instant\" + 0.008*\"elev\" + 0.008*\"jam\"\n",
      "Topic: 58 Words: 0.050*\"station\" + 0.047*\"horribl\" + 0.045*\"protein\" + 0.031*\"temp\" + 0.030*\"girlfriend\" + 0.024*\"gonna\" + 0.024*\"hit\" + 0.023*\"press\" + 0.021*\"bar\" + 0.020*\"ador\"\n",
      "Topic: 59 Words: 0.043*\"overpric\" + 0.043*\"scottsdal\" + 0.035*\"creativ\" + 0.032*\"dont\" + 0.024*\"north\" + 0.019*\"tartar\" + 0.018*\"dream\" + 0.017*\"cater\" + 0.017*\"yard\" + 0.015*\"snob\"\n",
      "Topic: 60 Words: 0.015*\"coke\" + 0.012*\"diet\" + 0.012*\"bistro\" + 0.011*\"squid\" + 0.010*\"app\" + 0.010*\"grub\" + 0.009*\"quantiti\" + 0.009*\"chinatown\" + 0.009*\"upstair\" + 0.008*\"angel\"\n",
      "Topic: 61 Words: 0.021*\"dough\" + 0.018*\"cooki\" + 0.018*\"hair\" + 0.017*\"clam\" + 0.017*\"soggi\" + 0.016*\"pizza\" + 0.015*\"oili\" + 0.014*\"artichok\" + 0.014*\"brat\" + 0.012*\"spinach\"\n",
      "Topic: 62 Words: 0.049*\"great\" + 0.029*\"gluten\" + 0.025*\"servic\" + 0.025*\"amaz\" + 0.024*\"love\" + 0.024*\"awesom\" + 0.023*\"atmospher\" + 0.022*\"staff\" + 0.019*\"friend\" + 0.018*\"good\"\n",
      "Topic: 63 Words: 0.047*\"donut\" + 0.031*\"beet\" + 0.029*\"tempura\" + 0.026*\"arizona\" + 0.025*\"california\" + 0.024*\"bacchan\" + 0.022*\"yellow\" + 0.022*\"defiant\" + 0.018*\"marrow\" + 0.018*\"yellowtail\"\n",
      "Topic: 64 Words: 0.019*\"minut\" + 0.017*\"wait\" + 0.012*\"order\" + 0.011*\"take\" + 0.011*\"tabl\" + 0.011*\"server\" + 0.010*\"ask\" + 0.010*\"come\" + 0.009*\"drink\" + 0.008*\"time\"\n",
      "Topic: 65 Words: 0.017*\"craft\" + 0.014*\"hole\" + 0.013*\"twist\" + 0.013*\"wall\" + 0.012*\"cinnamon\" + 0.012*\"chill\" + 0.011*\"beer\" + 0.011*\"cart\" + 0.010*\"till\" + 0.008*\"effort\"\n",
      "Topic: 66 Words: 0.065*\"casino\" + 0.036*\"catfish\" + 0.036*\"juic\" + 0.036*\"conveni\" + 0.028*\"valet\" + 0.023*\"ride\" + 0.021*\"yummi\" + 0.021*\"bike\" + 0.020*\"vega\" + 0.020*\"straight\"\n",
      "Topic: 67 Words: 0.018*\"potenti\" + 0.015*\"ignor\" + 0.014*\"filet\" + 0.014*\"longer\" + 0.012*\"frustrat\" + 0.011*\"that\" + 0.011*\"midnight\" + 0.009*\"healthier\" + 0.009*\"faster\" + 0.009*\"websit\"\n",
      "Topic: 68 Words: 0.111*\"vega\" + 0.036*\"hamburg\" + 0.028*\"john\" + 0.025*\"whiskey\" + 0.020*\"mile\" + 0.017*\"weekday\" + 0.017*\"diego\" + 0.016*\"best\" + 0.013*\"ravioli\" + 0.013*\"malt\"\n",
      "Topic: 69 Words: 0.037*\"chicago\" + 0.029*\"oyster\" + 0.029*\"lamb\" + 0.020*\"brunch\" + 0.017*\"killer\" + 0.016*\"blast\" + 0.016*\"thrill\" + 0.016*\"oper\" + 0.016*\"gras\" + 0.012*\"root\"\n",
      "Topic: 70 Words: 0.032*\"sick\" + 0.029*\"yesterday\" + 0.024*\"natur\" + 0.023*\"seawe\" + 0.023*\"awkward\" + 0.022*\"gracious\" + 0.022*\"judg\" + 0.020*\"word\" + 0.020*\"bummer\" + 0.020*\"remov\"\n",
      "Topic: 71 Words: 0.054*\"ramen\" + 0.032*\"noodl\" + 0.025*\"broth\" + 0.015*\"soup\" + 0.014*\"pork\" + 0.011*\"inexpens\" + 0.011*\"asian\" + 0.010*\"broccoli\" + 0.010*\"monta\" + 0.010*\"wonton\"\n",
      "Topic: 72 Words: 0.046*\"slaw\" + 0.043*\"hawaiian\" + 0.033*\"fountain\" + 0.033*\"cole\" + 0.029*\"pastrami\" + 0.028*\"mall\" + 0.025*\"process\" + 0.025*\"savori\" + 0.022*\"critic\" + 0.022*\"spacious\"\n",
      "Topic: 73 Words: 0.078*\"sushi\" + 0.043*\"roll\" + 0.013*\"miso\" + 0.012*\"tuna\" + 0.011*\"fish\" + 0.010*\"place\" + 0.010*\"nigiri\" + 0.010*\"fresh\" + 0.009*\"salmon\" + 0.008*\"great\"\n",
      "Topic: 74 Words: 0.066*\"buffet\" + 0.025*\"indian\" + 0.015*\"jalapeo\" + 0.015*\"concept\" + 0.014*\"fusion\" + 0.013*\"bagel\" + 0.012*\"naan\" + 0.010*\"line\" + 0.009*\"masala\" + 0.009*\"select\"\n",
      "Topic: 75 Words: 0.030*\"quesadilla\" + 0.029*\"knot\" + 0.025*\"lemonad\" + 0.021*\"peach\" + 0.019*\"creme\" + 0.018*\"shame\" + 0.018*\"expens\" + 0.016*\"watermelon\" + 0.016*\"brule\" + 0.015*\"impecc\"\n",
      "Topic: 76 Words: 0.025*\"lobster\" + 0.013*\"coconut\" + 0.010*\"cream\" + 0.009*\"salad\" + 0.009*\"insan\" + 0.009*\"vega\" + 0.008*\"flavorless\" + 0.008*\"detail\" + 0.008*\"whip\" + 0.007*\"meatloaf\"\n",
      "Topic: 77 Words: 0.018*\"banana\" + 0.015*\"york\" + 0.013*\"text\" + 0.011*\"cheesecak\" + 0.011*\"hibachi\" + 0.010*\"eastern\" + 0.010*\"spaghetti\" + 0.009*\"matt\" + 0.009*\"wanna\" + 0.007*\"starter\"\n",
      "Topic: 78 Words: 0.023*\"pesto\" + 0.020*\"farm\" + 0.016*\"tourist\" + 0.012*\"southwest\" + 0.012*\"villag\" + 0.012*\"sad\" + 0.012*\"rubberi\" + 0.011*\"cooler\" + 0.011*\"straw\" + 0.011*\"oatmeal\"\n",
      "Topic: 79 Words: 0.119*\"dog\" + 0.050*\"cute\" + 0.041*\"fool\" + 0.033*\"industri\" + 0.032*\"laugh\" + 0.025*\"easi\" + 0.023*\"citizen\" + 0.023*\"dump\" + 0.021*\"nearbi\" + 0.021*\"solo\"\n",
      "Topic: 80 Words: 0.045*\"employe\" + 0.041*\"gordon\" + 0.035*\"ramsay\" + 0.026*\"haha\" + 0.023*\"screw\" + 0.022*\"burger\" + 0.020*\"mahi\" + 0.017*\"capres\" + 0.016*\"buy\" + 0.015*\"kinda\"\n",
      "Topic: 81 Words: 0.053*\"thai\" + 0.041*\"curri\" + 0.018*\"tofu\" + 0.016*\"spici\" + 0.016*\"dumpl\" + 0.015*\"rice\" + 0.015*\"spring\" + 0.014*\"noodl\" + 0.013*\"chicken\" + 0.012*\"dish\"\n",
      "Topic: 82 Words: 0.059*\"gyro\" + 0.047*\"pita\" + 0.040*\"hummus\" + 0.038*\"greek\" + 0.015*\"starbuck\" + 0.015*\"crew\" + 0.013*\"ramsey\" + 0.012*\"fight\" + 0.011*\"salad\" + 0.011*\"challeng\"\n",
      "Topic: 83 Words: 0.044*\"chines\" + 0.018*\"milk\" + 0.014*\"express\" + 0.013*\"alright\" + 0.012*\"china\" + 0.012*\"blah\" + 0.011*\"tasteless\" + 0.010*\"chicken\" + 0.010*\"wasnt\" + 0.010*\"panda\"\n",
      "Topic: 84 Words: 0.055*\"vietnames\" + 0.025*\"cuisin\" + 0.022*\"valu\" + 0.020*\"execut\" + 0.020*\"thorough\" + 0.018*\"creat\" + 0.017*\"balanc\" + 0.017*\"pair\" + 0.016*\"jelli\" + 0.015*\"corpor\"\n",
      "Topic: 85 Words: 0.030*\"sub\" + 0.014*\"patient\" + 0.013*\"hotel\" + 0.012*\"assist\" + 0.012*\"packag\" + 0.011*\"arcadia\" + 0.010*\"jersey\" + 0.010*\"categori\" + 0.009*\"respect\" + 0.009*\"futur\"\n",
      "Topic: 86 Words: 0.024*\"sashimi\" + 0.022*\"hype\" + 0.014*\"post\" + 0.014*\"suck\" + 0.012*\"facebook\" + 0.011*\"accept\" + 0.011*\"member\" + 0.011*\"uncomfort\" + 0.011*\"giant\" + 0.010*\"yell\"\n",
      "-------------------------------------------------------------------\n",
      "Topic: 0 Words: 0.008*\"noodl\" + 0.006*\"order\" + 0.006*\"pork\" + 0.005*\"soup\" + 0.005*\"dish\" + 0.005*\"dumpl\" + 0.005*\"chicken\" + 0.005*\"beef\" + 0.005*\"like\" + 0.005*\"sauc\"\n",
      "Topic: 1 Words: 0.022*\"great\" + 0.018*\"love\" + 0.013*\"sushi\" + 0.012*\"servic\" + 0.011*\"excel\" + 0.011*\"chicken\" + 0.011*\"awesom\" + 0.011*\"amaz\" + 0.011*\"fresh\" + 0.010*\"best\"\n",
      "Topic: 2 Words: 0.015*\"ramen\" + 0.011*\"boba\" + 0.007*\"deliveri\" + 0.006*\"order\" + 0.006*\"chicken\" + 0.005*\"place\" + 0.005*\"love\" + 0.005*\"malaysian\" + 0.005*\"truck\" + 0.005*\"great\"\n",
      "Topic: 3 Words: 0.017*\"island\" + 0.016*\"korean\" + 0.011*\"gyoza\" + 0.010*\"tapa\" + 0.008*\"brisket\" + 0.008*\"panang\" + 0.008*\"watermelon\" + 0.007*\"styrofoam\" + 0.007*\"jerk\" + 0.007*\"flake\"\n",
      "Topic: 4 Words: 0.009*\"great\" + 0.007*\"price\" + 0.007*\"clean\" + 0.007*\"servic\" + 0.007*\"place\" + 0.007*\"staff\" + 0.006*\"time\" + 0.006*\"restaur\" + 0.006*\"friend\" + 0.006*\"nice\"\n",
      "Topic: 5 Words: 0.023*\"casino\" + 0.010*\"breakfast\" + 0.009*\"vega\" + 0.007*\"pricey\" + 0.006*\"tiger\" + 0.006*\"place\" + 0.006*\"overpric\" + 0.006*\"best\" + 0.006*\"defin\" + 0.006*\"guacamol\"\n",
      "Topic: 6 Words: 0.021*\"taro\" + 0.017*\"condens\" + 0.011*\"toast\" + 0.009*\"snoh\" + 0.009*\"conge\" + 0.008*\"simi\" + 0.006*\"mian\" + 0.006*\"buffet\" + 0.006*\"shabu\" + 0.006*\"close\"\n",
      "Topic: 7 Words: 0.009*\"lamb\" + 0.008*\"lunch\" + 0.007*\"great\" + 0.007*\"special\" + 0.006*\"thai\" + 0.005*\"love\" + 0.005*\"chines\" + 0.005*\"chicken\" + 0.005*\"price\" + 0.005*\"place\"\n",
      "Topic: 8 Words: 0.011*\"xiao\" + 0.010*\"margarita\" + 0.009*\"panda\" + 0.009*\"strawberri\" + 0.008*\"madison\" + 0.008*\"avocado\" + 0.007*\"express\" + 0.007*\"ichiban\" + 0.006*\"everyday\" + 0.006*\"buy\"\n",
      "Topic: 9 Words: 0.013*\"servic\" + 0.011*\"fast\" + 0.010*\"great\" + 0.008*\"deliveri\" + 0.008*\"east\" + 0.008*\"price\" + 0.008*\"friend\" + 0.007*\"best\" + 0.007*\"chines\" + 0.006*\"reason\"\n",
      "-------------------------------------------------------------------\n",
      "Topic: 0 Words: 0.016*\"close\" + 0.014*\"sushi\" + 0.009*\"tuna\" + 0.007*\"bake\" + 0.006*\"horribl\" + 0.006*\"locat\" + 0.006*\"roll\" + 0.006*\"salmon\" + 0.006*\"hibachi\" + 0.005*\"hour\"\n",
      "Topic: 1 Words: 0.011*\"lamb\" + 0.010*\"cart\" + 0.006*\"chicken\" + 0.005*\"tast\" + 0.005*\"rice\" + 0.005*\"good\" + 0.005*\"fri\" + 0.005*\"cucumb\" + 0.005*\"sauc\" + 0.004*\"sushi\"\n",
      "Topic: 2 Words: 0.010*\"order\" + 0.008*\"wait\" + 0.008*\"time\" + 0.007*\"minut\" + 0.007*\"say\" + 0.007*\"tell\" + 0.007*\"servic\" + 0.007*\"custom\" + 0.006*\"come\" + 0.006*\"take\"\n",
      "Topic: 3 Words: 0.007*\"good\" + 0.006*\"noodl\" + 0.006*\"place\" + 0.006*\"chines\" + 0.006*\"price\" + 0.006*\"servic\" + 0.005*\"come\" + 0.005*\"restaur\" + 0.005*\"better\" + 0.005*\"portion\"\n",
      "Topic: 4 Words: 0.008*\"chicken\" + 0.007*\"noodl\" + 0.007*\"soup\" + 0.006*\"rice\" + 0.006*\"sauc\" + 0.006*\"order\" + 0.005*\"dish\" + 0.005*\"chines\" + 0.005*\"flavor\" + 0.005*\"like\"\n",
      "Topic: 5 Words: 0.011*\"chicken\" + 0.008*\"fri\" + 0.007*\"beef\" + 0.007*\"rice\" + 0.007*\"tast\" + 0.006*\"order\" + 0.006*\"flavor\" + 0.005*\"like\" + 0.005*\"sauc\" + 0.005*\"dish\"\n",
      "Topic: 6 Words: 0.007*\"buffet\" + 0.005*\"chines\" + 0.005*\"restaur\" + 0.005*\"menu\" + 0.005*\"place\" + 0.004*\"good\" + 0.004*\"servic\" + 0.004*\"dish\" + 0.004*\"price\" + 0.004*\"come\"\n",
      "Topic: 7 Words: 0.008*\"banana\" + 0.007*\"certif\" + 0.005*\"chicken\" + 0.005*\"rank\" + 0.005*\"tastier\" + 0.004*\"layout\" + 0.004*\"luxor\" + 0.004*\"good\" + 0.004*\"ach\" + 0.004*\"order\"\n",
      "Topic: 8 Words: 0.008*\"panda\" + 0.007*\"unfriend\" + 0.007*\"tortilla\" + 0.006*\"express\" + 0.006*\"payment\" + 0.006*\"toast\" + 0.005*\"fridg\" + 0.005*\"squid\" + 0.005*\"deni\" + 0.005*\"court\"\n",
      "Topic: 9 Words: 0.009*\"address\" + 0.008*\"miso\" + 0.008*\"prime\" + 0.008*\"better\" + 0.007*\"stinki\" + 0.007*\"chines\" + 0.007*\"worst\" + 0.007*\"panda\" + 0.006*\"great\" + 0.006*\"averag\"\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in model_1.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))\n",
    "print('-------------------------------------------------------------------')\n",
    "\n",
    "# for idx, topic in model_1_1.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))\n",
    "# print('-------------------------------------------------------------------')\n",
    "\n",
    "for idx, topic in model_2.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))\n",
    "print('-------------------------------------------------------------------')\n",
    "\n",
    "for idx, topic in model_3.print_topics(-1):\n",
    "    print('Topic: {} Words: {}'.format(idx, topic))\n",
    "print('-------------------------------------------------------------------')\n",
    "# for idx, topic in model_4.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))\n",
    "# print('-------------------------------------------------------------------')\n",
    "# for idx, topic in model_5.print_topics(-1):\n",
    "#     print('Topic: {} Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (corpus, dictionary) = get_corpus(files[0] + \".txt\")\n",
    "\n",
    "# for k in range(5, 20):\n",
    "#     model = get_lda_topic_model(corpus, dictionary, num_topics=k)    \n",
    "#     cm = models.coherencemodel.CoherenceModel(model=model, corpus=corpus, coherence='u_mass')\n",
    "#     print(\"k=%d coherence=%f\"%(k,cm.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (corpus, dictionary) = get_corpus(files[1] + \".txt\")\n",
    "# hdp_2 = models.HdpModel(corpus, dictionary)\n",
    "# l=hdp_2.suggested_lda_model()\n",
    "# l.show_topics(20)\n",
    "\n",
    "# (corpus, dictionary) = get_corpus(files[2] + \".txt\")\n",
    "# hdp_3 = models.HdpModel(corpus, dictionary)\n",
    "# l=hdp_3.suggested_lda_model()\n",
    "# l.show_topics(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://qpleple.com/topic-coherence-to-evaluate-topic-models/\n",
    "\n",
    "Select number of topics for LDA model: https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

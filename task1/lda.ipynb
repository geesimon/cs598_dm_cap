{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "# from nltk.stem.porter import *\n",
    "# import numpy as np\n",
    "# import nltk\n",
    "\n",
    "# np.random.seed(2018)\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# from time import time\n",
    "# np.random.seed(2018)\n",
    "\n",
    "# #logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# #lda_model = gensim.models.LdaModel(bow_corpus, num_topics=10, id2word=dictionary,  eval_every=5, iterations = 1000, alpha='auto', gamma_threshold=0.01)\n",
    "# t0 = time()\n",
    "# #lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, iterations = 1000, passes = 2)\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, num_topics=10, id2word=dictionary,  eval_every=5, iterations = 1000, alpha='auto', gamma_threshold=0.01)\n",
    "# print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "# for idx, topic in lda_model.print_topics(-1):\n",
    "#     print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 100000/100000 [02:44<00:00, 606.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 78.656437s\n",
      "[(0, '0.021*\"pepperoni\" + 0.014*\"katsu\" + 0.012*\"knot\" + 0.011*\"kink\" + 0.011*\"pizza\" + 0.010*\"text\" + 0.009*\"pizzeria\" + 0.008*\"insan\" + 0.008*\"individu\" + 0.007*\"watermelon\"'), (1, '0.021*\"falafel\" + 0.020*\"bagel\" + 0.019*\"scrambl\" + 0.018*\"syrup\" + 0.013*\"gras\" + 0.013*\"sooooo\" + 0.012*\"jerk\" + 0.011*\"arepa\" + 0.011*\"marrow\" + 0.011*\"yellowtail\"'), (2, '0.022*\"sub\" + 0.022*\"philli\" + 0.015*\"cheesesteak\" + 0.012*\"footbal\" + 0.012*\"hibachi\" + 0.010*\"butcher\" + 0.009*\"chinatown\" + 0.009*\"setup\" + 0.008*\"dcor\" + 0.008*\"firehous\"'), (3, '0.008*\"great\" + 0.006*\"good\" + 0.006*\"place\" + 0.006*\"pizza\" + 0.005*\"servic\" + 0.005*\"love\" + 0.005*\"burger\" + 0.005*\"friend\" + 0.005*\"order\" + 0.005*\"like\"'), (4, '0.007*\"chocol\" + 0.007*\"strawberri\" + 0.006*\"bomb\" + 0.005*\"pretzel\" + 0.005*\"bun\" + 0.005*\"donut\" + 0.005*\"creativ\" + 0.005*\"kale\" + 0.005*\"fruit\" + 0.005*\"pancak\"'), (5, '0.027*\"crepe\" + 0.024*\"tapa\" + 0.021*\"ayc\" + 0.013*\"nigiri\" + 0.012*\"boba\" + 0.009*\"summerlin\" + 0.009*\"cut\" + 0.008*\"poke\" + 0.008*\"signatur\" + 0.008*\"blast\"'), (6, '0.008*\"minut\" + 0.007*\"wait\" + 0.006*\"tabl\" + 0.006*\"manag\" + 0.006*\"ask\" + 0.005*\"drink\" + 0.005*\"take\" + 0.005*\"say\" + 0.005*\"tell\" + 0.005*\"custom\"'), (7, '0.010*\"flavour\" + 0.010*\"thorough\" + 0.009*\"macaroon\" + 0.008*\"restroom\" + 0.008*\"brooklyn\" + 0.008*\"exampl\" + 0.008*\"pari\" + 0.007*\"bottomless\" + 0.007*\"bark\" + 0.007*\"jason\"'), (8, '0.035*\"gyro\" + 0.031*\"smoothi\" + 0.024*\"pita\" + 0.013*\"greek\" + 0.012*\"hummus\" + 0.011*\"quinoa\" + 0.011*\"kabob\" + 0.009*\"wellington\" + 0.008*\"mule\" + 0.008*\"bistro\"'), (9, '0.066*\"taco\" + 0.043*\"salsa\" + 0.033*\"burrito\" + 0.030*\"asada\" + 0.029*\"carn\" + 0.028*\"mexican\" + 0.019*\"enchilada\" + 0.015*\"tortilla\" + 0.015*\"pastor\" + 0.014*\"guacamol\"')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 27804/27804 [00:40<00:00, 690.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 24.634790s\n",
      "[(0, '0.007*\"buffet\" + 0.007*\"belli\" + 0.006*\"chines\" + 0.005*\"place\" + 0.005*\"like\" + 0.005*\"great\" + 0.005*\"restaur\" + 0.004*\"order\" + 0.004*\"chicken\" + 0.004*\"come\"'), (1, '0.025*\"malaysian\" + 0.021*\"clever\" + 0.019*\"salon\" + 0.017*\"satay\" + 0.011*\"udon\" + 0.011*\"northern\" + 0.009*\"bomb\" + 0.009*\"outstand\" + 0.009*\"papaya\" + 0.008*\"meatbal\"'), (2, '0.006*\"order\" + 0.005*\"thai\" + 0.005*\"time\" + 0.005*\"place\" + 0.005*\"curri\" + 0.004*\"restaur\" + 0.004*\"great\" + 0.004*\"like\" + 0.004*\"dish\" + 0.004*\"noodl\"'), (3, '0.010*\"taiwanes\" + 0.010*\"vegan\" + 0.007*\"shave\" + 0.006*\"place\" + 0.006*\"noodl\" + 0.006*\"great\" + 0.005*\"price\" + 0.005*\"order\" + 0.005*\"chines\" + 0.005*\"close\"'), (4, '0.024*\"taco\" + 0.013*\"taro\" + 0.009*\"sushi\" + 0.008*\"shave\" + 0.007*\"mexican\" + 0.006*\"great\" + 0.006*\"strawberri\" + 0.005*\"kimchi\" + 0.005*\"fusion\" + 0.005*\"snoh\"'), (5, '0.048*\"casino\" + 0.015*\"wendi\" + 0.015*\"best\" + 0.014*\"vega\" + 0.012*\"handmad\" + 0.009*\"kong\" + 0.009*\"hong\" + 0.009*\"favourit\" + 0.009*\"taiwan\" + 0.008*\"devour\"'), (6, '0.010*\"noodl\" + 0.008*\"pork\" + 0.007*\"beef\" + 0.007*\"soup\" + 0.007*\"dumpl\" + 0.007*\"sauc\" + 0.006*\"flavor\" + 0.006*\"dish\" + 0.006*\"spici\" + 0.005*\"duck\"'), (7, '0.011*\"boba\" + 0.006*\"sushi\" + 0.005*\"great\" + 0.005*\"place\" + 0.004*\"ramen\" + 0.004*\"roll\" + 0.004*\"chines\" + 0.004*\"avocado\" + 0.004*\"restaur\" + 0.004*\"smoothi\"'), (8, '0.007*\"condens\" + 0.005*\"toast\" + 0.004*\"place\" + 0.004*\"gyoza\" + 0.004*\"restaur\" + 0.004*\"friend\" + 0.004*\"staff\" + 0.003*\"great\" + 0.003*\"chines\" + 0.003*\"belli\"'), (9, '0.020*\"great\" + 0.015*\"servic\" + 0.012*\"love\" + 0.011*\"price\" + 0.011*\"chicken\" + 0.011*\"lunch\" + 0.011*\"ramen\" + 0.010*\"fast\" + 0.010*\"friend\" + 0.010*\"deliveri\"')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10912/10912 [00:16<00:00, 644.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 10.797335s\n",
      "[(0, '0.007*\"horribl\" + 0.006*\"place\" + 0.006*\"rice\" + 0.005*\"order\" + 0.005*\"servic\" + 0.005*\"fri\" + 0.005*\"chicken\" + 0.005*\"good\" + 0.005*\"poison\" + 0.005*\"better\"'), (1, '0.009*\"close\" + 0.007*\"lousi\" + 0.006*\"stinki\" + 0.006*\"overpric\" + 0.005*\"chicken\" + 0.005*\"rice\" + 0.004*\"loss\" + 0.004*\"prime\" + 0.004*\"pickl\" + 0.004*\"ping\"'), (2, '0.017*\"custard\" + 0.012*\"special\" + 0.011*\"concept\" + 0.008*\"taiwanes\" + 0.008*\"fusion\" + 0.007*\"mislead\" + 0.007*\"seek\" + 0.007*\"competit\" + 0.006*\"burger\" + 0.006*\"small\"'), (3, '0.015*\"wait\" + 0.010*\"servic\" + 0.010*\"minut\" + 0.009*\"order\" + 0.008*\"min\" + 0.008*\"hour\" + 0.007*\"tabl\" + 0.007*\"time\" + 0.007*\"tell\" + 0.007*\"rude\"'), (4, '0.008*\"mango\" + 0.007*\"mini\" + 0.007*\"profession\" + 0.006*\"salmon\" + 0.006*\"cathay\" + 0.006*\"skewer\" + 0.005*\"ocean\" + 0.005*\"male\" + 0.005*\"student\" + 0.004*\"noodl\"'), (5, '0.007*\"order\" + 0.005*\"time\" + 0.005*\"come\" + 0.005*\"chicken\" + 0.004*\"place\" + 0.004*\"restaur\" + 0.004*\"servic\" + 0.004*\"good\" + 0.004*\"like\" + 0.004*\"dish\"'), (6, '0.007*\"buffet\" + 0.005*\"place\" + 0.005*\"price\" + 0.005*\"noodl\" + 0.004*\"chines\" + 0.004*\"servic\" + 0.004*\"tast\" + 0.004*\"like\" + 0.004*\"restaur\" + 0.004*\"good\"'), (7, '0.006*\"duck\" + 0.006*\"noodl\" + 0.005*\"soup\" + 0.005*\"salad\" + 0.005*\"chicken\" + 0.005*\"kimchi\" + 0.005*\"order\" + 0.005*\"groupon\" + 0.004*\"good\" + 0.004*\"beef\"'), (8, '0.008*\"chicken\" + 0.007*\"chines\" + 0.007*\"noodl\" + 0.007*\"good\" + 0.006*\"beef\" + 0.006*\"soup\" + 0.006*\"rice\" + 0.006*\"tast\" + 0.006*\"taco\" + 0.005*\"like\"'), (9, '0.010*\"tuna\" + 0.006*\"sushi\" + 0.006*\"expens\" + 0.005*\"restaur\" + 0.005*\"health\" + 0.005*\"prawn\" + 0.005*\"good\" + 0.005*\"tortilla\" + 0.004*\"place\" + 0.004*\"busi\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text, min_len = 4):\n",
    "        if token not in STOPWORDS:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "def get_topic_model(file_name, num_topics = 10, use_tfidf = True):\n",
    "    documents = []\n",
    "\n",
    "    with open (file_name, 'r') as f:\n",
    "        documents = f.readlines()\n",
    "    \n",
    "    processed_docs = [preprocess(text) for text in tqdm(documents)]\n",
    "    \n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    if use_tfidf:\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        corpus = tfidf[corpus]\n",
    "\n",
    "    t0 = time()\n",
    "    #lda_model = models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary,  eval_every=5, iterations = 1000, alpha='auto', gamma_threshold=0.01)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    \n",
    "    return lda_model\n",
    "\n",
    "files = ['rest_review_sample_100000', 'categories/Chinese_pos', 'categories/Chinese_neg']\n",
    "model_1 = get_topic_model(files[0] + \".txt\")\n",
    "print(model_1.show_topics())\n",
    "model_2 = get_topic_model(files[1] + \".txt\")\n",
    "print(model_2.show_topics())\n",
    "model_3 = get_topic_model(files[2] + \".txt\")\n",
    "print(model_3.show_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020752"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.show_topics(formatted=False)[0][1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_1\n",
    "\n",
    "def get_topic_json(model, title):\n",
    "    topic = model.show_topics(formatted=False)\n",
    "    children_name = 'children'\n",
    "    name_name = 'name'\n",
    "    value_name = 'value'\n",
    "\n",
    "    topic_out = {name_name: title, children_name:[]}\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        topic_out[children_name].append({name_name: 'Topic ' + str(i), children_name:[]})\n",
    "        max_weight = model.show_topics(formatted=False)[i][1][0][1]\n",
    "        for j in range(0, len(topic[i][1])):\n",
    "            topic_out[children_name][i][children_name].append({name_name:model.show_topics(formatted=False)[i][1][j][0],\n",
    "                                                           value_name:  \"{0:.2f}\".format(model.show_topics(formatted=False)[i][1][j][1]/max_weight)})\n",
    "        \n",
    "    return topic_out\n",
    "\n",
    "with open(files[0] + '.json', 'w') as f:\n",
    "    f.write(json.dumps(get_topic_json(model_1, '100000_Samples')))\n",
    "    \n",
    "topic2 = get_topic_json(model_2, 'Positive')\n",
    "topic3 = get_topic_json(model_3, 'Negative')\n",
    "topic = {name_name:'Chinese Restaurant Reviews', children_name:[topic2, topic3]}\n",
    "with open('compare' + '.json', 'w') as f:\n",
    "    f.write(json.dumps(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"10000_Samples\", \"children\": [{\"name\": \"Topic 0\", \"children\": [{\"name\": \"pepperoni\", \"value\": \"1.00\"}, {\"name\": \"katsu\", \"value\": \"0.69\"}, {\"name\": \"knot\", \"value\": \"0.59\"}, {\"name\": \"kink\", \"value\": \"0.55\"}, {\"name\": \"pizza\", \"value\": \"0.54\"}, {\"name\": \"text\", \"value\": \"0.50\"}, {\"name\": \"pizzeria\", \"value\": \"0.43\"}, {\"name\": \"insan\", \"value\": \"0.39\"}, {\"name\": \"individu\", \"value\": \"0.38\"}, {\"name\": \"watermelon\", \"value\": \"0.36\"}]}, {\"name\": \"Topic 1\", \"children\": [{\"name\": \"falafel\", \"value\": \"1.00\"}, {\"name\": \"bagel\", \"value\": \"0.92\"}, {\"name\": \"scrambl\", \"value\": \"0.89\"}, {\"name\": \"syrup\", \"value\": \"0.83\"}, {\"name\": \"gras\", \"value\": \"0.61\"}, {\"name\": \"sooooo\", \"value\": \"0.61\"}, {\"name\": \"jerk\", \"value\": \"0.58\"}, {\"name\": \"arepa\", \"value\": \"0.53\"}, {\"name\": \"marrow\", \"value\": \"0.53\"}, {\"name\": \"yellowtail\", \"value\": \"0.51\"}]}, {\"name\": \"Topic 2\", \"children\": [{\"name\": \"sub\", \"value\": \"1.00\"}, {\"name\": \"philli\", \"value\": \"1.00\"}, {\"name\": \"cheesesteak\", \"value\": \"0.70\"}, {\"name\": \"footbal\", \"value\": \"0.57\"}, {\"name\": \"hibachi\", \"value\": \"0.54\"}, {\"name\": \"butcher\", \"value\": \"0.45\"}, {\"name\": \"chinatown\", \"value\": \"0.42\"}, {\"name\": \"setup\", \"value\": \"0.39\"}, {\"name\": \"dcor\", \"value\": \"0.36\"}, {\"name\": \"firehous\", \"value\": \"0.35\"}]}, {\"name\": \"Topic 3\", \"children\": [{\"name\": \"great\", \"value\": \"1.00\"}, {\"name\": \"good\", \"value\": \"0.81\"}, {\"name\": \"place\", \"value\": \"0.80\"}, {\"name\": \"pizza\", \"value\": \"0.72\"}, {\"name\": \"servic\", \"value\": \"0.69\"}, {\"name\": \"love\", \"value\": \"0.69\"}, {\"name\": \"burger\", \"value\": \"0.68\"}, {\"name\": \"friend\", \"value\": \"0.64\"}, {\"name\": \"order\", \"value\": \"0.62\"}, {\"name\": \"like\", \"value\": \"0.58\"}]}, {\"name\": \"Topic 4\", \"children\": [{\"name\": \"chocol\", \"value\": \"1.00\"}, {\"name\": \"strawberri\", \"value\": \"0.98\"}, {\"name\": \"bomb\", \"value\": \"0.85\"}, {\"name\": \"pretzel\", \"value\": \"0.74\"}, {\"name\": \"bun\", \"value\": \"0.73\"}, {\"name\": \"donut\", \"value\": \"0.73\"}, {\"name\": \"creativ\", \"value\": \"0.73\"}, {\"name\": \"kale\", \"value\": \"0.73\"}, {\"name\": \"fruit\", \"value\": \"0.69\"}, {\"name\": \"pancak\", \"value\": \"0.68\"}]}, {\"name\": \"Topic 5\", \"children\": [{\"name\": \"crepe\", \"value\": \"1.00\"}, {\"name\": \"tapa\", \"value\": \"0.87\"}, {\"name\": \"ayc\", \"value\": \"0.76\"}, {\"name\": \"nigiri\", \"value\": \"0.46\"}, {\"name\": \"boba\", \"value\": \"0.43\"}, {\"name\": \"summerlin\", \"value\": \"0.33\"}, {\"name\": \"cut\", \"value\": \"0.31\"}, {\"name\": \"poke\", \"value\": \"0.31\"}, {\"name\": \"signatur\", \"value\": \"0.31\"}, {\"name\": \"blast\", \"value\": \"0.29\"}]}, {\"name\": \"Topic 6\", \"children\": [{\"name\": \"minut\", \"value\": \"1.00\"}, {\"name\": \"wait\", \"value\": \"0.91\"}, {\"name\": \"tabl\", \"value\": \"0.84\"}, {\"name\": \"manag\", \"value\": \"0.74\"}, {\"name\": \"ask\", \"value\": \"0.73\"}, {\"name\": \"drink\", \"value\": \"0.71\"}, {\"name\": \"take\", \"value\": \"0.70\"}, {\"name\": \"say\", \"value\": \"0.63\"}, {\"name\": \"tell\", \"value\": \"0.62\"}, {\"name\": \"custom\", \"value\": \"0.61\"}]}, {\"name\": \"Topic 7\", \"children\": [{\"name\": \"flavour\", \"value\": \"1.00\"}, {\"name\": \"thorough\", \"value\": \"0.96\"}, {\"name\": \"macaroon\", \"value\": \"0.92\"}, {\"name\": \"restroom\", \"value\": \"0.82\"}, {\"name\": \"brooklyn\", \"value\": \"0.79\"}, {\"name\": \"exampl\", \"value\": \"0.78\"}, {\"name\": \"pari\", \"value\": \"0.76\"}, {\"name\": \"bottomless\", \"value\": \"0.72\"}, {\"name\": \"bark\", \"value\": \"0.71\"}, {\"name\": \"jason\", \"value\": \"0.69\"}]}, {\"name\": \"Topic 8\", \"children\": [{\"name\": \"gyro\", \"value\": \"1.00\"}, {\"name\": \"smoothi\", \"value\": \"0.89\"}, {\"name\": \"pita\", \"value\": \"0.69\"}, {\"name\": \"greek\", \"value\": \"0.37\"}, {\"name\": \"hummus\", \"value\": \"0.34\"}, {\"name\": \"quinoa\", \"value\": \"0.32\"}, {\"name\": \"kabob\", \"value\": \"0.32\"}, {\"name\": \"wellington\", \"value\": \"0.25\"}, {\"name\": \"mule\", \"value\": \"0.24\"}, {\"name\": \"bistro\", \"value\": \"0.24\"}]}, {\"name\": \"Topic 9\", \"children\": [{\"name\": \"taco\", \"value\": \"1.00\"}, {\"name\": \"salsa\", \"value\": \"0.66\"}, {\"name\": \"burrito\", \"value\": \"0.51\"}, {\"name\": \"asada\", \"value\": \"0.46\"}, {\"name\": \"carn\", \"value\": \"0.43\"}, {\"name\": \"mexican\", \"value\": \"0.42\"}, {\"name\": \"enchilada\", \"value\": \"0.28\"}, {\"name\": \"tortilla\", \"value\": \"0.23\"}, {\"name\": \"pastor\", \"value\": \"0.22\"}, {\"name\": \"guacamol\", \"value\": \"0.22\"}]}]}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(topic_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
